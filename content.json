{"meta":{"title":"赵树超个人博客","subtitle":"","description":"","author":"ZSC","url":"https://zhaoshuchao.top","root":"/"},"pages":[{"title":"","date":"2021-06-02T03:30:41.344Z","updated":"2021-06-02T03:30:41.344Z","comments":true,"path":"about/index.html","permalink":"https://zhaoshuchao.top/about/index.html","excerpt":"","text":"不断学习的Java开发小菜鸟"},{"title":"所有分类","date":"2021-06-01T09:23:56.444Z","updated":"2021-06-01T09:23:56.444Z","comments":true,"path":"categories/index.html","permalink":"https://zhaoshuchao.top/categories/index.html","excerpt":"","text":""},{"title":"friends","date":"2021-06-01T09:28:56.000Z","updated":"2021-06-01T09:28:56.429Z","comments":true,"path":"friends/index.html","permalink":"https://zhaoshuchao.top/friends/index.html","excerpt":"","text":""},{"title":"","date":"2021-06-01T09:25:39.964Z","updated":"2021-06-01T09:25:39.964Z","comments":true,"path":"mylist/index.html","permalink":"https://zhaoshuchao.top/mylist/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2021-06-01T09:24:52.912Z","updated":"2021-06-01T09:24:52.912Z","comments":true,"path":"tags/index.html","permalink":"https://zhaoshuchao.top/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Redis持久化","slug":"Redis持久化","date":"2021-06-02T08:25:38.000Z","updated":"2021-06-02T08:32:51.002Z","comments":true,"path":"2021/06/02/Redis持久化/","link":"","permalink":"https://zhaoshuchao.top/2021/06/02/Redis%E6%8C%81%E4%B9%85%E5%8C%96/","excerpt":"","text":"redis持久化的意义redis的数据全部在内存中，如果突然宕机，数据就会全部丢失，因此必须有一种机制来保证redis的数据在遇到突发状况的时候不会丢失，或者只丢失少量，于是必须根据一些策略来把redis内存中的数据写到磁盘中，这样当redis服务重启中，就可以根据磁盘中的数据来恢复数据到内存中。 redis持久化机制redis有两种持久化机制：AOF和RDB (1) RDBRDB是一次的全量备份，即周期性的把redis当前内存中的全量数据写入到一个快照文件中。redis是单线程程序，这个线程要同时负责多个客户端的读写请求，还要负责周期性的把当前内存中的数据写到快照文件中RDB中，数据写到RDB文件是IO操作，IO操作会严重影响redis的性能，甚至在持久化的过程中，读写请求会阻塞，为了解决这些问题，redis需要同时进行读写请求和持久化操作，这样又会导致另外的问题，持久化的过程中，内存中的数据还在改变，假如redis正在进行持久化一个大的数据结构，在这个过程中客户端发送一个删除请求，把这个大的数据结构删掉了，这时候持久化的动作还没有完成，那么redis该怎么办呢？ redis使用操作系统的多进程COW机制(Copy On Write)机制来实现快照的持久化，在持久化过程中调用 glibc(Linux下的C函数库) 的函数fork()产生一个子进程，快照持久化完全交给子进程来处理，父进程继续处理客户端的读写请求。子进程刚刚产生时，和父进程共享内存里面的代码段和数据段，这是Linux操作系统的机制，为了节约内存资源，所以尽可能让父子进程共享内存，这样在进程分离的一瞬间，内存的增长几乎没有明显变化。 子进程对当前内存中的数据进行持久化，并不会修改当前的数据结构，如果父进程收到了读写请求，那么会把处理的那一部分数据复制一份到内存，对复制后的数据进行修改，所以即使对某个数据进行了修改，redis持久化到RDB中的数据也是未修改的数据，这也是把RDB文件称为”快照”文件的原因，子进程所看到的数据在它被创建的一瞬间就固定下来了，父进程修改的某个数据只是该数据的复制品。 实际上，内存中的全量数据由一个个的”数据段页面“组成，每个数据段页面的大小为4K，客户端要修改的数据在哪个页面中，就会复制一份这个页面到内存中，这个复制的过程称为”页面分离“，在持久化过程中，随着分离出的页面越来越多，内存就会持续增长，但是不会超过原内存的2倍，因为在一次持久化的过程中，几乎不会出现所有的页面都会分离的情况，读写请求针对的只是原数据中的小部分，大部分redis数据还是”冷数据“。 (2) AOFAOF日志存储的是redis服务器的顺序指令序列，即对内存中数据进行修改的指令记录。当redis收到客户端修改指令后，先进行参数校验，如果校验通过，先把该指令存储到AOF日志文件中，也就是先存到磁盘，然后再执行该修改指令。 当redis宕机后重启后，可以读取该AOF文件中的指令，进行数据恢复，恢复的过程就是把记录的指令再顺序执行一次，这样就可以恢复到宕机之前的状态。 redis在长期运行过程中，AOF日志会越来越大，如果redis服务重启后根据很大的AOF文件来顺序执行指令，将会非常耗时，导致redis服务长时间无法对外提供服务，所以需要对AOF文件进行”瘦身”。”瘦身”的过程称作AOF重写(rewrite)。 Rewrite``` 的原理是，主进程```fork```一个子进程，对当前内存中的数据进行遍历，转换成一系列的redis操作指令，并序列化到一个新的AOF日志中，然后把序列化操作期间新收到的操作指令追加到新的AOF文件中，追加完毕后就立即**替换**旧的AOF文件，这样就完成了\"瘦身\"工作，即AOF Rewrite。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596redis把操作指令追加到AOF文件这个过程，并不是直接写到AOF文件中，而是先写到操作系统的内存缓存中，这个内存缓存是由操作系统内核分配的，然后操作系统内核会**异步**地把内存缓存中的redis操作指令刷写到AOF文件中。一个新问题是，假如内存缓存中的redis指令还没有来得及刷写到AOF文件中就宕机了，那么这部分未刷写的指令就会丢失，不过，```glibc```函数库提供了 ```fsync()``` 函数，该函数可以将指定文件的内容强制从内存缓存中刷写到磁盘上。fsync操作的**周期**对redis的性能有很大影响，如何配置将在本文后续的内容中给出建议。&gt; AOF过程&gt; ![Image](https://gitee.com/zhao_shu_chao/img/raw/master/blog/20210528140155.webp)&gt; AOF Rewrite过程&gt; ![Image](https://gitee.com/zhao_shu_chao/img/raw/master/blog/20210528140200.webp)#### (3) redis-4.x 混合持久化重启redis时，我们很少使用RDB来恢复内存状态，因为会丢失大量数据。我们通常使用AOF日志重放，但是重放AOF日志性能相对RDB来说要慢很多，这样在redis实例很大的情况下，启动需要花费很长的时间。redis-4.0为了解决这个问题，带来了一个新的持久化选项——**混合持久化**。将RDB文件的内容和增量的AOF日志文件存在一起。这里的AOF日志不再是全量的日志，而是自持久化开始到持久化结束的这段时间发生的**增量AOF日志**，通常这部分AOF日志很小。&gt; redis-4.x混合持久化机制&gt; ![Image](https://gitee.com/zhao_shu_chao/img/raw/master/blog/20210528140203.webp)redis重启的时候，可以**先加载RDB的内容**，然后**再重放增量AOF日志**，就可以完全替代之前的AOF全量文件重放，恢复效率因此大幅得到提升。### redis 持久化机制对比#### (1) RDB的优缺点##### 优点：* RDB会生成多个数据文件，每个数据文件都代表了某一个时刻中redis的数据，这种多个数据文件的方式，非常适合做**冷备**，可以将这种完整的数据文件发送到一些远程的安全存储上去。* 当进行RDB持久化时，对redis服务处理**读写请求的影响非常小**，可以让redis保持**高性能**，因为redis主进程只需要fork一个子进程，让子进程执行磁盘IO操作来进行RDB持久化即可。生成一次RDB文件的过程就是把当前时刻内存中的数据一次性写入文件中，而AOF则需要先把当前内存中的小量数据转换为操作指令，然后把指令写到内存缓存中，然后再刷写入磁盘。* 相对于AOF持久化机制来说，直接基于RDB数据文件来重启和恢复redis的数据会更加**快速**。AOF，存放的是指令日志，做数据恢复的时候，要回放和执行所有的指令日志，从而恢复内存中的所有数据。而RDB，就是一份数据文件，恢复的时候，**直接加载到内存中**即可。##### 缺点：* 如果想要在redis故障时，尽可能少的丢失数据，那么RDB没有AOF好。一般来说，RDB数据快照文件，都是每隔5分钟，或者更长时间生成一次，这个时候就得接受一旦redis进程宕机，那么会丢失最近5分钟的数据。这个问题，也是RDB最大的缺点，就是不适合做**第一优先**的恢复方案，如果你依赖RDB做第一优先恢复方案，会导致数据丢失的比较多。* RDB每次在fork子进程来执行RDB快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，甚至数秒。所以一般不要让生成RDB文件的**间隔**太长，否则每次生成的RDB文件太大了，对redis本身的性能会有影响。#### (2) AOF的优缺点##### 优点：* AOF可以更好的保护数据**不丢失**，一般AOF会每隔1秒，通过一个后台线程执行一次fsync操作，最多丢失1秒钟的数据。* AOF日志文件以append-only模式写入，所以没有任何磁盘寻址的开销，写入性能非常高，而且文件**不容易破损**，即使文件尾部破损，也很容易修复。* AOF日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。因为在rewrite的时候，会对其中的指令进行压缩，会创建出一份需要恢复数据的最小日志出来。* AOF日志文件的命令通过非常可读的方式进行记录，这个特性非常适合做灾难性的误删除的**紧急恢复**。比如某人不小心用```flushall```命令清空了所有数据，只要这个时候后台```rewrite```还没有发生，那么就可以立即拷贝AOF文件，将最后一条```flushall```命令给删了，然后再将该AOF文件放回去，就可以通过恢复机制，自动恢复所有数据。##### 缺点：* 对于同一份数据来说，AOF日志文件通常比RDB数据快照文件更大。* AOF的**写性能**比RDB的写性能低，因为AOF一般会配置成每秒fsync一次日志文件，当然，每秒一次fsync，性能也还是很高的，只不过比起 RDB来说性能低，如果要保证一条数据都不丢，也是可以的，AOF的fsync设置成每写入一条数据，fsync一次，但是这样，redis的性能会大大下降。* 基于AOF文件做**恢复**的速度不如基于RDB文件做恢复的速度。#### (3) 混合持久化的优缺点##### 优点：结合了RDB和AOF的优点，使得数据恢复的效率大幅提升##### 缺点：兼容性不好，redis-4.x新增，虽然最终的文件也是.aof格式的文件，但在4.0之前版本都不识别该aof文件，同时由于前部分是RDB格式，阅读性较差。#### 如何选择redis持久化机制RDB和AOF到底该如何选择* 不要仅仅使用RDB，因为那样会导致你丢失很多数据* 也不要仅仅使用AOF，一是数据恢复慢，二是可靠性也不如RDB，毕竟RDB文件中存储的就是某一时刻实实在在的数据，而AOF只是操作指令，把数据转换为操作指令不一定是百分百没问题的。* 综合使用AOF和RDB两种持久化机制，用AOF来保证数据不丢失，作为数据恢复的第一选择; 用RDB来做不同程度的冷备，在AOF文件都丢失或损坏不可用的时候，还可以使用RDB来进行快速的数据恢复* ##### redis重启的时候，先加载RDB的内容，然后再重放增量AOF日志， #### (5) AOF和RDB同时工作* ##### redis在写RDB文件的时候不会执行AOF rewrite; redis在执行AOF rewrite的时候不会生成新的RDB;* 如果redis正在生成新的RDB文件，此时用户执行```bgrewriteaof```命令手动重写AOF文件，那么等RDB快照生成之后，才会去执行AOF rewrite；* 同时有RDB文件和AOF日志文件，那么redis重启的时候，会**优先使用AOF**进行数据恢复，因为其中的日志更完整。---### redis 持久化机制的配置 ######################### 通用 ######################### 持久化文件(包括RDB文件和AOF文件)的存储目录，默认.dir dir /home/hadoop/data/redis/6379 ######################### RDB ######################### RDB文件的文件名称，默认dump.rdbdbfilename dump.rdb 生成RDB文件的策略，默认为以下3种，意思是：每隔60s(1min)，如果有超过10000个key发生了变化，就写一份新的RDB文件每隔300s(5min)，如果有超过10个key发生了变化，就写一份新的RDB文件每隔900s(15min)，如果有超过1个key发生了变化，就写一份新的RDB文件配置多种策略可以同时生效，无论满足哪一种条件都会写一份新的RDB文件save 900 1save 300 10save 60 10000 是否开启RDB文件压缩，该功能可以节约磁盘空间，默认为yesrdbcompression yes 在写入文件和读取文件时是否开启rdb文件检查，检查是否有无损坏如果在启动时检查发现文件损坏，则停止启动，默认yesrdbchecksum yes ######################### AOF ######################### 是否开启AOF机制，默认为noappendonly yes AOF文件的名称，默认为appendonly.aofappendfilename “appendonly.aof” fsync的策略，默认为everyseceverysec：每秒fsync一次no：redis不主动fsync，完全交由操作系统决定always：1条指令fsync一次appendfsync everysec AOF文件rewrite策略当上一次重写后的AOF文件的增长比例达到100%比如上一次重写AOF文件后，新文件大小为128M当新文件再次增长了100%，达到了256M并且增长了100%后的文件的大小大于64M，那么开始重写AOF文件auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb 是否加载破损的AOF文件，默认为yes，如果设置为no那么redis启动时如果发现AOF文件破损，就会报错并且拒绝启动redis服务。aof-load-truncated yes ######################### 混合持久化 ######################### 是否开启混合持久化机制，默认为noaof-use-rdb-preamble no 12345678&gt; 建议配置：把```appendonly```设置为yes，根据需要修改dir，其他均保持默认即可。#### 其他相关命令* 手动生成新的RDB文件 阻塞主进程，直到生成新的RDB文件save 异步生成RDB文件，fork子进程去生成新的RDB文件，主进程不阻塞bgsave 123* 手动重写AOF文件 bgrewriteaof 123* 停止redis服务 安全停止redis服务，在停止之前会生成一份新的RDB文件redis-cli SHUTDOWN 不安全，会造成数据丢失kill -9 redis_pid 123* 检查持久化文件 检查AOF文件redis-check-aof /your/path/appendonly.aof 检查RDB文件redis-check-rdb /your/path/dump.rdb 123* 修复AOF文件 如果redis在append数据到AOF文件时，机器宕机了，可能会导致AOF文件破损，使用以下命令修复AOF文件$REDIS_HOME/bin/redis-check-aof –fix 123查看持久化信息 查看持久化信息info Persistence 查看状态信息info stats```","categories":[{"name":"Java","slug":"Java","permalink":"https://zhaoshuchao.top/categories/Java/"},{"name":"Redis","slug":"Java/Redis","permalink":"https://zhaoshuchao.top/categories/Java/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://zhaoshuchao.top/tags/Redis/"}]},{"title":"Redis基础知识","slug":"Redis基础知识","date":"2021-06-02T08:23:39.000Z","updated":"2021-06-02T08:33:10.180Z","comments":true,"path":"2021/06/02/Redis基础知识/","link":"","permalink":"https://zhaoshuchao.top/2021/06/02/Redis%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","excerpt":"","text":"Redis概念Redis是一个使用C语言编写的，开源的高性能非关系型的数据库，键值对形式 应用场景在日常的 Web 应用对数据库的访问中，读操作的次数远超写操作，比例大概在 1:9 到 3:7，所以需要读的可能性是比写的可能大得多的。当我们使用 SQL 语句去数据库进行读写操作时，数据库就会 去磁盘把对应的数据索引取回来，这是一个相对较慢的过程。 如果我们把数据放在 Redis 中，也就是直接放在内存之中，让服务端直接去读取内存中的数据，那么这样 速度 明显就会快上不少 *(高性能)*，并且会 极大减小数据库的压力 *(特别是在高并发情况下)*。 但是使用内存进行数据存储开销也是比较大的，限于成本 的原因，一般我们只是使用 Redis 存储一些 常用和主要的数据，比如用户登录的信息等。 高并发的情况下，MySQL不适合存取数据，效率太低。 做缓存，比如我们将用户登陆信息存进redis,这样我们可以更加方便的管理用户的权限，将用户信息存进Redis中，每次用户登陆的时候在http请求头Header中携带一个token ,访问其他地址的时候，后端直接用这个token解析用户数据，判断是否有权限访问。 数据类型常用字符串Stringstring 类型是 Redis 最基本的数据类型，string 类型的值最大能存储 512MB。 常用命令：set、get、decr、incr、mget等。 字典HashRedis hash 是一个键值(key=&gt;value)对集合；是一个 string 类型的 field 和 value 的映射表，hash 特别适合用于存储对象。 常用命令：hget、hset、hgetall等。 应用场景：存储一些结构化的数据，比如用户的昵称、年龄、性别、积分等，存储一个用户信息对象数据。 当然也可以JSON序列号一下存入string 列表ListRedis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。 list类型经常会被用于消息队列的服务，以完成多程序之间的消息交换。 常用命令：lpush、rpush、lpop、rpop、lrange等。 集合SetRedis的Set是string类型的无序集合。和列表一样，在执行插入和删除和判断是否存在某元素时，效率是很高的。集合最大的优势在于可以进行交集并集差集操作 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)。 应用场景： 1、利用交集求共同好友。 2、利用唯一性，可以统计访问网站的所有独立IP。 3、好友推荐的时候根据tag求交集，大于某个threshold（临界值的）就可以推荐。 常用命令：sadd、spop、smembers、sunion等。 有序集合SortedSet。Redis zset 和 set 一样也是string类型元素的集合,且不允许重复的成员。 不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。 zset的成员是唯一的,但分数(score)却可以重复。 sorted set是插入有序的，即自动排序。 常用命令：zadd、zrange、zrem、zcard等。 当你需要一个有序的并且不重复的集合列表时，那么可以选择sorted set数据结构。 应用举例： （1）例如存储全班同学的成绩，其集合value可以是同学的学号，而score就可以是成绩。（2）排行榜应用，根据得分列出topN的用户等。 扩展HyperLogLog基数统计(Cardinality Counting) 通常是用来统计一个集合中不重复的元素个数。 HyperLogLog是一个概率算法。会有一定的误差性。 一个 HyperLogLog 实际占用的空间大约是 12 KB，但 Redis 对于内存的优化非常变态，当 计数比较小 的时候，大多数桶的计数值都是 零，这个时候 Redis 就会适当节约空间，转换成另外一种 稀疏存储方式，与之相对的，正常的存储模式叫做 密集存储，这种方式会恒定地占用 12 KB。 HyperLoglog 也只需要 12 K 内存，在 标准误差 0.81% 的前提下，能够统计 2的64次方 个数据！ 在大量统计数据的时候可以替代Set，极大极大的压缩了内存占用情况 HyperLogLog 提供了两个指令 PFADD 和 PFCOUNT，字面意思就是一个是增加，另一个是获取计数。PFADD 和 set 集合的 SADD 的用法是一样的，来一个用户 ID，就将用户 ID 塞进去就是，PFCOUNT 和 SCARD 的用法是一致的，直接获取计数值 参考：https://www.wmyskxz.com/2020/03/02/reids-4-shen-qi-de-hyperloglog-jie-jue-tong-ji-wen-ti/ GeoHash这是业界比较通用的，用于 地理位置距离排序 的一个算法，Redis 也采用了这样的算法。GeoHash 算法将 二维的经纬度 数据映射到 一维 的整数，这样所有的元素都将在挂载到一条线上，距离靠近的二维坐标映射到一维后的点之间距离也会很接近。当我们想要计算 「附近的人时」，首先将目标位置映射到这条线上，然后在这个一维的线上获取附近的点就行了。 它的核心思想就是把整个地球看成是一个 二维的平面，然后把这个平面不断地等分成一个一个小的方格，每一个 坐标元素都位于其中的 唯一一个方格 中，等分之后的 方格越小，那么坐标也就 越精确 这个数据类型可以实现一些功能，如附近的人，附近的公司等等操作 参考：https://www.wmyskxz.com/2020/03/12/redis-6-geohash-cha-zhao-fu-jin-de-ren/ Pub/SubBloomFilter布隆过滤器（Bloom Filter） 是 1970 年由布隆提出的。它 实际上 是一个很长的二进制向量和一系列随机映射函数 *(下面详细说)*，实际上你也可以把它 简单理解 为一个不怎么精确的 set 结构，当你使用它的 contains 方法判断某个对象是否存在时，它可能会误判。但是布隆过滤器也不是特别不精确，只要参数设置的合理，它的精确度可以控制的相对足够精确，只会有小小的误判概率。 当布隆过滤器说某个值存在时，这个值 可能不存在；当它说不存在时，那么 一定不存在。打个比方，当它说不认识你时，那就是真的不认识，但是当它说认识你的时候，可能是因为你长得像它认识的另外一个朋友 *(脸长得有些相似)*，所以误判认识你。 布隆过滤器的使用场景 基于上述的功能，我们大致可以把布隆过滤器用于以下的场景之中： 大数据判断是否存在：这就可以实现出上述的去重功能，如果你的服务器内存足够大的话，那么使用 HashMap 可能是一个不错的解决方案，理论上时间复杂度可以达到 O(1 的级别，但是当数据量起来之后，还是只能考虑布隆过滤器。 解决缓存穿透：我们经常会把一些热点数据放在 Redis 中当作缓存，例如产品详情。 通常一个请求过来之后我们会先查询缓存，而不用直接读取数据库，这是提升性能最简单也是最普遍的做法，但是 如果一直请求一个不存在的缓存，那么此时一定不存在缓存，那就会有 大量请求直接打到数据库 上，造成 缓存穿透，布隆过滤器也可以用来解决此类问题。 爬虫/ 邮箱等系统的过滤：平时不知道你有没有注意到有一些正常的邮件也会被放进垃圾邮件目录中，这就是使用布隆过滤器 误判 导致的。 参考：https://www.wmyskxz.com/2020/03/11/redis-5-yi-ji-shu-ju-guo-lu-he-bu-long-guo-lu-qi/ https://juejin.cn/post/6844903982209449991 持久化什么是持久化？Redis 的数据 全部存储 在 内存 中，如果 突然宕机，数据就会全部丢失，因此必须有一套机制来保证 Redis 的数据不会因为故障而丢失，这种机制就是 Redis 的 持久化机制，它会将内存中的数据库状态 保存到磁盘 中。 持久化发生了什么我们来稍微考虑一下 Redis 作为一个 “内存数据库” 要做的关于持久化的事情。通常来说，从客户端发起请求开始，到服务器真实地写入磁盘，需要发生如下几件事情： 客户端向数据库 发送写命令 (数据在客户端的内存中) 数据库 接收 到客户端的 写请求 (数据在服务器的内存中) 数据库 调用系统 API 将数据写入磁盘 (数据在内核缓冲区中) 操作系统将 写缓冲区 传输到 磁盘控控制器 (数据在磁盘缓存中) 操作系统的磁盘控制器将数据 写入实际的物理媒介 中 (数据在磁盘中) Redis 中的两种持久化方式RDB（快照）Redis 快照 是最简单的 Redis 持久性模式。当满足特定条件时，它将生成数据集的时间点快照，例如，如果先前的快照是在 2 分钟前创建的，并且现在已经至少有 100 次新写入，则将创建一个新的快照。此条件可以由用户配置 Redis 实例来控制，也可以在运行时修改而无需重新启动服务器。快照作为包含整个数据集的单个 .rdb 文件生成。 AOF快照不是很持久。如果运行 Redis 的计算机停止运行，电源线出现故障或者您 kill -9 的实例意外发生，则写入 Redis 的最新数据将丢失。尽管这对于某些应用程序可能不是什么大问题，但有些使用案例具有充分的耐用性，在这些情况下，快照并不是可行的选择。 AOF(Append Only File - 仅追加文件) 它的工作方式非常简单：每次执行 修改内存 中数据集的写操作时，都会 记录 该操作。假设 AOF 日志记录了自 Redis 实例创建以来 所有的修改性指令序列，那么就可以通过对一个空的 Redis 实例 顺序执行所有的指令，也就是 「重放」，来恢复 Redis 当前实例的内存数据结构的状态。 Redis 4.0 的混合持久化重启 Redis 时，我们很少使用 rdb 来恢复内存状态，因为会丢失大量数据。我们通常使用 AOF 日志重放，但是重放 AOF 日志性能相对 rdb 来说要慢很多，这样在 Redis 实例很大的情况下，启动需要花费很长的时间。 Redis 4.0 为了解决这个问题，带来了一个新的持久化选项——混合持久化。将 rdb 文件的内容和增量的 AOF 日志文件存在一起。这里的 AOF 日志不再是全量的日志，而是 自持久化开始到持久化结束 的这段时间发生的增量 AOF 日志，通常这部分 AOF 日志很小： 于是在 Redis 重启的时候，可以先加载 rdb 的内容，然后再重放增量 AOF 日志就可以完全替代之前的 AOF 全量文件重放，重启效率因此大幅得到提升。 参考：https://www.wmyskxz.com/2020/03/13/redis-7-chi-jiu-hua-yi-wen-liao-jie/ RDB 和 AOF 各自有什么优缺点？RDB | 优点 只有一个文件 dump.rdb，方便持久化。 容灾性好，一个文件可以保存到安全的磁盘。 性能最大化，fork 子进程来完成写操作，让主进程继续处理命令，所以使 IO 最大化。使用单独子进程来进行持久化，主进程不会进行任何 IO 操作，保证了 Redis 的高性能 相对于数据集大时，比 AOF 的 启动效率 更高。 RDB | 缺点 数据安全性低。RDB 是间隔一段时间进行持久化，如果持久化之间 Redis 发生故障，会发生数据丢失。所以这种方式更适合数据要求不严谨的时候； AOF | 优点 数据安全，aof 持久化可以配置 appendfsync 属性，有 always，每进行一次命令操作就记录到 aof 文件中一次。 通过 append 模式写文件，即使中途服务器宕机，可以通过 redis-check-aof 工具解决数据一致性问题。 AOF 机制的 rewrite 模式。AOF 文件没被 rewrite 之前（文件过大时会对命令 进行合并重写），可以删除其中的某些命令（比如误操作的 flushall） AOF | 缺点 AOF 文件比 RDB 文件大，且 恢复速度慢。 数据集大 的时候，比 rdb 启动效率低。 恢复过程拷贝 AOF 文件到 Redis 的数据目录，启动 redis-server AOF 的数据恢复过程：Redis 虚拟一个客户端，读取 AOF 文件恢复 Redis 命令和参数，然后执行命令从而恢复数据，这些过程主要在 loadAppendOnlyFile() 中实现。 拷贝 RDB 文件到 Redis 的数据目录，启动 redis-server 即可，因为 RDB 文件和重启前保存的是真实数据而不是命令状态和参数。 分布式锁为何需要分布式锁一般情况下，我们使用分布式锁主要有两个场景： 避免不同节点重复相同的工作：比如用户执行了某个操作有可能不同节点会发送多封邮件； 避免破坏数据的正确性：如果两个节点在同一条数据上同时进行操作，可能会造成数据错误或不一致的情况出现； 原理基于 Redis 的单线程：由于 Redis 是单线程，所以命令会以串行的方式执行，并且本身提供了像 SETNX(set if not exists) 这样的指令，本身具有互斥性； 分布式锁必须满足一下条件 互斥性。在任意时刻，只有一个客户端能持有锁。 不会发生死锁。即使有一个客户端在持有锁的期间崩溃而没有主动解锁，也能保证后续其他客户端能加锁。 有容错性。只要大部分的Redis节点正常运行，客户端就可以加锁和解锁。 加锁和解锁必须是同一个客户端，客户端自己不能把别人加的锁给解了。 实现方式加锁就一行代码： jedis.set(String key, String value, String nxxx, String expx, int time)这个set()方法一共有五个形参： 第一个为key，我们使用key来当锁，因为key是唯一的。 第二个为value，我们传的是requestId，有key作为锁不就够了吗，为什么还要用到value？原因就是我们在上面讲到可靠性时，分布式锁要满足第四个条件，通过给value赋值为requestId，我们就知道这把锁是哪个请求加的了，在解锁的时候就可以有依据。requestId可以使用UUID.randomUUID().toString()方法生成。 第三个为nxxx，这个参数我们填的是NX，意思是SET IF NOT EXIST，即当key不存在时，我们进行set操作；若key已经存在，则不做任何操作； 第四个为expx，这个参数我们传的是PX，意思是我们要给这个key加一个过期的设置，具体时间由第五个参数决定。 第五个为time，与第四个参数相呼应，代表key的过期时间。 总的来说，执行上面的set()方法就只会导致两种结果： 当前没有锁（key不存在），那么就进行加锁操作，并对锁设置个有 效期，同时value表示加锁的客户端。 已有锁存在，不做任何操作。 我们的加锁代码满足我们可靠性里描述的三个条件。首先，set()加入了NX参数，可以保证如果已有key存在，则函数不会调用成功，也就是只有一个客户端能持有锁，满足互斥性。其次，由于我们对锁设置了过期时间，即使锁的持有者后续发生崩溃而没有解锁，锁也会因为到了过期时间而自动解锁（即key被删除），不会发生死锁。最后，因为我们将value赋值为requestId，代表加锁的客户端请求标识，那么在客户端在解锁的时候就可以进行校验是否是同一个客户端 正确的加锁方式：123456789101112131415161718192021222324public class RedisTool &#123; private static final String LOCK_SUCCESS = &quot;OK&quot;; private static final String SET_IF_NOT_EXIST = &quot;NX&quot;; private static final String SET_WITH_EXPIRE_TIME = &quot;PX&quot;; /** * 尝试获取分布式锁 * @param jedis Redis客户端 * @param lockKey 锁 * @param requestId 请求标识 * @param expireTime 超期时间 * @return 是否获取成功 */ public static boolean tryGetDistributedLock(Jedis jedis, String lockKey, String requestId, int expireTime) &#123; String result = jedis.set(lockKey, requestId, SET_IF_NOT_EXIST, SET_WITH_EXPIRE_TIME, expireTime); if (LOCK_SUCCESS.equals(result)) &#123; return true; &#125; return false; &#125;&#125; 错误的加锁方式：setnx()方法作用就是SET IF NOT EXIST，expire()方法就是给锁加一个过期时间。乍一看好像和前面的set()方法结果一样，然而由于这是两条Redis命令，不具有原子性，如果程序在执行完setnx()之后突然崩溃，导致锁没有设置过期时间。那么将会发生死锁。网上之所以有人这样实现，是因为低版本的jedis并不支持多参数的set()方法。 123456789public static void wrongGetLock1(Jedis jedis, String lockKey, String requestId, int expireTime) &#123; Long result = jedis.setnx(lockKey, requestId); if (result == 1) &#123; // 若在这里程序突然崩溃，则无法设置过期时间，将发生死锁 jedis.expire(lockKey, expireTime); &#125;&#125; 正确解锁方式1234567891011121314151617181920212223242526public class RedisTool &#123; private static final Long RELEASE_SUCCESS = 1L; /** * 释放分布式锁 * @param jedis Redis客户端 * @param lockKey 锁 * @param requestId 请求标识 * @return 是否释放成功 */ public static boolean releaseDistributedLock(Jedis jedis, String lockKey, String requestId) &#123; // LUA脚本 String script = &quot;if redis.call(&#x27;get&#x27;, KEYS[1]) == ARGV[1] then return redis.call(&#x27;del&#x27;, KEYS[1]) else return 0 end&quot;; Object result = jedis.eval(script, Collections.singletonList(lockKey), Collections.singletonList(requestId)); if (RELEASE_SUCCESS.equals(result)) &#123; return true; &#125; return false; &#125;&#125; 第一行代码，我们写了一个简单的Lua脚本代码，上一次见到这个编程第二行代码，我们将Lua代码传到jedis.eval()方法里，并使参数KEYS[1]赋值为lockKey，ARGV[1]赋值为requestId。eval()方法是将Lua代码交给Redis服务端执行。 那么这段Lua代码的功能是什么呢? 首先获取锁对应的value值，检查是否与requestId相等，如果相等则删除锁（解锁）。那么为什么要使用Lua语言来实现呢？因为要确保上述操作是原子性的。那么为什么执行eval()方法可以确保原子性，源于Redis的特性 简单来说，就是在eval命令执行Lua代码的时候，Lua代码将被当成一个命令去执行，并且直到eval命令执行完成，Redis才会执行其他命令。 错误示例这种解锁代码乍一看也是没问题，与正确姿势差不多，唯一区别的是分成两条命令去执行，代码如下： 12345678public static void wrongReleaseLock2(Jedis jedis, String lockKey, String requestId) &#123; // 判断加锁与解锁是不是同一个客户端 if (requestId.equals(jedis.get(lockKey))) &#123; // 若在此时，这把锁突然不是这个客户端的，则会误解锁 jedis.del(lockKey); &#125;&#125; 问题在于如果调用jedis.del()方法的时候，这把锁已经不属于当前客户端的时候会解除他人加的锁。那么是否真的有这种场景？答案是肯定的，比如客户端A加锁，一段时间之后客户端A解锁，在执行jedis.del()之前，锁突然过期了，此时客户端B尝试加锁成功，然后客户端A再执行del()方法，则将客户端B的锁给解除了。 Redis 分布式锁的问题1）锁超时假设现在我们有两台平行的服务 A B，其中 A 服务在 获取锁之后 由于未知神秘力量突然 挂了，那么 B 服务就永远无法获取到锁了： 所以我们需要额外设置一个超时时间，来保证服务的可用性。 但是另一个问题随即而来：如果在加锁和释放锁之间的逻辑执行得太长，以至于超出了锁的超时限制，也会出现问题。因为这时候第一个线程持有锁过期了，而临界区的逻辑还没有执行完，与此同时第二个线程就提前拥有了这把锁，导致临界区的代码不能得到严格的串行执行。 为了避免这个问题，Redis 分布式锁不要用于较长时间的任务。如果真的偶尔出现了问题，造成的数据小错乱可能就需要人工的干预。 有一个稍微安全一点的方案是 将锁的 value 值设置为一个随机数，释放锁时先匹配随机数是否一致，然后再删除 key，这是为了 确保当前线程占有的锁不会被其他线程释放，除非这个锁是因为过期了而被服务器自动释放的。 但是匹配 value 和删除 key 在 Redis 中并不是一个原子性的操作，也没有类似保证原子性的指令，所以可能需要使用像 Lua 这样的脚本来处理了，因为 Lua 脚本可以 保证多个指令的原子性执行。 2）单点/多点问题如果 Redis 采用单机部署模式，那就意味着当 Redis 故障了，就会导致整个服务不可用。 而如果采用主从模式部署，我们想象一个这样的场景：服务 A 申请到一把锁之后，如果作为主机的 Redis 宕机了，那么 服务 B 在申请锁的时候就会从从机那里获取到这把锁，为了解决这个问题，Redis 作者提出了一种 RedLock 红锁 的算法 *(Redission 同 Jedis)*： 123456789// 三个 Redis 集群RLock lock1 = redissionInstance1.getLock(&quot;lock1&quot;);RLock lock2 = redissionInstance2.getLock(&quot;lock2&quot;);RLock lock3 = redissionInstance3.getLock(&quot;lock3&quot;);RedissionRedLock lock = new RedissionLock(lock1, lock2, lock2);lock.lock();// do something....lock.unlock(); 缓存雪崩，击穿，穿透缓存雪崩产生的原因 同一时间内，大量Redis中的缓存同时失效。 举个简单的例子：如果所有首页的Key失效时间都是12小时，中午12点刷新的，我零点有个秒杀活动大量用户涌入，假设当时每秒 6000 个请求，本来缓存在可以扛住每秒 5000 个请求，但是缓存当时所有的Key都失效了。此时 1 秒 6000 个请求全部落数据库，数据库必然扛不住。这就是我理解的缓存雪崩。 解决方式 处理缓存雪崩简单，在批量往Redis存数据的时候，把每个Key的失效时间都加个随机值就好了，这样可以保证数据不会在同一时间大面积失效。 1setRedis（Key，value，time + Math.random() * 10000）； 缓存穿透缓存穿透是指缓存和数据库中都没有的数据，而用户不断发起请求，我们数据库的 id 都是1开始自增上去的，如发起为id值为 -1 的数据或 id 为特别大不存在的数据。这时的用户很可能是攻击者，攻击会导致数据库压力过大，严重会击垮数据库。 像这种你如果不对参数做校验，数据库id都是大于0的，我一直用小于0的参数去请求你，每次都能绕开Redis直接打到数据库，数据库也查不到，每次都这样，并发高点就容易崩掉了。 解决方式 在接口层增加校验，比如用户鉴权校验，参数做校验，不合法的参数直接代码Return，比如：id 做基础校验，id &lt;=0的直接拦截等 这里我想提的一点就是，我们在开发程序的时候都要有一颗“不信任”的心，就是不要相信任何调用方，比如你提供了API接口出去，你有这几个参数，那我觉得作为被调用方，任何可能的参数情况都应该被考虑到，做校验，因为你不相信调用你的人，你不知道他会传什么参数给你。 从缓存取不到的数据，在数据库中也没有取到，这时也可以将对应Key的Value对写为null、位置错误、稍后重试这样的值具体取啥问产品，或者看具体的场景，缓存有效时间可以设置短点，如30秒（设置太长会导致正常情况也没法使用）。 Redis还有一个高级用法布隆过滤器（Bloom Filter）这个也能很好的防止缓存穿透的发生，他的原理也很简单就是利用高效的数据结构和算法快速判断出你这个Key是否在数据库中存在，不存在你return就好了，存在你就去查了DB刷新KV再return。 缓存击穿缓存击穿嘛，这个跟缓存雪崩有点像，但是又有一点不一样，缓存雪崩是因为大面积的缓存失效，打崩了DB，而缓存击穿不同的是缓存击穿是指一个Key非常热点，在不停的扛着大并发，大并发集中对这一个点进行访问，当这个Key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库，就像在一个完好无损的桶上凿开了一个洞。 解决：设置热点数据永远不过期。或者加上互斥锁就能搞定了 Redis 淘汰策略Redis key过期的方式有三种： 惰性删除：当读/写一个已经过期的key时，会触发惰性删除策略，直接删除掉这个过期key（无法保证冷数据被及时删掉） 定期删除：Redis会定期主动淘汰一批已过期的key（随机抽取一批key检查） 内存淘汰机制：当前已用内存超过maxmemory限定时，触发主动清理策略 如果没有设置有效期，即使内存用完，redis 自动回收机制也是看设置了有效期的，不会动没有设定有效期的，如果清理后内存还是满的，就不再接受写操作。 redis最大内存不足”时,数据清除策略,默认为”volatile-lru”。 策略 描述 volatile-lru 从已设置过期时间的 KV 集中优先对最近最少使用(less recently used)的数据淘汰（默认） volitile-ttl 从已设置过期时间的 KV 集中优先对剩余时间短(time to live)的数据淘汰 random 从已设置过期时间的 KV 集中随机选择数据淘汰 allkeys-lru 从已设置过期时间的 KV 集中随机选择数据淘汰 allKeys-random 从所有 KV 集中随机选择数据淘汰 noeviction 从所有 KV 集中随机选择数据淘汰 4.0 版本后增加以下两种 volatile-lfu：从已设置过期时间的数据集(server.db[i].expires)中挑选最不经常使用的数据淘汰 allkeys-lfu：当内存不足以容纳新写入数据时，在键空间中，移除最不经常使用的 key Redis 为什么早期版本选择单线程？官方解释因为 Redis 是基于内存的操作，CPU 不是 Redis 的瓶颈，Redis 的瓶颈最有可能是 机器内存的大小 或者 网络带宽。既然单线程容易实现，而且 CPU 不会成为瓶颈，那就顺理成章地采用单线程的方案了。 简单总结一下 使用单线程模型能带来更好的 可维护性，方便开发和调试； 使用单线程模型也能 并发 的处理客户端的请求；*(I/O 多路复用机制)* Redis 服务中运行的绝大多数操作的 性能瓶颈都不是 CPU； Redis 为什么这么快？ 纯内存操作：读取不需要进行磁盘 I/O，所以比传统数据库要快上不少；*(但不要有误区说磁盘就一定慢，例如 Kafka 就是使用磁盘顺序读取但仍然较快)* 单线程，无锁竞争：这保证了没有线程的上下文切换，不会因为多线程的一些操作而降低性能； 多路 I/O 复用模型，非阻塞 I/O：采用多路 I/O 复用技术可以让单个线程高效的处理多个网络连接请求（尽量减少网络 IO 的时间消耗）； 高效的数据结构，加上底层做了大量优化：Redis 对于底层的数据结构和内存占用做了大量的优化，例如不同长度的字符串使用不同的结构体表示，HyperLogLog 的密集型存储结构等等.. Other缓存一致性 读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。 更新的时候，先更新数据库，然后再删除缓存。 redis的线程模型Redis 内部使用文件事件处理器 file event handler，这个文件事件处理器是单线程的，所以 Redis 才叫做单线程的模型。它采用 IO 多路复用机制同时监听多个 Socket，根据 Socket 上的事件来选择对应的事件处理器进行处理。","categories":[{"name":"Java","slug":"Java","permalink":"https://zhaoshuchao.top/categories/Java/"},{"name":"Redis","slug":"Java/Redis","permalink":"https://zhaoshuchao.top/categories/Java/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://zhaoshuchao.top/tags/Redis/"}]},{"title":"SQL优化","slug":"SQL优化","date":"2021-06-02T08:18:16.000Z","updated":"2021-06-02T08:29:37.577Z","comments":true,"path":"2021/06/02/SQL优化/","link":"","permalink":"https://zhaoshuchao.top/2021/06/02/SQL%E4%BC%98%E5%8C%96/","excerpt":"","text":"简单的SQL优化 对查询优化，应尽量避免全表扫描，首先应考虑在where以及order by涉及到的列上建立索引.只返回有用的字段，不要全部返回(不要用select * from 返回的字段一点要有用，尽量实现覆盖索引，避免回表操作) 应尽量避免在where子句中对字段进行null值判断，否则将导致放弃使用索引而进行全表扫描.（建议创建字段时设置一个默认值，避免null值,比如字符默认为空串，数字默认为0） 应尽量避免在where子句中使用or来连接条件，否则将导致引擎放弃使用而进行全表扫描 1select id from t where num = 10 or num = 20 可以使用union函数查询，也可以这样查询： 123select id from t where num = 10 union allselect id from t where num = 20 in 和not in要小心使用，避免全表扫描 对于连续的数值，能用 between 就不要用 in 了： 12select id from t where num in(1,2,3) 可能造成全表扫描select id from t where num between 1 and 3 匹配索引 避免在where子句中对字段进行表达式操作 应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描 索引遵循最左匹配原则，从左到右命中索引，碰到 &gt;,&lt;,like等等运算操作则会放弃索引，查询尽量避免，如果不能避免，一定要放到最后，尽量命中更多的索引 自己的SQL调优过程 数据库调优其实一般情况都是我们的SQL调优，SQL的调优就可以解决大部分问题了(但是这些调优我们应该尽量在开发时就注意到这些问题，在开发阶段避免大部分的问题)，还有一点比较重要的就是SQL执行环节的调优 SQL的执行过程： 客户端 –&gt; 连接器 –&gt; 分析器 –&gt; 优化器 –&gt; 执行器 –&gt; 引擎 –&gt; 查询结果 我们所谓的调优也就是在，执行器执行之前的分析器，优化器阶段完成的，那我们开发工作中怎么去调优的呢？ 一般在开发涉及SQL的业务都会去本地环境跑一遍SQL，用explain去看一下执行计划，看看分析的结果是否符合自己的预期，用没用到相关的索引，然后再去线上环境跑一下看看执行时间（这里只有查询语句，修改语句也无法在线上执行）。 排除缓存干扰因为在MySQL8.0之前我们的数据库是存在缓存这样的情况的，我之前就被坑过，因为存在缓存，我发现我sql怎么执行都是很快，当然第一次其实不快但是我没注意到，以至于上线后因为缓存经常失效，导致rt（Response time）时高时低。 后面就发现了是缓存的问题，我们在执行SQL的时候，记得加上SQL_NO_CACHE 去跑SQL，这样跑出来的时间就是真实的查询时间了。 我说一下为什么缓存会失效，而且是经常失效。 如果我们当前的MySQL版本支持缓存而且我们又开启了缓存，那每次请求的查询语句和结果都会以key-value的形式缓存在内存中的，大家也看到我们的结构图了，一个请求会先去看缓存是否存在，不存在才会走解析器。 缓存失效比较频繁的原因就是，只要我们一对表进行更新，那这个表所有的缓存都会被清空，其实我们很少存在不更新的表，特别是我之前的电商场景，可能静态表可以用到缓存，但是我们都走大数据离线分析，缓存也就没用了。 大家如果是8.0以上的版本就不用担心这个问题，如果是8.0之下的版本，记得排除缓存的干扰。 Explain最开始提到了用执行计划去分析，explain是SQL调优都会回答到的, 因为这基本上是写SQL的必备操作 使用EXPLAIN关键字可以模拟优化器执行SQL查询语句，从而知道MySQL是如何处理你的SQL语句的。分析你的查询语句或是表结构的性能瓶颈。 通过EXPLAIN，我们可以分析出以下结果：(重要) 表的读取顺序 数据读取操作的操作类型 哪些索引可以使用 哪些索引被实际使用 表之间的引用 每张表有多少行被优化器查询 使用方法就是 EXPLAIN +SQL语句 各个字段的含义 id : select查询的序列号，包含一组数字，表示查询中执行select子句或操作表的顺序 id相同，执行顺序由上至下 id不同，如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行 select_type : 用来表示查询的类型，主要是用于区别普通查询、联合查询、子查询等的复杂查询。 SIMPLE 简单的select查询，查询中不包含子查询或者UNION PRIMARY 查询中若包含任何复杂的子部分，最外层查询则被标记为PRIMARY SUBQUERY 在SELECT或WHERE列表中包含了子查询 DERIVED 在FROM列表中包含的子查询被标记为DERIVED（衍生），MySQL会递归执行这些子查询，把结果放在临时表中 UNION 若第二个SELECT出现在UNION之后，则被标记为UNION：若UNION包含在FROM子句的子查询中，外层SELECT将被标记为：DERIVED UNION RESULT 从UNION表获取结果的SELECT table ：指的就是当前执行的表 type：type所显示的是查询使用了哪种类型，type包含的类型包括如下图所示的几种： 从最好到最差依次是： 1system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; all 一般来说，得保证查询至少达到range级别，最好能达到ref。 system : 表只有一行记录（等于系统表），这是const类型的特列，平时不会出现，这个也可以忽略不计 const : 表示通过索引一次就找到了，const用于比较primary key 或者unique索引。因为只匹配一行数据，所以很快。如将主键置于where列表中，MySQL就能将该查询转换为一个常量。(如where id = 1) eq_ref : ** 唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配**。常见于主键或唯一索引扫描 ref : 非唯一性索引扫描，返回匹配某个单独值的所有行，本质上也是一种索引访问，它返回所有匹配某个单独值的行，然而，它可能会找到多个符合条件的行，所以他应该属于查找和扫描的混合体。 **range: ** 只检索给定范围的行，使用一个索引来选择行，key列显示使用了哪个索引，一般就是在你的where语句中出现between、&lt; 、&gt;、in等的查询，这种范围扫描索引比全表扫描要好，因为它只需要开始于索引的某一点，而结束于另一点，不用扫描全部索引。 index : Full Index Scan，Index与All区别为index类型只遍历索引树。这通常比ALL快，因为索引文件通常比数据文件小。（也就是说虽然all和Index都是读全表，但index是从索引中读取的，而all是从硬盘读取的） all : Full Table Scan 将遍历全表以找到匹配的行 possible_keys : 显示可能应用在这张表中的索引，一个或多个。查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询实际使用。 Key:实际使用的索引，如果为NULL，则没有使用索引。（可能原因包括没有建立索引或索引失效） 查询中若使用了覆盖索引（select 后要查询的字段刚好和创建的索引字段完全相同），则该索引仅出现在key列表中 key_len: 表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度，在不损失精确性的情况下，长度越短越好。key_len显示的值为索引字段的最大可能长度，并非实际使用长度，即key_len是根据表定义计算而得，不是通过表内检索出的。 ref:显示索引的那一列被使用了，如果可能的话，最好是一个常数。哪些列或常量被用于查找索引列上的值。 rows: 根据表统计信息及索引选用情况，大致估算出找到所需的记录所需要读取的行数，也就是说，用的越少越好 Extra:包含不适合在其他列中显式但十分重要的额外信息 统计这个统计的行数就是完全对的么？索引一定会走到最优索引么？ 行数只是一个接近的数字，不是完全正确的，索引也不一定就是走最优的，是可能走错的。 我的总行数大概有10W行，但是我去用explain去分析sql的时候，就会发现只得到了9.4W，为啥行数只是个近视值呢？ 因为MySQL中数据的单位都是页，MySQL又采用了采样统计的方法，采样统计的时候，InnoDB默认会选择N个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。 我们数据是一直在变的，所以索引的统计信息也是会变的，会根据一个阈值，重新做统计。 至于MySQL索引可能走错也很好理解，如果走A索引要扫描100行，B索引有只要20行，但是他可能选择走A索引，你可能会想MySQL是不是有病啊，其实不是的。 一般走错都是因为优化器在选择的时候发现，走A索引没有额外的代价，比如走B索引并不能直接拿到我们的值，还需要回到主键索引才可以拿到，多了一次回表的过程，这个也是会被优化器考虑进去的。 他发现走A索引不需要回表，没有额外的开销，所有他选错了。 如果是上面的统计信息错了，那简单，我们用analyze table tablename 就可以重新统计索引信息了，所以在实践中，如果你发现explain的结果预估的rows值跟实际情况差距比较大，可以采用这个方法来处理。 还有一个方法就是force index强制走正确的索引，或者优化SQL，最后实在不行，可以新建索引，或者删掉错误的索引。 减少回表根据自己返回的字段需要，创建合适的联合索引，实现覆盖索引。就可以减少回表了。 回表就是，辅助索引在查询的时候，不会直接查询到对应的数据行，而是查询到该行的主键ID，然后在通过ID进行主键索引查询，查询到对应的数据行。这个通过主键ID在查一次的过程就叫回表。 如果我们要查询三个字段的值，可以根据这三个字段创建一个联合索引，这样就实现了索引覆盖，查询的数据都在索引行中，这样就不用再进行一次回表操作了 PS: 创建联合索引的时候要遵循 最左匹配原则 索引下推已经知道了前缀索引规则，那我就说一个官方帮我们优化的东西，索引下推。 select * from itemcenter where name like &#39;赵%&#39; and hight = 175 and age = 20; 所以这个语句在搜索索引树的时候，只能用 “敖”，找到第一个满足条件的记录ID1，当然，这还不错，总比全表扫描要好。 然后呢？ 当然是判断其他条件是否满足，比如size。 在MySQL 5.6之前，只能从ID1开始一个个回表，到主键索引上找出数据行，再对比字段值。 而MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。 条件字段函数操作日常开发过程中，大家经常对很多字段进行函数操作，如果对日期字段操作，浮点字符操作等等，大家需要注意的是，如果对字段做了函数计算，就用不上索引了，这是MySQL的规定。 对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。 唯一索引普通索引选择难题核心是需要回答到change buffer，那change buffer又是个什么东西呢？ 当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InooDB会将这些更新操作缓存在change buffer中，这样就不需要从磁盘中读入这个数据页了。 在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中与这个页有关的操作，通过这种方式就能保证这个数据逻辑的正确性。 需要说明的是，虽然名字叫作change buffer，实际上它是可以持久化的数据。也就是说，change buffer在内存中有拷贝，也会被写入到磁盘上。 将change buffer中的操作应用到原数据页，得到最新结果的过程称为merge。 除了访问这个数据页会触发merge外，系统有后台线程会定期merge。在数据库正常关闭（shutdown）的过程中，也会执行merge操作。 显然，如果能够将更新操作先记录在change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用buffer pool的，所以这种方式还能够避免占用内存，提高内存利用率 那么，什么条件下可以使用change buffer呢？ 对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。 要判断表中是否存在这个数据，而这必须要将数据页读入内存才能判断，如果都已经读入到内存了，那直接更新内存会更快，就没必要使用change buffer了。 因此，唯一索引的更新就不能使用change buffer，实际上也只有普通索引可以使用。 change buffer用的是buffer pool里的内存，因此不能无限增大，change buffer的大小，可以通过参数innodb_change_buffer_max_size来动态设置，这个参数设置为50的时候，表示change buffer的大小最多只能占用buffer pool的50%。 将数据从磁盘读入内存涉及随机IO的访问，是数据库里面成本最高的操作之一，change buffer因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的。 change buffer的使用场景因为merge的时候是真正进行数据更新的时刻，而change buffer的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做merge之前，change buffer记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。 因此，对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时change buffer的使用效果最好，这种业务模型常见的就是账单类、日志类的系统。 反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在change buffer，但之后由于马上要访问这个数据页，会立即触发merge过程。这样随机访问IO的次数不会减少，反而增加了change buffer的维护代价，所以，对于这种业务模式来说，change buffer反而起到了副作用。 在不考虑数据重复的情况下，数据重复情况，必须使用普通索引 总的来说就是，对于写多读少的业务来说，可以采用普通索引（因为这样每次修改的数据都会记录在changeBuffer中，merge次次数很少） 对于写入马上查询的业务来说，使用唯一索引（因为这个如果使用普通索引，会频繁的造成merge,而且需要维护一个changeBuffer） flush脏页刷新机制数据库调优总结：简单的SQL优化：查询时的返回字段优化，where条件优化，null值判断；创建索引：查询频繁以及数据重复较小的字段创建索引；创建的索引失效情况，联合索引重点突出最左匹配原则 自己优化流程： 分析SQL执行流程 用EXPLAIN分析具体某一条SQL执行流程 排查问题时去掉SQL缓存 怎么样优化： 覆盖索引（减少回表） 联合索引（目的就是为了能够覆盖索引，提高效率）（创建联合索引时遵循最左匹配原则）合理安排SQL顺序 单一索引时 根据业务选择唯一索引还是普通索引（根据change buffer 多写少读用普通索引 写后随即读取用唯一索引） 还有一点要注意的是MySQL选择utf-8字符集要选用uft8mb4 索引字段不要做函数操作 MySQL的引擎Innodb引擎 Innodb引擎提供了对数据库ACID事务的支持，并且实现了SQL标准的四种隔离级别。 该引擎还提供了行级锁和外键约束，它的设计目标是处理大容量数据库系统，它本身其实就是基于MySQL后台的完整数据库系统，MySQL运行时Innodb会在内存中建立缓冲池，用于缓冲数据和索引。 当需要使用数据库事务时，该引擎当然是首选。由于锁的粒度更小，写操作不会锁定全表，所以在并发较高时，使用Innodb引擎会提升效率。 但是使用行级锁也不是绝对的，如果在执行一个SQL语句时MySQL不能确定要扫描的范围，InnoDB表同样会锁全表。 MyIASM引擎 它没有提供对数据库事务的支持，也不支持行级锁和外键，因此当INSERT(插入)或UPDATE(更新)数据时即写操作需要锁定整个表，效率便会低一些。 不过和Innodb不同，MyIASM中存储了表的行数，于是SELECT COUNT(*) FROM TABLE时只需要直接读取已经保存好的值而不需要进行全表扫描。如果表的读操作远远多于写操作且不需要数据库事务的支持，那么MyIASM也是很好的选择。 主要区别： MyIASM是非事务安全的，而InnoDB是事务安全的 MyIASM锁的粒度是表级的，而InnoDB支持行级锁 InnoDB支持外键，而MyISAM不支持。对一个包含外键的InnoDB表转为MYISAM会失败 InnoDB是聚集索引，使用B+Tree作为索引结构，数据文件是和（主键）索引绑在一起的（表数据文件本身就是按B+Tree组织的一个索引结构），必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，因为主键太大，其他索引也都会很大。 MyISAM是非聚集索引，也是使用B+Tree作为索引结构，索引和数据文件是分离的，索引保存的是数据文件的指针。主键索引和辅助索引是独立的。 MyIASM表保存成文件形式，跨平台使用更加方便 InnoDB表必须有唯一索引（如主键）（用户没有指定的话会自己找/生产一个隐藏列Row_id来充当默认主键），而Myisam可以没有 如何选择 是否要支持事务，如果要请选择innodb，如果不需要可以考虑MyISAM； 如果表中绝大多数都只是读查询，可以考虑MyISAM，如果既有读也有写，请使用InnoDB。 系统奔溃后，MyISAM恢复起来更困难，能否接受； MySQL5.5版本开始Innodb已经成为Mysql的默认引擎(之前是MyISAM)，说明其优势是有目共睹的，如果你不知道用什么，那就用InnoDB，至少不会差。 InnoDB为什么推荐使用自增ID作为主键？ 答：自增ID可以保证每次插入时B+索引是从右边扩展的，可以避免B+树和频繁合并和分裂（对比使用UUID）。如果使用字符串主键和随机主键，会使得数据随机插入，效率比较差。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://zhaoshuchao.top/categories/MySQL/"},{"name":"SQL优化","slug":"MySQL/SQL优化","permalink":"https://zhaoshuchao.top/categories/MySQL/SQL%E4%BC%98%E5%8C%96/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://zhaoshuchao.top/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"https://zhaoshuchao.top/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"SQL语句","slug":"SQL语句","date":"2021-06-02T08:00:50.000Z","updated":"2021-06-02T08:30:19.163Z","comments":true,"path":"2021/06/02/SQL语句/","link":"","permalink":"https://zhaoshuchao.top/2021/06/02/SQL%E8%AF%AD%E5%8F%A5/","excerpt":"","text":"1234567891011121314151617181920212223242526# 直接将查询结果导入或复制到新创建的表CREATE TABLE n SELECT * FROM m;# 删除一个存在表DROP TABLE IF EXISTS m;# 更改存在表的名称ALTER TABLE n RENAME m;# 添加列ALTER TABLE table_name ADD 列名 INT（11）# 删除列ALTER TABLE mytable DROP COLUMN col;# 插入数据INSERT INTO mytable(col1, col2) VALUES(val1, val2);# 修改数据UPDATE mytable SET col = val WHERE id = 1;# DESTINCT:去重。相同值只会出现一次。它作用于所有列，也就是说**所有列的值都相同才算相同SELECT DISTINCT col1, col2 FROM mytable;# 添加索引ALTER TABLE n ADD INDEX i_age (age) 模糊查询通常使用拼接字符串形式： 1like concat(‘%’，#&#123;xxx&#125;,&#x27;%&#x27;) WHERE 过滤行，HAVING 过滤分组，行过滤应当先于分组过滤 除了汇总字段外，SELECT 语句中的每一字段都必须在 GROUP BY 子句中给出； 12345SELECT col, COUNT(*) AS numFROM mytableWHERE col &gt; 2GROUP BY colHAVING num &gt;= 2; UNION ：组合查询 使用 UNION 来组合两个查询，如果第一个查询返回 M 行，第二个查询返回 N 行，那么组合查询的结果一般为 M+N 行。 每个查询必须包含相同的列、表达式和聚集函数。 默认会去除相同行，如果需要保留相同行，使用 UNION ALL 只能包含一个 ORDER BY 子句，并且必须位于语句的最后。 **IFNULL(expr1,expr2)**：如果第一个参数不为空，则返回第一个参数，否则返回第二个参数。 **ISNULL(expr)**：判断是否是空，是空则返回1，否则返回0。 **IF(expr1,expr2,expr3)**：如果第一个表达式的值为TRUE（不为0或null），则返回第二个参数的值，否则返回第三个参数的值。 SQL的执行顺序12345678910(1) SELECT(2) DISTINCT &lt;select_list&gt;(3) FROM &lt;left_table&gt;(4) &lt;join_type&gt; JOIN &lt;right_table&gt;(5) ON &lt;join_condition&gt;(6) WHERE &lt;where_condition&gt;(7) GROUP BY &lt;group_by_list&gt;(8) HAVING &lt;having_condition&gt;(9) ORDER BY &lt;order_by_condition&gt;(10) LIMIT &lt;limit_number&gt; case when then else end语句用于查询满足多种条件的情况，类似java中的if…else，还有的就是用于进行行转列的查询，这个是放在select 子句后面的，是充当的是字段的作用。 具体用法就是：分为两种，一种是简单的函数形式，另一种就是表达式的形式。 简单的函数形式： 1case 字段 when 值 then 结果 else 其他情况 end； 表达式的形式： 1case when 字段=值（这里写表达式，例如 score=80） then 结果 else 其他情况 end； 注意：THEN后边的值与ELSE后边的值类型应一致，否则会报错 Other函数1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# 聚合函数SELECT count(id) AS total FROM n; # 总数SELECT sum(age) AS all_age FROM n; # 总和SELECT avg(age) AS all_age FROM n; # 平均值SELECT max(age) AS all_age FROM n; # 最大值SELECT min(age) AS all_age FROM n; # 最小值# 数学函数SELECT abs(-5); # 绝对值SELECT bin(15), oct(15), hex(15); # 二进制，八进制，十六进制SELECT pi(); # 圆周率3.141593SELECT ceil(5.5); # 大于x的最小整数值6SELECT floor(5.5); # 小于x的最大整数值5SELECT greatest(3,1,4,1,5,9,2,6); # 返回集合中最大的值9SELECT least(3,1,4,1,5,9,2,6); # 返回集合中最小的值1SELECT mod(5,3); # 余数2SELECT rand(); # 返回０到１内的随机值，每次不一样SELECT rand(5); # 提供一个参数(种子)使RAND()随机数生成器生成一个指定的值。SELECT round(1415.1415); # 四舍五入1415SELECT round(1415.1415, 3); # 四舍五入三位数1415.142SELECT round(1415.1415, -1); # 四舍五入整数位数1420SELECT truncate(1415.1415, 3); # 截短为3位小数1415.141SELECT truncate(1415.1415, -1); # 截短为-1位小数1410SELECT sign(-5); # 符号的值负数-1SELECT sign(5); # 符号的值正数1SELECT sqrt(9); # 平方根3SELECT sqrt(9); # 平方根3# 字符串函数SELECT concat(&#x27;a&#x27;, &#x27;p&#x27;, &#x27;p&#x27;, &#x27;le&#x27;); # 连接字符串-appleSELECT concat_ws(&#x27;,&#x27;, &#x27;a&#x27;, &#x27;p&#x27;, &#x27;p&#x27;, &#x27;le&#x27;); # 连接用&#x27;,&#x27;分割字符串-a,p,p,leSELECT insert(&#x27;chinese&#x27;, 3, 2, &#x27;IN&#x27;); # 将字符串&#x27;chinese&#x27;从3位置开始的2个字符替换为&#x27;IN&#x27;-chINeseSELECT left(&#x27;chinese&#x27;, 4); # 返回字符串&#x27;chinese&#x27;左边的4个字符-chinSELECT right(&#x27;chinese&#x27;, 3); # 返回字符串&#x27;chinese&#x27;右边的3个字符-eseSELECT substring(&#x27;chinese&#x27;, 3); # 返回字符串&#x27;chinese&#x27;第三个字符之后的子字符串-ineseSELECT substring(&#x27;chinese&#x27;, -3); # 返回字符串&#x27;chinese&#x27;倒数第三个字符之后的子字符串-eseSELECT substring(&#x27;chinese&#x27;, 3, 2); # 返回字符串&#x27;chinese&#x27;第三个字符之后的两个字符-inSELECT trim(&#x27; chinese &#x27;); # 切割字符串&#x27; chinese &#x27;两边的空字符-&#x27;chinese&#x27;SELECT ltrim(&#x27; chinese &#x27;); # 切割字符串&#x27; chinese &#x27;两边的空字符-&#x27;chinese &#x27;SELECT rtrim(&#x27; chinese &#x27;); # 切割字符串&#x27; chinese &#x27;两边的空字符-&#x27; chinese&#x27;SELECT repeat(&#x27;boy&#x27;, 3); # 重复字符&#x27;boy&#x27;三次-&#x27;boyboyboy&#x27;SELECT reverse(&#x27;chinese&#x27;); # 反向排序-&#x27;esenihc&#x27;SELECT length(&#x27;chinese&#x27;); # 返回字符串的长度-7SELECT upper(&#x27;chINese&#x27;), lower(&#x27;chINese&#x27;); # 大写小写 CHINESE chineseSELECT ucase(&#x27;chINese&#x27;), lcase(&#x27;chINese&#x27;); # 大写小写 CHINESE chineseSELECT position(&#x27;i&#x27; IN &#x27;chinese&#x27;); # 返回&#x27;i&#x27;在&#x27;chinese&#x27;的第一个位置-3SELECT position(&#x27;e&#x27; IN &#x27;chinese&#x27;); # 返回&#x27;i&#x27;在&#x27;chinese&#x27;的第一个位置-5SELECT strcmp(&#x27;abc&#x27;, &#x27;abd&#x27;); # 比较字符串，第一个参数小于第二个返回负数- -1SELECT strcmp(&#x27;abc&#x27;, &#x27;abb&#x27;); # 比较字符串，第一个参数大于第二个返回正数- 1# 时间函数SELECT current_date, current_time, now(); # 2018-01-13 12:33:43 2018-01-13 12:33:43SELECT hour(current_time), minute(current_time), second(current_time); # 12 31 34SELECT year(current_date), month(current_date), week(current_date); # 2018 1 1SELECT quarter(current_date); # 1SELECT monthname(current_date), dayname(current_date); # January SaturdaySELECT dayofweek(current_date), dayofmonth(current_date), dayofyear(current_date); # 7 13 13# 控制流函数SELECT if(3&gt;2, &#x27;t&#x27;, &#x27;f&#x27;), if(3&lt;2, &#x27;t&#x27;, &#x27;f&#x27;); # t fSELECT ifnull(NULL, &#x27;t&#x27;), ifnull(2, &#x27;t&#x27;); # t 2SELECT isnull(1), isnull(1/0); # 0 1 是null返回1，不是null返回0SELECT nullif(&#x27;a&#x27;, &#x27;a&#x27;), nullif(&#x27;a&#x27;, &#x27;b&#x27;); # null a 参数相同或成立返回null，不同或不成立则返回第一个参数SELECT CASE 2 WHEN 1 THEN &#x27;first&#x27; WHEN 2 THEN &#x27;second&#x27; WHEN 3 THEN &#x27;third&#x27; ELSE &#x27;other&#x27; END ; # second# 系统信息函数SELECT database(); # 当前数据库名-testSELECT connection_id(); # 当前用户id-306SELECT user(); # 当前用户-root@localhostSELECT version(); # 当前mysql版本SELECT found_rows(); # 返回上次查询的检索行数 用户1234567891011121314151617# 增加用户CREATE USER &#x27;test&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;test&#x27;;INSERT INTO mysql.user(Host, User, Password) VALUES (&#x27;localhost&#x27;, &#x27;test&#x27;, Password(&#x27;test&#x27;)); # 在用户表中插入用户信息，直接操作User表不推荐# 删除用户DROP USER &#x27;test&#x27;@&#x27;localhost&#x27;;DELETE FROM mysql.user WHERE User=&#x27;test&#x27; AND Host=&#x27;localhost&#x27;;FLUSH PRIVILEGES ;# 更改用户密码SET PASSWORD FOR &#x27;test&#x27;@&#x27;localhost&#x27; = PASSWORD(&#x27;test&#x27;);UPDATE mysql.user SET Password=Password(&#x27;t&#x27;) WHERE User=&#x27;test&#x27; AND Host=&#x27;localhost&#x27;;FLUSH PRIVILEGES ;# 用户授权GRANT ALL PRIVILEGES ON *.* TO test@localhost IDENTIFIED BY &#x27;test&#x27;;# 授予用&#x27;test&#x27;密码登陆成功的test@localhost用户操作所有数据库的所有表的所有的权限FLUSH PRIVILEGES ; # 刷新系统权限表,使授予权限生效# 撤销用户授权REVOKE DELETE ON *.* FROM &#x27;test&#x27;@&#x27;localhost&#x27;; # 取消该用户的删除权限","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://zhaoshuchao.top/categories/MySQL/"},{"name":"SQL语句","slug":"MySQL/SQL语句","permalink":"https://zhaoshuchao.top/categories/MySQL/SQL%E8%AF%AD%E5%8F%A5/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://zhaoshuchao.top/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"https://zhaoshuchao.top/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"MySQL中的索引","slug":"MySQL中的索引","date":"2021-06-02T07:59:04.000Z","updated":"2021-06-02T08:30:20.270Z","comments":true,"path":"2021/06/01/MySQL中的索引/","link":"","permalink":"https://zhaoshuchao.top/2021/06/01/MySQL%E4%B8%AD%E7%9A%84%E7%B4%A2%E5%BC%95/","excerpt":"","text":"什么是索引 帮助MySQL高效获取数据的数据结构。更通俗的说，数据库索引好比是一本书前面的目录，能加快数据库的查询速度。 一般来说索引本身也很大，不可能全部存储在内存中，因此索引往往是存储在磁盘上的文件中的（可能存储在单独的索引文件中，也可能和数据一起存储在数据文件中） 索引的优势和劣势优势 可以 提高数据检索的效率，降低数据库的IO成本，类似于书的目录。 通过索引列对数据进行排序，降低数据排序的成本，降低了CPU的消耗。 被索引的列会自动进行排序，包括【单列索引】和【组合索引】，只是组合索引的排序要复杂一些。 如果按照索引列的顺序进行排序，对应order by语句来说，效率就会提高很多。 劣势 索引会占据磁盘空间 索引虽然会提高查询效率，但是会降低更新表的效率。比如每次对表进行增删改操作，MySQL不仅要保存数据，还有保存或者更新对应的索引文件。 索引类型主键索引索引列中的值必须是唯一的，不允许有空值。 普通索引（NORMAL）MySQL中基本索引类型，没有什么限制，允许在定义索引的列中插入重复值和空值。 唯一索引（UNIQUE）索引列中的值必须是唯一的，但是允许为空值。 全文索引（FULLTEXT）只能在文本类型CHAR,VARCHAR,TEXT类型字段上创建全文索引。字段长度比较大时，如果创建普通索引，在进行like模糊查询时效率比较低，这时可以创建全文索引。 MyISAM和InnoDB中都可以使用全文索引。 空间索引（SPATIAL）MySQL在5.7之后的版本支持了空间索引，而且支持OpenGIS几何数据模型。MySQL在空间索引这方面遵循OpenGIS几何数据模型规则。 了解即可 前缀索引在文本类型如CHAR,VARCHAR,TEXT类列上创建索引时，可以指定索引列的长度，但是数值类型不能指定。 12alter table user add index index1(email) //普通索引alter table user add index index2(email(6)) //前缀索引 其他（按照索引列数量分类）单列索引组合索引组合索引的使用，需要遵循最左匹配原则。一般情况下在条件允许的情况下使用组合索引替代多个单列索引使用。 索引的数据结构 Innnodb中支持的只有Hash结构和B+结构，如果没有特别说明的话，一般是使用B+结构的 下面的分析依次是Hash –&gt; 普通二叉树 –&gt; 平衡二叉树 –&gt; B树 –&gt; B+树 HashHash表，在Java中的HashMap，TreeMap就是Hash表结构，以键值对的方式存储数据。我们使用Hash表存储表数据Key可以存储索引列，Value可以存储行记录或者行磁盘地址。Hash表在等值查询时效率很高，时间复杂度为O(1)；但是不支持范围快速查找，范围查找时还是只能通过扫描全表方式。 字段值所对应的数组下标是哈希算法随机算出来的，所以可能出现哈希冲突。 hash索引检索一次到位，而不需要想B+树那样从根节点访问到叶子节点。不过在有大量重复值得情况下，hash索引的效率极低，因为要频发地处理Hash冲突。 那Hash表在哪些场景比较适合 : 等值查询的场景，就只有KV（Key，Value）的情况，例如Redis、Memcached等这些NoSQL的中间件。 二叉树 二叉树特点： 每个节点最多有2个分叉，左子树和右子树数据顺序左小右大。 缺陷 这个特点就是为了保证每次查找都可以这折半而减少IO次数，但是二叉树就很考验第一个根节点的取值，因为很容易在这个特点下出现我们并发想发生的情况“树不分叉了”，这就很难受很不稳定。 显然这种情况不稳定的我们再选择设计上必然会避免这种情况的 平衡二叉树平衡二叉树是采用二分法思维，平衡二叉查找树除了具备二叉树的特点，最主要的特征是树的左右两个子树的层级最多相差1。在插入删除数据时通过左旋/右旋操作保持二叉树的平衡，不会出现左子树很高、右子树很矮的情况。 使用平衡二叉查找树查询的性能接近于二分查找法，时间复杂度是 O(log2n)。查询id=6，只需要两次IO。 平衡二叉树存在的问题 时间复杂度和树高相关。树有多高就需要检索多少次，每个节点的读取，都对应一次磁盘 IO 操作。树的高度就等于每次查询数据时磁盘 IO 操作的次数。 平衡二叉树不支持范围查询快速查找，范围查询时需要从根节点多次遍历，查询效率不高。 B树：改造二叉树MySQL的数据是存储在磁盘文件中的，查询处理数据时，需要先把磁盘中的数据加载到内存中，磁盘IO 操作非常耗时，所以我们优化的重点就是尽量减少磁盘 IO 操作。访问二叉树的每个节点就会发生一次IO，如果想要减少磁盘IO操作，就需要尽量降低树的高度。那如何降低树的高度呢？ 假如key为bigint=8字节，每个节点有两个指针，每个指针为4个字节，一个节点占用的空间16个字节（8+4*2=16）。 因为在MySQL的InnoDB存储引擎一次IO会读取的一页（默认一页16K）的数据量，而二叉树一次IO有效数据量只有16字节，空间利用率极低。为了最大化利用一次IO空间，一个简单的想法是在每个节点存储多个元素，在每个节点尽可能多的存储数据。每个节点可以存储1000个索引（16k/16=1000），这样就将二叉树改造成了多叉树，通过增加树的叉树，将树从高瘦变为矮胖。构建1百万条数据，树的高度只需要2层就可以（1000*1000=1百万），也就是说只需要2次磁盘IO就可以查询到数据。磁盘IO次数变少了，查询数据的效率也就提高了 这种数据结构我们称为B树，B树是一种多叉平衡查找树，如下图主要特点： B树的节点中存储着多个元素，每个内节点有多个分叉。 节点中的元素包含键值和数据，节点中的键值从大到小排列。也就是说，在所有的节点都储存数据。 父节点当中的元素不会出现在子节点中。 所有的叶子结点都位于同一层，叶节点具有相同的深度，叶节点之间没有指针连接。 举个例子，在b树中查询数据的情况： 假如我们查询值等于10的数据。查询路径磁盘块1-&gt;磁盘块2-&gt;磁盘块5。 第一次磁盘IO：将磁盘块1加载到内存中，在内存中从头遍历比较，10&lt;15，走左路，到磁盘寻址磁盘块2。 第二次磁盘IO：将磁盘块2加载到内存中，在内存中从头遍历比较，7&lt;10，到磁盘中寻址定位到磁盘块5。 第三次磁盘IO：将磁盘块5加载到内存中，在内存中从头遍历比较，10=10，找到10，取出data，如果data存储的行记录，取出data，查询结束。如果存储的是磁盘地址，还需要根据磁盘地址到磁盘中取出数据，查询终止。 相比二叉平衡查找树，在整个查找过程中，虽然数据的比较次数并没有明显减少，但是磁盘IO次数会大大减少。同时，由于我们的比较是在内存中进行的，比较的耗时可以忽略不计。B树的高度一般2至3层就能满足大部分的应用场景，所以使用B树构建索引可以很好的提升查询的效率。 缺点： B树不支持范围查询的快速查找，你想想这么一个情况如果我们想要查找10和35之间的数据，查找到15之后，需要回到根节点重新遍历查找，需要从根节点进行多次遍历，查询效率有待提高。 如果data存储的是行记录，行的大小随着列数的增多，所占空间会变大。这时，一个页中可存储的数据量就会变少，树相应就会变高，磁盘IO次数就会变大。 B+树：改造B树B+树，作为B树的升级版，在B树基础上，MySQL在B树的基础上继续改造，使用B+树构建索引。B+树和B树最主要的区别在于非叶子节点是否存储数据的问题 B树：非叶子节点和叶子节点都会存储数据。 B+树：只有叶子节点才会存储数据，非叶子节点至存储键值。叶子节点之间使用双向指针连接，最底层的叶子节点形成了一个双向有序链表。 B+树的最底层叶子节点包含了所有的索引项。从图上可以看到，B+树在查找数据的时候，由于数据都存放在最底层的叶子节点上，所以每次查找都需要检索到叶子节点才能查询到数据。所以在需要查询数据的情况下每次的磁盘的IO跟树高有直接的关系，但是从另一方面来说，由于数据都被放到了叶子节点，所以放索引的磁盘块锁存放的索引数量是会跟这增加的，所以相对于B树来说，B+树的树高理论上情况下是比B树要矮的。也存在索引覆盖查询的情况，在索引中数据满足了当前查询语句所需要的全部数据，此时只需要找到索引即可立刻返回，不需要检索到最底层的叶子节点。 等值查询：假如我们查询值等于9的数据。查询路径磁盘块1-&gt;磁盘块2-&gt;磁盘块6。 第一次磁盘IO：将磁盘块1加载到内存中，在内存中从头遍历比较，9&lt;15，走左路，到磁盘寻址磁盘块2。 第二次磁盘IO：将磁盘块2加载到内存中，在内存中从头遍历比较，7&lt;9&lt;12，到磁盘中寻址定位到磁盘块6。 第三次磁盘IO：将磁盘块6加载到内存中，在内存中从头遍历比较，在第三个索引中找到9，取出data，如果data存储的行记录，取出data，查询结束。如果存储的是磁盘地址，还需要根据磁盘地址到磁盘中取出数据，查询终止。（这里需要区分的是在InnoDB中Data存储的为行数据，而MyIsam中存储的是磁盘地址。） 过程如图： 范围查询：假如我们想要查找9和26之间的数据。查找路径是磁盘块1-&gt;磁盘块2-&gt;磁盘块6-&gt;磁盘块7。 首先查找值等于9的数据，将值等于9的数据缓存到结果集。这一步和前面等值查询流程一样，发生了三次磁盘IO。 查找到15之后，底层的叶子节点是一个有序列表，我们从磁盘块6，键值9开始向后遍历筛选所有符合筛选条件的数据。 第四次磁盘IO：根据磁盘6后继指针到磁盘中寻址定位到磁盘块7，将磁盘7加载到内存中，在内存中从头遍历比较，9&lt;25&lt;26，9&lt;26&lt;=26，将data缓存到结果集。 主键具备唯一性（后面不会有&lt;=26的数据），不需再向后查找，查询终止。将结果集返回给用户。 可以看到B+树可以保证等值和范围查询的快速查找，MySQL的索引就采用了B+树的数据结构。 B+ 树简介一个很好的博客：https://blog.csdn.net/qq_26222859/article/details/80631121 总结： Hash不支持范围查询，二叉树树高很高，只有B树跟B+有的一比。 B树一个节点可以存储多个元素，相对于完全平衡二叉树整体的树高降低了，磁盘IO效率提高了。 而B+树是B树的升级版，只是把非叶子节点冗余一下，这么做的好处是为了提高范围查找的效率。提高了的原因也无非是会有指针指向下一个节点的叶子节点。 Mysql选用B+树这种数据结构作为索引，可以提高查询索引时的磁盘IO效率，并且可以提高范围查询的效率，并且B+树里的元素也是有序的。 InnoDB索引的实现主键索引（聚簇索引）每个InnoDB表都有一个聚簇索引 ，聚簇索引使用B+树构建，叶子节点存储的数据是整行记录。一般情况下，聚簇索引等同于主键索引，当一个表没有创建主键索引时，InnoDB会自动创建一个ROWID字段来构建聚簇索引。InnoDB创建索引的具体规则如下： 在表上定义主键PRIMARY KEY，InnoDB将主键索引用作聚簇索引。 如果表没有定义主键，InnoDB会选择第一个不为NULL的唯一索引列用作聚簇索引。 如果以上两个都没有，InnoDB 会使用一个6 字节长整型的隐式字段 ROWID字段构建聚簇索引。该ROWID字段会在插入新行时自动递增。 除聚簇索引之外的所有索引都称为辅助索引。在中InnoDB，辅助索引中的叶子节点存储的数据是该行的主键值都。 在检索时，InnoDB使用此主键值在聚簇索引中搜索行记录。 这里以user_innodb为例，user_innodb的id列为主键，age列为普通索引。 InnoDB的数据和索引存储在一个文件t_user_innodb.ibd中。InnoDB的数据组织方式，是聚簇索引。 主键索引的叶子节点会存储数据行，辅助索引只会存储主键值。 等值查询数据： select * from user_innodb where id = 28; 先在主键树中从根节点开始检索，将根节点加载到内存，比较28&lt;75，走左路。（1次磁盘IO） 将左子树节点加载到内存中，比较16&lt;28&lt;47，向下检索。（1次磁盘IO） 检索到叶节点，将节点加载到内存中遍历，比较16&lt;28，18&lt;28，28=28。查找到值等于28的索引项，直接可以获取整行数据。将改记录返回给客户端。（1次磁盘IO） 磁盘IO数量：3次。 辅助索引除聚簇索引之外的所有索引都称为辅助索引，InnoDB的辅助索引只会存储主键值而非磁盘地址。 MyIsam是存储磁盘地址 以表user_innodb的age列为例，age索引的索引结果如下图。 底层叶子节点的按照（age，id）的顺序排序，先按照age列从小到大排序，age列相同时按照id列从小到大排序。 使用辅助索引需要检索两遍索引：首先检索辅助索引获得主键，然后使用主键到主索引中检索获得记录。 画图分析等值查询的情况： select * from t_user_innodb where age=19; 根据在辅助索引树中获取的主键id，到主键索引树检索数据的过程称为回表查询。 磁盘IO数：辅助索引3次+获取记录回表3次 组合索引还是以自己创建的一个表为例：表 abc_innodb，id为主键索引，创建了一个联合索引idx_abc(a,b,c)。 select * from abc_innodb order by a, b, c, id; 组合索引的数据结构： 组合索引的查询过程： select * from abc_innodb where a = 13 and b = 16 and c = 4; 最左匹配原则：最左前缀匹配原则和联合索引的索引存储结构和检索方式是有关系的。 在组合索引树中，最底层的叶子节点按照第一列a列从左到右递增排列，但是b列和c列是无序的，b列只有在a列值相等的情况下小范围内递增有序，而c列只能在a，b两列相等的情况下小范围内递增有序。 就像上面的查询，B+树会先比较a列来确定下一步应该搜索的方向，往左还是往右。如果a列相同再比较b列。但是如果查询条件没有a列，B+树就不知道第一步应该从哪个节点查起。 可以说创建的idx_abc(a,b,c)索引，相当于创建了(a)、（a,b）（a,b,c）三个索引。、 组合索引的最左前缀匹配原则：使用组合索引查询时，mysql会一直向右匹配直至遇到范围查询(&gt;、&lt;、between、like)就停止匹配。**如果查询条件是b a c 顺序的话也可以命中索引，因为优化器会自动调整a，b，c的顺序 ** 覆盖索引覆盖索引并不是说是索引结构，覆盖索引是一种很常用的优化手段。因为在使用辅助索引的时候，我们只可以拿到主键值，相当于获取数据还需要再根据主键查询主键索引再获取到数据。但是试想下这么一种情况，在上面abc_innodb表中的组合索引查询时，如果我只需要abc字段的，那是不是意味着我们查询到组合索引的叶子节点就可以直接返回了，而不需要回表。这种情况就是覆盖索引。 可以看一下执行计划： 未使用到覆盖索引： 避免回表在InnoDB的存储引擎中，使用辅助索引查询的时候，因为辅助索引叶子节点保存的数据不是当前记录的数据而是当前记录的主键索引，索引如果需要获取当前记录完整数据就必然需要根据主键值从主键索引继续查询。这个过程我们成位回表。想想回表必然是会消耗性能影响性能。那如何避免呢？ 使用索引覆盖，举个例子：现有User表（id(PK),name(key),sex,address,hobby…） 如果在一个场景下，select id,name,sex from user where name =’zhangsan’;这个语句在业务上频繁使用到，而user表的其他字段使用频率远低于它，在这种情况下，如果我们在建立 name 字段的索引的时候，不是使用单一索引，而是使用联合索引（name，sex）这样的话再执行这个查询语句是不是根据辅助索引查询到的结果就可以获取当前语句的完整数据。这样就可以有效地避免了回表再获取sex的数据。 回表大概就是我们有个主键为ID的索引，和一个普通name字段的索引，我们在普通字段上搜索： select * from tableName where name = &#39;tom&#39; 执行的流程是先查询到name索引上的“tom”，然后找到他的id是2，最后去主键索引，找到id为2对应的值。 回到主键索引树搜索的过程，就是回表。不过也有方法避免回表，那就是覆盖索引。 联合索引的使用联合索引，在建立索引的时候，尽量在多个单列索引上判断下是否可以使用联合索引。联合索引的使用不仅可以节省空间，还可以更容易的使用到索引覆盖。试想一下，索引的字段越多，是不是更容易满足查询需要返回的数据呢。比如联合索引（a_b_c），是不是等于有了索引：a，a_b，a_b_c三个索引，这样是不是节省了空间，当然节省的空间并不是三倍于（a，a_b，a_b_c）三个索引，因为索引树的数据没变，但是索引data字段的数据确实真实的节省了。 联合索引的创建原则，在创建联合索引的时候因该把频繁使用的列、区分度高的列放在前面，频繁使用代表索引利用率高，区分度高代表筛选粒度大，这些都是在索引创建的需要考虑到的优化场景，也可以在常需要作为查询返回的字段上增加到联合索引中，如果在联合索引上增加一个字段而使用到了覆盖索引，那建议这种情况下使用联合索引。 总结： 考虑当前是否已经存在多个可以合并的单列索引，如果有，那么将当前多个单列索引创建为一个联合索引。 当前索引存在频繁使用作为返回字段的列，这个时候就可以考虑当前列是否可以加入到当前已经存在索引上，使其查询语句可以使用到覆盖索引。 B+树索引一个B+树的节点中到底存多少个元素最合适？B+树中一个节点为一页或页的倍数最为合适。 因为如果一个节点的大小小于1页，那么读取这个节点的时候其实也会读出1页，造成资源的浪费。 如果一个节点的大小大于1页，比如1.2页，那么读取这个节点的时候会读出2页，也会造成资源的浪费。 所以为了不造成浪费，所以最后把一个节点的大小控制在1页、2页、3页、4页等倍数页大小最为合适。 页的概念首先Mysql的基本存储结构是页(记录都存在页里边)： 一页16KB 各个数据页可以组成一个双向链表 而每个数据页中的记录又可以组成一个单向链表 每个数据页都会为存储在它里边儿的记录生成一个页目录，在通过主键查找某条记录的时候可以在页目录中使用二分法快速定位到对应的槽，然后再遍历该槽对应分组中的记录即可快速找到指定的记录 以其他列(非主键)作为搜索条件：只能从最小记录开始依次遍历单链表中的每条记录。 所以说，如果我们写 select * from user where username=’tom’这样没有进行任何优化的sql语句，默认会这样做： 需要遍历双向链表，找到所在的页，定位到记录所在的页 从所在的页内中查找相应的记录 由于不是根据主键查询，只能遍历所在页的单链表了 很明显，在数据量很大的情况下这样查找会很慢！看起来跟回表有点点像。 最左匹配原则： 索引可以简单如一个列 (a)，也可以复杂如多个列 (a,b,c,d)，即联合索引。 如果是联合索引，那么key也由多个列组成，同时，索引只能用于查找key是否存在（相等），遇到范围查询 (&gt;、&lt;、between、like左匹配)等就不能进一步匹配了，后续退化为线性查找。 因此，列的排列顺序决定了可命中索引的列数。 例子 如有索引 (a,b,c,d)，查询条件 a=1 and b=2 and c&gt;3 and d=4，索引会从左到右依次命中，直到碰到范围查询，即会在每个节点依次命中a、b、c，无法命中d。(c已经是范围查询了，d肯定是排不了序了) b = 2 如果建立(a,b)顺序的索引，是匹配不到(a,b)索引的；但是如果查询条件是a = 1 and b = 2或者a=1(又或者是b = 2 and b = 1)就可以，因为 聚簇索引和非聚簇索引聚簇索引 聚簇索引并不是一种单独的索引类型，而是一种数据存储方式。 简单来说就是绑定数据的索引叫聚簇索引，而其他绑定主键id的叫非聚簇索引 聚簇索引就是按照每张表的主键构造一颗B+树，同时叶子节点中存放的就是整张表的行记录数据，也将聚集索引的叶子节点称为数据页。这个特性决定了索引组织表中数据也是索引的一部分，每张表只能拥有一个聚簇索引。 Innodb通过默认主键聚集数据，如果没有定义主键，innodb会选择非空的唯一索引代替。如果没有这样的索引，innodb会隐式的定义一个主键来作为聚簇索引。 聚簇索引的优缺点 优点： 数据访问更快，因为聚簇索引将索引和数据保存在同一个B+树中，因此从聚簇索引中获取数据比非聚簇索引更快 聚簇索引对于主键的排序查找和范围查找速度非常快 缺点 插入速度严重依赖于插入顺序，按照主键的顺序插入是最快的方式，否则将会出现页分裂，严重影响性能。因此，对于InnoDB表，我们一般都会定义一个自增的ID列为主键 更新主键的代价很高，因为将会导致被更新的行移动。因此，对于InnoDB表，我们一般定义主键为不可更新 非聚簇索引也就是辅助索引 在聚簇索引之上创建的索引称之为辅助索引，辅助索引访问数据总是需要二次查找。辅助索引叶子节点存储的不再是行的物理位置，而是主键值。通过辅助索引首先找到的是主键值，再通过主键值找到数据行的数据页，再通过数据页中的Page Directory找到数据行。 Innodb辅助索引的叶子节点并不包含行记录的全部数据，叶子节点除了包含键值外，还包含了相应行数据的聚簇索引键。 辅助索引的存在不影响数据在聚簇索引中的组织，所以一张表可以有多个辅助索引。在innodb中有时也称辅助索引为二级索引。 非聚簇索引查询可以通过覆盖索引的方式提高查询速度，避免回表操作 总结同时要创建出好的索引要顾及到很多的方面： 最左前缀匹配原则。这是非常重要、非常重要、非常重要（重要的事情说三遍）的原则，MySQL会一直向右匹配直到遇到范围查询 （&gt;,&lt;,BETWEEN,LIKE）就停止匹配。 对字符串进行索引，应该定制一个前缀长度，可以节省大量的索引空间。 避免创建过多的索引，索引会额外占用磁盘空间，降低写操作效率。 尽量选择区分度高的列作为索引，区分度的公式是 COUNT(DISTINCT col)/COUNT(*)。表示字段不重复的比率，比率越大我们扫描的记录数就越少。 索引列不能参与计算，尽量保持列“干净”。比如， FROM_UNIXTIME(create_time)=’2016-06-06’ 就不能使用索引，原因很简单，B+树中存储的都是数据表中的字段值，但是进行检索时，需要把所有元素都应用函数才能比较，显然这样的代价太大。所以语句要写成 ： create_time=UNIX_TIMESTAMP(‘2016-06-06’)。 尽可能的扩展索引，不要新建立索引。比如表中已经有了a的索引，现在要加（a,b）的索引，那么只需要修改原来的索引即可。 单个多列组合索引和多个单列索引的检索查询效果不同，因为在执行SQL时，MySQL只能使用一个索引，会从多个单列索引中选择一个限制最为严格的索引(经指正，在MySQL5.0以后的版本中，有“合并索引”的策略，翻看了《高性能MySQL 第三版》，书作者认为：还是应该建立起比较好的索引，而不应该依赖于“合并索引”这么一个策略)。 “合并索引”策略简单来讲，就是使用多个单列索引，然后将这些结果用“union或者and”来合并起来","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://zhaoshuchao.top/categories/MySQL/"},{"name":"索引","slug":"MySQL/索引","permalink":"https://zhaoshuchao.top/categories/MySQL/%E7%B4%A2%E5%BC%95/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://zhaoshuchao.top/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"https://zhaoshuchao.top/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"反射","slug":"反射","date":"2021-06-02T07:57:43.000Z","updated":"2021-06-02T08:32:15.141Z","comments":true,"path":"2021/06/01/反射/","link":"","permalink":"https://zhaoshuchao.top/2021/06/01/%E5%8F%8D%E5%B0%84/","excerpt":"","text":"如何通过反射创建对象简单的说，反射机制就是在程序的运行过程中被允许对程序本身进行操作，比如自我检查，进行装载，还可以获取类本身，类的所有成员变量和方法，类的对象，还可以在运行过程中动态的创建类的实例，通过实例来调用类的方法，这就是反射机制一个比较重要的功能了。那么要通过程序来理解反射机制，首先要理解类的加载过程 在Java程序执行的时候，要经历三个步骤：加载、连接和初始化。首先程序要加载到JVM的方法区中，然后进行连接，最后初始化。这里就主要介绍一下类的加载。如上图，首先，JVM会从硬盘中读取Java源文件并将其加载到方法区中同时生成类名.class文件，也就是类对象，这个类对象中包含了我们创建类的实例时所需要的模板信息，也就是源代码中的成员变量和方法等。Class本身也是一个类，它的主要功能之一就是生成类加载时的class文件，为类的初始化及实例化做准备。而我们在程序中通过关键字new创建的对象创建的是类的对象，而不是类对象，二者的区别如图中所示。 反射的作用和意义1、反射的应用场合：在编译时根本无法知道该对象或类可能属于哪些类，程序只依靠运行时信息来发现该对象和类的真实信息. 2、反射的作用：通过反射可以使程序代码访问装载到JVM 中的类的内部信息 获取已装载类的成员变量信息 获取已装载类的方法 获取已装载类的构造方法信息 利用反射机制可以获取类对象（也就是我们前面介绍的类对象，获取类对象之后我们便获取了类的模板，可以对类进行一些操作），有以下三种方法： 1.类名.class() 2.对象名.getClass() 3.Class.forName(具体的类名) 用反射创建一个对象： 1234567891011/** * 示例 */public class Demo &#123; public static void main(String[] args) throws Exception &#123; Class clz = Class.forName(&quot;cn.itcast_01.User&quot;); Object obj = clz.newInstance(); // 创建一个对象 System.out.println(obj); &#125;&#125; 反射机制在我们所学习的框架中有很大的应用，而在我们实际开发中用的并不多 IOC运用反射机制和工厂模式的概念 在传统的开发过程中，对象是程序使用使用new创建出来的；但是在spring中是通过IOC容器创建，再推送给调用者。 Spring 中的反射: 创建 Bean 实例时的反射： 1234// 通过类加载器，根据 class 路径，得到其类对象Class&lt;?&gt; clz = Thread.currentThread().getContextClassLoader().loadClass(&quot;org.deppwang.litespring.v1.service.PetStoreService&quot;);// 根据类对象生成 Bean 实例return clz.newInstance(); 构造方法依赖注入时的反射： 1234567// 通过反射获取当前类所有的构造方法信息（Constructor 对象）Constructor&lt;?&gt;[] candidates = beanClass.getDeclaredConstructors();// 设置构造方法参数实例Object[] argsToUse = new Object[parameterTypes.length];argsToUse[i] = getBean(beanNames.get(i));// 使用带有参数的 Constructor 对象实现实例化 Bean。此时使用反射跟上面一样（newInstance0），只是多了参数return constructorToUse.newInstance(argsToUse); setter() 方法依赖注入时的反射 123456// 通过反射获取当前类所有的方法信息（Method 对象）Method[] methods = bean.getClass().getDeclaredMethods();// 获得方法参数实例Object propertyBean = getBean(propertyName);// 通过反射执行调用 setter() 方法。invoke：调用方法，propertyBean 作为方法的参数method.invoke(bean, propertyBean); @Autowired 依赖注入时的反射 12345678// 通过反射得到当前类所有的字段信息（Field 对象）Field[] fields = bean.getClass().getDeclaredFields();// 判断字段是否有 @Autowired 注解Annotation ann = field.getAnnotation(Autowired.class);// 设置字段可连接，相当于将非 public（private、default、protect）更改为 publicfield.setAccessible(true);// 通过反射设置字段的值field.set(bean, getBean(field.getName()));","categories":[{"name":"Java","slug":"Java","permalink":"https://zhaoshuchao.top/categories/Java/"},{"name":"Java基础","slug":"Java/Java基础","permalink":"https://zhaoshuchao.top/categories/Java/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zhaoshuchao.top/tags/JavaSE/"}]},{"title":"HashMap","slug":"HashMap","date":"2021-06-02T07:55:03.000Z","updated":"2021-06-02T08:32:27.944Z","comments":true,"path":"2021/06/01/HashMap/","link":"","permalink":"https://zhaoshuchao.top/2021/06/01/HashMap/","excerpt":"","text":"前置知识HashHash表Hash表也称散列表，直译为哈希表，hash表是一种根据关键字值（key-value）而直接进行访问的数据结构。 在哈希表的键值对关系中，key到value中间还存在着一个映射值，这个映射值就是数组的下标index，key正是通过映射到数组对应的下标index而访问到value值的，通过一个映射函数f(key)映射到数组下标，这个函数我们称之为哈希函数 Hash冲突哈希表中，每个key通过哈希函数的计算都会得到一个唯一的index，但并不是每个index都对应一个唯一的key，就是说可能有两个以上的key映射到同一个index，这就产生了哈希冲突的问题 一个好的hash算法，应该是尽量避免不同的key映射出相同的index，这样才能减少哈希冲突的出现。比如在HashMap中解决哈希冲突采用的是拉链法，这种方法把冲突于某个数组下标的数据都保存到对应数组单元中一个链表中，如下图所示，这种数据结构中数组单元保存不是单一的数值，而是一个链表。按照这种方式，如果哈希冲突越多，可能造成数组的利用率就越低，因为有些数组单元可能被闲置，而数组单元上的链表可能会越大，这势必影响到Map的性能，所以尽可能地避免哈希冲突很重要 常见的hash算法a. 直接定址法：直接以关键字k或者k加上某个常数（k+c）作为哈希地址（H(k)=ak+b）。 b. 数字分析法：提取关键字中取值比较均匀的数字作为哈希地址（如一组出生日期，相较于年-月，月-日的差别要大得多，可以降低冲突概率） c. 分段叠加法：按照哈希表地址位数将关键字分成位数相等的几部分，其中最后一部分可以比较短。然后将这几部分相加，舍弃最高进位后的结果就是该关键字的哈希地址。 d. 平方取中法：如果关键字各个部分分布都不均匀的话，可以先求出它的平方值，然后按照需求取中间的几位作为哈希地址。 e. 伪随机数法：选择一随机函数，取关键字的随机值作为散列地址，通常用于关键字长度不同的场合。 f. 除留余数法：用关键字k除以某个不大于哈希表长度m的数p，将所得余数作为哈希表地址（H(k)=k%p, p&lt;=m; p一般取m或素数）。 解决hash冲突开放定址法： 所谓的开放定址法就是一旦发生了冲突，就去寻找下一个空的散列地址，只要散列表足够大，空的散列地址总能找到，并将记录存入公式为：fi(key) = (f(key)+di) MOD m (di=1,2,3,……,m-1) 当冲突发生时，使用某种探测技术在散列表中形成一个探测序列。沿此序列逐个单元地查找，直到找到给定的关键字，或者碰到一个开放的地址（即该地址单元为空）为止（若要插入，在探查到开放的地址，则可将待插入的新结点存人该地址单元）。查找时探测到开放的地址则表明表中无待查的关键字，即查找失败。 再哈希法： 再哈希法又叫双哈希法，有多个不同的Hash函数，当发生冲突时，使用第二个，第三个，….，等哈希函数计算地址，直到无冲突。虽然不易发生聚集，但是增加了计算时间。 链地址法： 链地址法的基本思想是：每个哈希表节点都有一个next指针，多个哈希表节点可以用next指针构成一个单向链表，被分配到同一个索引上的多个节点可以用这个单向 链表连接起来，如：键值对k2, v2与键值对k1, v1通过计算后的索引值都为2，这时及产生冲突，但是可以通道next指针将k2, k1所在的节点连接起来，这样就解决了哈希的冲突问题 建立公共溢出区： 这种方法的基本思想是：将哈希表分为基本表和溢出表两部分，凡是和基本表发生冲突的元素，一律填入溢出表 计算符号 &lt;&lt; : 左移运算符，num &lt;&lt; 1,相当于num乘以2 低位补0 &gt;&gt;: 右移运算符 &gt;&gt;&gt; : 无符号右移，忽略符号位，空位都以0补齐 ^ : 位异或 第一个操作数的的第n位于第二个操作数的第n位相反，那么结果的第n为也为1，否则为0 &amp; : 与运算 第一个操作数的的第n位于第二个操作数的第n位如果都是1，那么结果的第n为也为1，否则为0 | : 或运算 第一个操作数的的第n位于第二个操作数的第n位 只要有一个是1，那么结果的第n为也为1，否则为0 ~ : 非运算 操作数的第n位为1，那么结果的第n位为0，反之，也就是取反运算(一元操作符：只操作一个数) HashMap中的Hash算法index的获取很简单，就是把h对 length-1取模，其中length为数组长度，所以关键的是h是怎么计算得到的，也很简单 就是通过 h = (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16)得到的 123456789static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; //获取index的方法，1.7源码，1.8中用tab[(n - 1) &amp; hash]代替，但原理一样static int indexFor(int h, int length) &#123; return h &amp; (length-1);&#125; HashMap的容量为什么是2的幂次方2的幂次方是指数组长度length的大小，假如length等于2的幂次方，那样length-1的二进制数据的低位就全部为1了，比如当数组长度为16，那么15的二进制就为1111，只有这样，在计算数组下标index的时候才能更好地利用h的散列性，举个例子： 1234567891011121314151617181920212223242526比如 length-1=15，二进制即为1111，分别跟三个不同的h值进行与运算，计算如下1111 &amp; 101010100101001001000 结果：1000 = 81111 &amp; 101000101101001001001 结果：1001 = 91111 &amp; 101010101101101001010 结果：1010 = 101111 &amp; 101100100111001101100 结果： 1100 = 12 但是如果length为11的话，那么length-1的二进制则表示为1010，与同样的三个h值与运算，计算如下1010 &amp; 101010100101001001000 结果：1000 = 81010 &amp; 101000101101001001001 结果：1000 = 81010 &amp; 101010101101101001010 结果：1010 = 101010 &amp; 101100100111001101100 结果： 1000 = 8 很明显，当数组长度为16的时候，没有产生哈希冲突，而为11的时候，产生了3次哈希冲突，所以这就说明了为什么HashMap的容量建议为2的幂次方 正文HashMapHashMap的组成结构HashMap是我们非常常用的数据结构，由数组和链表组合构成的数据 大概如下，数组里面每个地方都存了Key-Value这样的实例，在Java7叫Entry在Java8中叫Node。 因为他本身所有的位置都为null，在put插入的时候会根据key的hash值去计算一个index值。 就比如我put（”张三“，3），我插入了为”帅丙“的元素，这个时候我们会通过哈希函数计算出插入的位置，计算出来index是2那结果如下。 index = index(hash(“张三”))= 2 我们都知道数组长度是有限的，在有限的长度里面我们使用哈希，哈希本身就存在概率性，就是”张三“和”李四“我们都去hash有一定的概率会一样，就像上面的情况我再次哈希”李四“极端情况也会hash到一个值上，那就形成了链表。 这种头插法是JDK1.7实现方式，JDK1.8为尾插法 每一个节点都会保存自身的hash、key、value、以及下个节点，我看看Node的源码。 1234567static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next;..........&#125; put方法流程 判断数组是否为空，为空进行初始化; 不为空，计算 k 的 hash 值，通过(n - 1) &amp; hash计算应当存放在数组中的下标 index; 查看 table[index] 是否存在数据，没有数据就构造一个Node节点存放在 table[index] 中； 存在数据，说明发生了hash冲突(存在二个节点key的hash值一样), 继续判断key是否相等，相等，用新的value替换原数据(onlyIfAbsent为false)； 如果不相等，判断当前节点类型是不是树型节点，如果是树型节点，创造树型节点插入红黑树中；(如果当前节点是树型节点证明当前已经是红黑树了) 如果不是树型节点，创建普通Node加入链表中；判断链表长度是否大于 8并且数组长度大于64， 大于的话链表转换为红黑树； 插入完成之后判断当前节点数是否大于阈值，如果大于开始扩容为原数组的二倍。 插入链表方式java8之前是头插法，就是说新来的值会取代原有的值，原有的值就顺推到链表中去，就像上面的例子一样，因为写这个代码的作者认为后来的值被查找的可能性更大一点，提升查找的效率。 扩容有两个因素： Capacity：HashMap当前长度。 LoadFactor：负载因子，默认值0.75f 当HashMap的当前长度达到总长度的0.75。触发扩容。扩容时，新建一个空HashMap，长度为原Map的2倍，然后对数据重新Hash,存入新的Map 头插法造成的问题我们要在容量为2的容器里面用不同线程插入A，B，C，假如我们在resize之前打个短点，那意味着数据都插入了但是还没resize那扩容前可能是这样的。 我们可以看到链表的指向A-&gt;B-&gt;C 因为resize的赋值方式，也就是使用了单链表的头插入方式，同一位置上新元素总会被放在链表的头部位置，在旧数组中同一条Entry链上的元素，通过重新计算索引位置后，有可能被放到了新数组的不同位置上。 就可能出现下面的情况，大家发现问题没有？ B的下一个指针指向了A 一旦几个线程都调整完成，就可能出现环形链表 JDK1.8尾插法因为java8之后链表有红黑树的部分，大家可以看到代码已经多了很多if else的逻辑判断了，红黑树的引入巧妙的将原本O(n)的时间复杂度降低到了O(logn)。 JDK1.8中，当链表的长度大于8，数组长度大于等于 64,这个链表就会进化为红黑树。当红黑树的个数小于6，就会退化为链表 当hash冲突很多的时候，红黑树查询效率比链表要高 使用头插会改变链表的上的顺序，但是如果使用尾插，在扩容时会保持链表元素原本的顺序，就不会出现链表成环的问题了。 就是说原本是A-&gt;B，在扩容后那个链表还是A-&gt;B Java7在多线程操作HashMap时可能引起死循环，原因是扩容转移后前后链表顺序倒置，在转移过程中修改了原来链表中节点的引用关系。 Java8在同样的前提下并不会引起死循环，原因是扩容转移后前后链表顺序不变，保持之前节点的引用关系。 我认为即使不会出现死循环，但是通过源码看到put/get方法都没有加同步锁，多线程情况最容易出现的就是：无法保证上一秒put的值，下一秒get的时候还是原值，所以线程安全还是无法保证。 HashMap线程安全问题在并发的情况下，我们一般都会使用HashTable或者ConcurrentHashMap，但是因为前者的并发度的原因基本上没啥使用场景了，所以存在线程不安全的场景我们都使用的是ConcurrentHashMap。 HashTable实现安全很简单，直接get/put加锁 HashTableHashTable和HashMap的区别 Hashtable 是不允许键或值为 null 的，HashMap 的键值则都可以为 null。 实现方式不同：Hashtable 继承了 Dictionary类，而 HashMap 继承的是 AbstractMap 类。 Dictionary 是 JDK 1.0 添加的，貌似没人用过这个，我也没用过。 初始化容量不同：HashMap 的初始容量为：16，Hashtable 初始容量为：11，两者的负载因子默认都是：0.75。 扩容机制不同：当现有容量大于总容量 * 负载因子时，HashMap 扩容规则为当前容量翻倍，Hashtable 扩容规则为当前容量翻倍 + 1。 迭代器不同：HashMap 中的 Iterator 迭代器是 fail-fast 的，而 Hashtable 的 Enumerator 不是 fail-fast 的。 所以，当其他线程改变了HashMap 的结构，如：增加、删除元素，将会抛出ConcurrentModificationException 异常，而 Hashtable 则不会。 ConcurrentHashMapConcurrentHashMap 底层是基于 数组 + 链表 组成的，不过在 jdk1.7 和 1.8 中具体实现稍有不同。 JDK1.7先说一下他在1.7中的数据结构吧： 如图所示，是由 Segment 数组、HashEntry 组成，和 HashMap 一样，仍然是数组加链表。 Segment 是 ConcurrentHashMap 的一个内部类 HashEntry跟HashMap差不多的，但是不同点是，他使用volatile去修饰了他的数据Value还有下一个节点next。 volatile的特性是啥？ 保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。（实现可见性） 禁止进行指令重排序。（实现有序性） volatile 只能保证对单次读/写的原子性。i++ 这种操作不能保证原子性。 原理上来说，ConcurrentHashMap 采用了分段锁技术，其中 Segment 继承于 ReentrantLock。 不会像 HashTable 那样不管是 put 还是 get 操作都需要做同步处理，理论上 ConcurrentHashMap 支持 CurrencyLevel (Segment 数组数量)的线程并发。 每当一个线程占用锁访问一个 Segment 时，不会影响到其他的 Segment。 就是说如果容量大小是16他的并发度就是16，可以同时允许16个线程操作16个Segment而且还是线程安全的。 JDK1.8抛弃了原有的 Segment 分段锁，而采用了 CAS + synchronized 来保证并发安全性。 跟HashMap很像，也把之前的HashEntry改成了Node，但是作用不变，把值和next采用了volatile去修饰，保证了可见性，并且也引入了红黑树，在链表大于一定值的时候会转换（默认是8）。 put操作 ConcurrentHashMap在进行put操作的还是比较复杂的，大致可以分为以下步骤： 根据 key 计算出 hashcode 。 判断是否需要进行初始化。 即为当前 key 定位出的 Node，如果为空表示当前位置可以写入数据，利用 CAS 尝试写入，失败则自旋保证成功。 如果当前位置的 hashcode == MOVED == -1,则需要进行扩容。 如果都不满足，则利用 synchronized 锁写入数据。 如果数量大于 TREEIFY_THRESHOLD 则要转换为红黑树。 get操作 根据计算出来的 hashcode 寻址，如果就在桶上那么直接返回值。 如果是红黑树那就按照树的方式获取值。 就不满足那就按照链表的方式遍历获取值。 小结：1.8 在 1.7 的数据结构上做了大的改动，采用红黑树之后可以保证查询效率（O(logn)），甚至取消了 ReentrantLock 改为了 synchronized，这样可以看出在新版的 JDK 中对 synchronized 优化是很到位的。","categories":[{"name":"Java","slug":"Java","permalink":"https://zhaoshuchao.top/categories/Java/"},{"name":"Java基础","slug":"Java/Java基础","permalink":"https://zhaoshuchao.top/categories/Java/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zhaoshuchao.top/tags/JavaSE/"},{"name":"集合","slug":"集合","permalink":"https://zhaoshuchao.top/tags/%E9%9B%86%E5%90%88/"}]},{"title":"HashMap解析","slug":"HashMap解析","date":"2021-06-02T07:55:03.000Z","updated":"2021-06-02T08:32:32.371Z","comments":true,"path":"2021/06/01/HashMap解析/","link":"","permalink":"https://zhaoshuchao.top/2021/06/01/HashMap%E8%A7%A3%E6%9E%90/","excerpt":"","text":"HashMap源码解析JDK1.8先看看hashMap在jdk 1.8的结构，用的是数组+链表+红黑树的结构，也叫哈希桶，在jdk 1.8之前都是数组+链表的结构，因为在链表的查询操作都是O(N)的时间复杂度，而且hashMap中查询操作也是占了很大比例的，如果当节点数量多，转换为红黑树结构，那么将会提高很大的效率，因为红黑树结构中，增删改查都是O(log n)。 哈希桶就是数组里面的一个位置中所占所有数据，例如，下图中，绿色节点所占的该数组的位置，以及它连接的链表，整体为一个哈希桶。 hashMap的属性：12345678910111213141516171819202122232425262728public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable &#123; //序列号，序列化的时候使用。 private static final long serialVersionUID = 362498820763181265L; /**默认容量，1向左移位4个，00000001变成00010000，也就是2的4次方为16，使用移位是因为移位是计算机基础运算，效率比加减乘除快。**/ static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; //最大容量，2的30次方。 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; //加载因子，用于扩容使用。 static final float DEFAULT_LOAD_FACTOR = 0.75f; //当某个桶节点数量大于8时，会转换为红黑树。 static final int TREEIFY_THRESHOLD = 8; //当某个桶节点数量小于6时，会转换为链表，前提是它当前是红黑树结构。 static final int UNTREEIFY_THRESHOLD = 6; //当整个hashMap中元素数量大于64时，也会进行转为红黑树结构。 static final int MIN_TREEIFY_CAPACITY = 64; //存储元素的数组，transient关键字表示该属性不能被序列化 transient Node&lt;K,V&gt;[] table; //将数据转换成set的另一种存储形式，这个变量主要用于迭代功能。 transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; //元素数量 transient int size; //统计该map修改的次数 transient int modCount; //临界值，也就是元素数量达到临界值时，会进行扩容。 int threshold; //也是加载因子，只不过这个是变量。 final float loadFactor; 这里讲讲为什么默认容量大小为16，加载因子为0.75，主要原因是这两个常量的值都是经过大量的计算和统计得出来的最优解，仅仅是这样而已。 上面是hashMap的属性，尽量的解释给大家，下面再说说它里面的内部类，并不是所有的内部类，只说常用的。 12345678910static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; boolean red; TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; super(hash, key, val, next); &#125;&#125; 使用静态内部类，是为了方便调用，而不用每次调用里面的属性或者方法都需要new一个对象。这是一个红黑树的结构，如果没有学过红黑树的同学，自己去看一下，内容太多，就不在这里阐述了。 12345678910111213 static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125;&#125; 里面还包含了一个结点内部类，是一个单向链表。上面这两个内部类再加上之前的Node&lt;K,V&gt;[] table属性，组成了hashMap的结构，哈希桶。 构造方法：大致懂了hashMap的结构，我们来看看构造方法，一共有3个。 1234567891011121314151617181920212223public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; &#125; public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR);&#125; public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal load factor: &quot; + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity);&#125; 第一个，空参构造，使用默认的加载因子0.75；第二个，设置初始容量，并使用默认的加载因子；第三个，设置初始容量和加载因子，其实第二个构造方法也是调用了第三个。下面，在看看最后一个构造函数。 1234567891011121314151617181920212223242526272829303132 public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false);&#125; final void putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict) &#123; //获取该map的实际长度 int s = m.size(); if (s &gt; 0) &#123; //判断table是否初始化，如果没有初始化 if (table == null) &#123; // pre-size /**求出需要的容量，因为实际使用的长度=容量*0.75得来的，+1是因为小数相除，基本都不会是整数，容量大小不能为小数的，后面转换为int，多余的小数就要被丢掉，所以+1，例如，map实际长度22，22/0.75=29.3,所需要的容量肯定为30，有人会问如果刚刚好除得整数呢，除得整数的话，容量大小多1也没什么影响**/ float ft = ((float)s / loadFactor) + 1.0F; //判断该容量大小是否超出上限。 int t = ((ft &lt; (float)MAXIMUM_CAPACITY) ? (int)ft : MAXIMUM_CAPACITY); /**对临界值进行初始化，tableSizeFor(t)这个方法会返回大于t值的，且离其最近的2次幂，例如t为29，则返回的值是32**/ if (t &gt; threshold) threshold = tableSizeFor(t); &#125; //如果table已经初始化，则进行扩容操作，resize()就是扩容。 else if (s &gt; threshold) resize(); //遍历，把map中的数据转到hashMap中。 for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) &#123; K key = e.getKey(); V value = e.getValue(); putVal(hash(key), key, value, false, evict); &#125; &#125;&#125; 该构造函数，传入一个Map，然后把该Map转为hashMap，resize方法在下面添加元素的时候会详细讲解，在上面中entrySet方法会返回一个Set&lt;Map.Entry&lt;K, V&gt;&gt;，泛型为Map的内部类Entry，它是一个存放key-value的实例，也就是Map中的每一个key-value就是一个Entry实例，为什么使用这个方式进行遍历，因为效率高，具体自己百度一波，putVal方法把取出来的每个key-value存入到hashMap中，待会会仔细讲解。 构造函数和属性讲得差不多了，下面要讲解的是增删改查的操作以及常用的、重要的方法，毕竟里面的方法太多了，其它的就自己去看看吧。 添加元素： 在讲解put方法之前，先看看hash方法，看怎么计算哈希值的。 12345 static final int hash(Object key) &#123; int h; /**先获取到key的hashCode，然后进行移位再进行异或运算，为什么这么复杂，不用想肯定是为了减少hash冲突**/ return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 下面来看看put方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566 public V put(K key, V value) &#123; /**四个参数，第一个hash值，第四个参数表示如果该key存在值，如果为null的话，则插入新的value，最后一个参数，在hashMap中没有用，可以不用管，使用默认的即可**/ return putVal(hash(key), key, value, false, true);&#125; final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; //tab 哈希数组，p 该哈希桶的首节点，n hashMap的长度，i 计算出的数组下标 Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //获取长度并进行扩容，使用的是懒加载，table一开始是没有加载的，等put后才开始加载 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; /**如果计算出的该哈希桶的位置没有值，则把新插入的key-value放到此处，此处就算没有插入成功，也就是发生哈希冲突时也会把哈希桶的首节点赋予p**/ if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); //发生哈希冲突的几种情况 else &#123; // e 临时节点的作用， k 存放该当前节点的key Node&lt;K,V&gt; e; K k; //第一种，插入的key-value的hash值，key都与当前节点的相等，e = p，则表示为首节点 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //第二种，hash值不等于首节点，判断该p是否属于红黑树的节点 else if (p instanceof TreeNode) /**为红黑树的节点，则在红黑树中进行添加，如果该节点已经存在，则返回该节点（不为null），该值很重要，用来判断put操作是否成功，如果添加成功返回null**/ e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); //第三种，hash值不等于首节点，不为红黑树的节点，则为链表的节点 else &#123; //遍历该链表 for (int binCount = 0; ; ++binCount) &#123; //如果找到尾部，则表明添加的key-value没有重复，在尾部进行添加 if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); //判断是否要转换为红黑树结构 if (binCount &gt;= TREEIFY_THRESHOLD - 1) treeifyBin(tab, hash); break; &#125; //如果链表中有重复的key，e则为当前重复的节点，结束循环 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; //有重复的key，则用待插入值进行覆盖，返回旧值。 if (e != null) &#123; V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; //到了此步骤，则表明待插入的key-value是没有key的重复，因为插入成功e节点的值为null //修改次数+1 ++modCount; //实际长度+1，判断是否大于临界值，大于则扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); //添加成功 return null;&#125; 上面就是具体的元素添加，在元素添加里面涉及到扩容，我们来看看扩容方法resize。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103final Node&lt;K,V&gt;[] resize() &#123; //把没插入之前的哈希数组做我诶oldTal Node&lt;K,V&gt;[] oldTab = table; //old的长度 int oldCap = (oldTab == null) ? 0 : oldTab.length; //old的临界值 int oldThr = threshold; //初始化new的长度和临界值 int newCap, newThr = 0; //oldCap &gt; 0也就是说不是首次初始化，因为hashMap用的是懒加载 if (oldCap &gt; 0) &#123; //大于最大值 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; //临界值为整数的最大值 threshold = Integer.MAX_VALUE; return oldTab; &#125; //标记##，其它情况，扩容两倍，并且扩容后的长度要小于最大值，old长度也要大于16 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) //临界值也扩容为old的临界值2倍 newThr = oldThr &lt;&lt; 1; &#125; /**如果oldCap&lt;0，但是已经初始化了，像把元素删除完之后的情况，那么它的临界值肯定还存在， 如果是首次初始化，它的临界值则为0 **/ else if (oldThr &gt; 0) newCap = oldThr; //首次初始化，给与默认的值 else &#123; newCap = DEFAULT_INITIAL_CAPACITY; //临界值等于容量*加载因子 newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; //此处的if为上面标记##的补充，也就是初始化时容量小于默认值16的，此时newThr没有赋值 if (newThr == 0) &#123; //new的临界值 float ft = (float)newCap * loadFactor; //判断是否new容量是否大于最大值，临界值是否大于最大值 newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; //把上面各种情况分析出的临界值，在此处真正进行改变，也就是容量和临界值都改变了。 threshold = newThr; //表示忽略该警告 @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;) //初始化 Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; //赋予当前的table table = newTab; //此处自然是把old中的元素，遍历到new中 if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; //临时变量 Node&lt;K,V&gt; e; //当前哈希桶的位置值不为null，也就是数组下标处有值，因为有值表示可能会发生冲突 if ((e = oldTab[j]) != null) &#123; //把已经赋值之后的变量置位null，当然是为了好回收，释放内存 oldTab[j] = null; //如果下标处的节点没有下一个元素 if (e.next == null) //把该变量的值存入newCap中，e.hash &amp; (newCap - 1)并不等于j newTab[e.hash &amp; (newCap - 1)] = e; //该节点为红黑树结构，也就是存在哈希冲突，该哈希桶中有多个元素 else if (e instanceof TreeNode) //把此树进行转移到newCap中 ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; /**此处表示为链表结构，同样把链表转移到newCap中，就是把链表遍历后，把值转过去，在置位null**/ Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; //返回扩容后的hashMap return newTab;&#125; 上部分内容就是整个扩容过程的操作，下面再来看看删除方法，remove。 删除元素：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public V remove(Object key) &#123; //临时变量 Node&lt;K,V&gt; e; /**调用removeNode(hash(key), key, null, false, true)进行删除，第三个value为null，表示，把key的节点直接都删除了，不需要用到值，如果设为值，则还需要去进行查找操作**/ return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value;&#125;/**第一参数为哈希值，第二个为key，第三个value，第四个为是为true的话，则表示删除它key对应的value，不删除key,第四个如果为false，则表示删除后，不移动节点**/final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; //tab 哈希数组，p 数组下标的节点，n 长度，index 当前数组下标 Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; //哈希数组不为null，且长度大于0，然后获得到要删除key的节点所在是数组下标位置 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; //nodee 存储要删除的节点，e 临时变量，k 当前节点的key，v 当前节点的value Node&lt;K,V&gt; node = null, e; K k; V v; //如果数组下标的节点正好是要删除的节点，把值赋给临时变量node if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; //也就是要删除的节点，在链表或者红黑树上，先判断是否为红黑树的节点 else if ((e = p.next) != null) &#123; if (p instanceof TreeNode) //遍历红黑树，找到该节点并返回 node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else &#123; //表示为链表节点，一样的遍历找到该节点 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; /**注意，如果进入了链表中的遍历，那么此处的p不再是数组下标的节点，而是要删除结点的上一个结点**/ p = e; &#125; while ((e = e.next) != null); &#125; &#125; //找到要删除的节点后，判断!matchValue，我们正常的remove删除，!matchValue都为true if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; //如果删除的节点是红黑树结构，则去红黑树中删除 if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); //如果是链表结构，且删除的节点为数组下标节点，也就是头结点，直接让下一个作为头 else if (node == p) tab[index] = node.next; else /**为链表结构，删除的节点在链表中，把要删除的下一个结点设为上一个结点的下一个节点**/ p.next = node.next; //修改计数器 ++modCount; //长度减一 --size; /**此方法在hashMap中是为了让子类去实现，主要是对删除结点后的链表关系进行处理**/ afterNodeRemoval(node); //返回删除的节点 return node; &#125; &#125; //返回null则表示没有该节点，删除失败 return null;&#125; 删除还有clear方法，把所有的数组下标元素都置位null，下面在来看看较为简单的获取元素与修改元素操作。 获取元素： 123456789101112131415161718192021222324252627282930313233 public V get(Object key) &#123; Node&lt;K,V&gt; e; //也是调用getNode方法来完成的 return (e = getNode(hash(key), key)) == null ? null : e.value;&#125; final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; //first 头结点，e 临时变量，n 长度,k key Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; //头结点也就是数组下标的节点 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; //如果是头结点，则直接返回头结点 if (first.hash == hash &amp;&amp; ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; //不是头结点 if ((e = first.next) != null) &#123; //判断是否是红黑树结构 if (first instanceof TreeNode) //去红黑树中找，然后返回 return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123; //链表节点，一样遍历链表，找到该节点并返回 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; //找不到，表示不存在该节点 return null;&#125; 修改元素： 元素的修改也是put方法，因为key是唯一的，所以修改元素，是把新值覆盖旧值。","categories":[{"name":"Java","slug":"Java","permalink":"https://zhaoshuchao.top/categories/Java/"},{"name":"Java基础","slug":"Java/Java基础","permalink":"https://zhaoshuchao.top/categories/Java/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zhaoshuchao.top/tags/JavaSE/"},{"name":"集合","slug":"集合","permalink":"https://zhaoshuchao.top/tags/%E9%9B%86%E5%90%88/"}]},{"title":"GC机制(一)","slug":"GC机制-一","date":"2021-06-02T07:47:18.000Z","updated":"2021-06-02T08:32:25.390Z","comments":true,"path":"2021/06/01/GC机制-一/","link":"","permalink":"https://zhaoshuchao.top/2021/06/01/GC%E6%9C%BA%E5%88%B6-%E4%B8%80/","excerpt":"","text":"JVM中的垃圾回收对象怎么什么时候被回收？JVM判断对象回收有两种方式： 引用记数引用记数比较简单，JVM为每个对象维护一个引用计数，假设A对象引用计数为零说明没有任务对象引用A对象，那A对象就可以被回收了，但是引用计数有个缺点就是无法解决循环引用**的问题。 循环引用就是两个孤零零的对象互相引用，a对象引用b,b对象引用a ，他们的引用计数都不为零，故此处陷入死循环 GC Roots(标记)可达性分析算法，也叫标记 GC Roots通过一系列的名为GC Roots的对象作为起始点，从这些节点开始向下搜索，搜索过的路径称为引用链，当一个对象到GC Roots没有任何引用链相连时，则证明对象是不可用的。 标记的过程其实就是，从根对象开始遍历所有的对象，然后将所有存活的对象标记为可达的对象。 在Java中，可以作为GC Roots的对象包括下面几种： 虚拟机栈中引用的对象； 方法区中类静态属性引用的对象； 方法区中的常量引用的对象； 本地方法栈中引用的对象（Native对象） 例如： GC Roots算法是从离散数学中的图论引入的，程序把所有的引用关系看作一张图，从一个节点 GC ROOT 开始，寻找对应的引用节点，找到这个节点以后，继续寻找这个节点的引用节点，当所有的引用节点寻找完毕之后，剩余的节点则被认为是没有被引用到的节点，即无用的节点。如上图中的 ObjF、ObjD、ObjE通过 GC Root 是无法找到的，所以它们是无用节点。 总的来说就是当一个对象通过GC Roots搜索不到时，说明对象可以被回收了 垃圾回收算法 四大基本算法：引用计数法，标记清除法，标记整理法，复制算法 分带算法，分区算法 引用计数法引用计数算法很简单，它实际上是通过在对象头中分配一个空间来保存该对象被引用的次数。如果该对象被其它对象引用，则它的引用计数加一，如果删除对该对象的引用，那么它的引用计数就减一，当该对象的引用计数为0时，那么该对象就会被回收。 有代码如下: String p = new String(“abc”); abc这个字符串对象的引用计数值为1. 将引用变量p的值设置为0 p = null; 引用计数垃圾收集机制，它只是在引用计数变化为0时即刻发生，而且只针对某一个对象以及它所依赖的其它对象。所以，我们一般也称呼引用计数垃圾收集为直接的垃圾收集机制.垃圾收集的开销被分摊到整个应用程序的运行当中了，而不是在进行垃圾收集时，要挂起整个应用的运行，直到对堆中所有对象的处理都结束。因此，采用引用计数的垃圾收集不属于严格意义上的”Stop-The-World”的垃圾收集机制。 优点: 实时性较高, 不需要等到内存不够时才回收 垃圾回收时不用挂起整个程序, 不影响程序正常运行. 缺点: 回收时不移动对象, 所以会造成内存碎片问题. 解决不了对象相互引用 标记清除它的做法是当堆中的有效内存空间被耗尽的时候，就会停止整个程序（也被成为stop the world），然后进行两项工作，第一项则是标记，第二项则是清除。标记：从根集合（GC Root)开始扫描，每到达一个对象就会标记该对象为存活状态， 清除：过程将遍历堆中所有的对象，将没有标记的对象全部清除掉。 优点 : 实现简单 缺点 : 效率低, 因为标记和清除两个动作都要遍历所有的对象 垃圾收集后有可能会造成大量的内存碎片, 垃圾回收时会造成应用程序暂停. 标记整理既然叫标记-整理算法，那么它也分为两个阶段，一个是标记(mark)，一个是整理(compact). 标记 : 标记的过程其实就是，从根对象开始遍历所有的对象，然后将所有存活的对象标记为可达的对象。 压缩 : 移动所有的可达对象到堆内存的同一个区域中，使他们紧凑的排列在一起，从而将所有非可达对象释放出来的空闲内存都集中在一起，通过这样的方式来达到减少内存碎片的目的。 标记整理就没有内存碎片的问题了，也是从根集合（GC Root)开始扫描进行标记然后清除无用的对象，清除完成后它会整理内存。 优点 : 标记压缩算法是对标记清除算法的优化, 解决了碎片化的问题 缺点 : 还是效率问题, 在标记清除算法上又多加了一步, 效率可想而知会更慢 复制算法复制算法的核心就是，将原有的内存空间一分为二，每次只用其中的一块，在垃圾回收时，将正在使用的对象复制到另一个内存空间中，并以此排列, 然后将该内存空间清空，交换两个内存的角色，完成垃圾的回收。 复制算法会将JVM堆分成二等分，如果堆设置的是1g，那使用复制算法的时候堆就会有被划分为两块区域各512m。给对象分配内存的时候总是使用其中的一块来分配，分配满了以后，GC就会进行标记，然后将存活的对象移动到另外一块空白的区域，然后清除掉所有没有存活的对象，这样重复的处理，始终就会有一块空白的区域没有被合理的利用到。 优点: 在垃圾多的情况下(新生代), 效率较高 清理后, 内存无碎片 缺点: 浪费了一半的内存空间 在存活对象较多的情况下(老年代), 效率较差 分代收集算法分代收集算法是目前大部分 JVM 的垃圾收集器采用的算法。 核心思想是根据对象存活的生命周期将内存划分为若干个不同的区域。一般情况下将堆区划分为老年代（Old Generation）和新生代（Young Generation），老年代的特点是每次垃圾收集时只有少量对象需要被回收，而新生代的特点是每次垃圾回收时都有大量的对象需要被回收，那么就可以根据不同代的特点采取最适合的收集算法。 分代算法其实就是这样的，根据回收对象的特点进行选择，在jvm中， • 新生代适合使用复制算法， • 老年代适合使用标记整理算法 区域划分：新生代占1/3 老年代占2/3 年轻代: 所有新生成的对象首先都是放在年轻代的。年轻代的目标就是尽可能快速的收集掉那些生命周期短的对象。 新生代内存按照8:1:1的比例分为一个 eden 区和两个 survivor(幸存区) 区（from survivor，to survivor）。大部分对象在 Eden 区中生成。回收时先将 eden 区存活对象复制到from 区，然后清空 eden 区，当这个 from 区也存放满了时，则将 eden 区和 from 区存活对象复制到另一个 to 区，然后清空 eden 和这个 from 区，此时 from 区是空的，然后将 from 区和 to 区交换，即保持 to 区为空， 如此往复。 新生代这样划分是为了更好的管理堆内存中的对象，方便GC算法—复制算法来进行垃圾回收。JVM每次只会使用eden和其中一块survivor来为对象服务，所以无论什么时候，都会有一块survivor空间，因此新生代实际可用空间只有90%。 当 to区不足以存放 eden 和 from 的存活对象时，就将存活对象直接存放到老年代。若是老年代也满了就会触发一次 Full GC ，也就是新生代、老年代都进行回收。 新生代发生的 GC 也叫做 Minor GC ，Minor GC 发生频率比较高(不一定等 Eden 区满了才触发)，也叫小GC，每一次Full GC都会产生小GC 老年代 在年轻代中经历了 N 次垃圾回收后仍然存活的对象，就会被放到年老代中。因此，可以认为年老代中存放的都是一些生命周期较长的对象。 内存比新生代也大很多(大概比例是1:2)，当老年代内存满时触发 Major GC 即 Full GC，Full GC 发生频率比较低，老年代对象存活时间比较长，存活率标记高。 什么时候对象会进入老年代？ 1. 根据对象年龄 JVM会给对象增加一个年龄（age）的计数器，对象每“熬过”一次GC，年龄就要+1，待对象到达设置的阈值（默认为15岁）就会被移移动到老年代，可通过-XX:MaxTenuringThreshold调整这个阈值。 即 一次Minor GC后，对象年龄就会+1，达到阈值的对象就移动到老年代，其他存活下来的对象会继续保留在新生代中。 2. 动态年龄判断 根据对象年龄有另外一个策略也会让对象进入老年代，不用等待15次GC之后进入老年代，他的大致规则就是，假如当前放对象的Survivor，一批对象的总大小大于这块Survivor内存的50%，那么大于这批对象年龄的对象，就可以直接进入老年代了。 现有A（age=2）、B(age=2)、D(age=10)、E(age=11)这四个对象，假如from区是100m，如果A + B + D的内存大小超过50m，现在D的年龄是10，那E都会被移动到老年代。实际上这个计算逻辑是这样的：年龄1 + 年龄2 + 年龄n的多个对象总和超过Survivor区的50%，那就会把年龄n以上的对象都放入老年代。 3. 大对象直接进入老年代 如果设置了-XX:PretenureSizeThreshold这个参数，那么如果你要创建的对象大于这个参数的值，比如分配一个超大的字节数组，此时就直接把这个大对象放入到老年代，不会经过新生代。 这么做就可以避免大对象在新生代，屡次躲过GC，还得把他们来复制来复制去的，最后才进入老年代，这么大的对象来回复制，是很耗费时间的。 GC 类型： Minor GC(新生代 GC):新生代 GC，指发生在新生代的垃圾收集动作，因为 Java 对象大多都具备朝生熄灭的特点，所以 Minor GC 十分频繁，回收速度也较快。 Major GC(老年代 GC):老年代 GC，指发生在老年代的垃圾收集动作，当出现 Major GC 时，一般也会伴有至少一次的 Minor GC（并非绝对，例如 Parallel Scavenge 收集器会单独直接触发 Major GC 的机制）。 Major GC 的速度一般会比 Minor GC 慢十倍以上。 Full GC:清理整个堆空间—包括年轻代和老年代。Major GC == Full GC。 产生 Full GC 可能的原因： 年老代被写满。 System.gc() 被显示调用。 上一次 GC 之后 Heap（堆） 的各域分配策略动态变化。 分区收集算法上面介绍的分代收集算法是将对象的生命周期按长短划分为两个部分, 而分区算法则将整个堆空间划分为连续的不同小区间, 每个小区间独立使用, 独立回收. 这样做的好处是可以控制一次回收多少个小区间. 在相同条件下, 堆空间越大, 一次GC耗时就越长, 从而产生的停顿也越长. 为了更好地控制GC产生的停顿时间, 将一块大的内存区域分割为多个小块, 根据目标停顿时间, 每次合理地回收若干个小区间(而不是整个堆), 从而减少一次GC所产生的停顿. 垃圾回收器的分类串行垃圾回收器Serial 收集器 新生代串行回收器SerialGC : 采用复制算法实现, 单线程垃圾回收, 独占式垃圾回收器 老年代串行回收器SerialOldGC : 采用标记压缩算法, 单线程独占式垃圾回收器 并行垃圾回收器Parallel收集器 新生代Parallel ScavengeGC回收器: 采用复制算法多线程独占式回收器 老年代Parallel OldGC回收器: 采用标记压缩算法, 多线程独占式回收器 我的本机使用jconsole，查看是PS MarkSweep JDK版本为：jdk1.8.0_91 CMS回收器CMS全称 (Concurrent Mark Sweep)，是一款并发的、使用标记-清除算法的垃圾回收器. 启用CMS回收器参数 : -XX:+UseConcMarkSweepGC。 使用场景：GC过程短暂停，适合对时延要求较高的服务，用户线程不允许长时间的停顿。 优点: CMS收集器是一种以获取最短回收停顿时间为目标的收集器. 并发收集，低停顿 缺点：服务长时间运行，造成严重的内存碎片化。算法实现比较复杂；CMS收集器对CPU资源非常敏感 G1回收器 G1(Garbage-First)是一款面向服务端应用的并发垃圾回收器, 主要目标用于配备多颗CPU的服务器治理大内存. 是 JDK1.7 提供的一个新收集器，是当今收集器技术发展的最前沿成果之一 G1计划作为并发标记-清除收集器的长期替代品 启用G1收集器参数: -XX:+UseG1GC 启用G1收集器. G1将整个Java堆划分为多个大小相等的独立区域(Region), 虽然还保留有新生代和老年代的概念, 但新生代和老年代不再是物理隔离的了, 它们都是一部分Region(不需要连续)的集合. 每块区域既有可能属于Old区、也有可能是Young区, 因此不需要一次就对整个老年代/新生代回收. 而是当线程并发寻找可回收的对象时, 有些区块包含可回收的对象要比其他区块多很多. 虽然在清理这些区块时G1仍然需要暂停应用线程, 但可以用相对较少的时间优先回收垃圾较多的Region(这也是G1命名的来源). 这种方式保证了G1可以在有限的时间内获取尽可能高的收集效率. JDK8的垃圾回收器我们通过查看Java参数的方式查询到默认的垃圾回收器为ParallelGC 引用《深入理解Java虚拟机：JVM高级特性与最佳实践》的介绍： UseParallelGC = Parallel Scavenge + PS MarkSweep。 PS MarkSweep和Serial Old很像，一般书上会把他们当做一个去讲解。 这个PS MarkSweep默认的实现实际上是一层皮，它底下真正做mark-sweep-compact工作的代码是跟分代式GC框架里的serial old（这个collector名字叫做MarkSweepCompact）是共用同一份代码的。也就是说实际上PS MarkSweep与MarkSweepCompact在HotSpot VM里是同一个collector实现，包了两张不同的皮；这个collector是串行的。 其实也可以通过jconsole工具直接看到使用的垃圾收集器 设置参数如下： ​","categories":[{"name":"Java","slug":"Java","permalink":"https://zhaoshuchao.top/categories/Java/"},{"name":"JVM","slug":"Java/JVM","permalink":"https://zhaoshuchao.top/categories/Java/JVM/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://zhaoshuchao.top/tags/JVM/"},{"name":"GC","slug":"GC","permalink":"https://zhaoshuchao.top/tags/GC/"}]},{"title":"AQS辅助类","slug":"AQS辅助类","date":"2021-06-02T07:43:35.000Z","updated":"2021-06-02T08:32:22.033Z","comments":true,"path":"2021/06/01/AQS辅助类/","link":"","permalink":"https://zhaoshuchao.top/2021/06/01/AQS%E8%BE%85%E5%8A%A9%E7%B1%BB/","excerpt":"","text":"倒计数器 CountDownLatch一个同步辅助类，在完成一组正在其他线程中执行的操作之前，它允许一个或多个线程一直等待。 CountDownLatch是一个辅助同步器类，用来作计数使用， 它的作用有点类似于生活中的倒数计数器，先设定一个计数初始值，当计数降到0时，将会触发一些事件，如火箭的倒数计时。 初始计数值在构造CountDownLatch对象时传入，每调用一次 countDown() 方法，计数值就会减1。 线程可以调用CountDownLatch的await方法进入阻塞，当计数值降到0时，所有之前调用await阻塞的线程都会释放。 注意：CountDownLatch的初始计数值一旦降到0，无法重置。如果需要重置，可以考虑使用CyclicBarrier。 12345678910111213141516171819202122232425262728293031/** * @Description: JUC强大的辅助类 之 倒计数器 * 案例：6个同学陆续离开教室后值班同学才可以关门。 * 1、一个线程 关门 * 2、卡断6次 6个同学出口后 一个线程关门 */public class CountDownLatchDemo &#123; public static void main(String[] args) throws Exception &#123; CountDownLatch countDownLatch = new CountDownLatch(6);//6个同学 //值班同学 要关门 for (int i = 1; i &lt;= 6; i++) &#123; new Thread(() -&gt; &#123; try &#123; Thread.sleep(new Random().nextInt(3000)); System.out.println(&quot;第&quot; + Thread.currentThread().getName() + &quot;同学要离开教室&quot;); countDownLatch.countDown();//倒计数 -1 0 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;, String.valueOf(i)).start(); &#125; //等待 6个同学都离开 // 这里new CountDownLatch(6)的参数是几就表示要等待几次 countDownLatch.await(); System.out.println(Thread.currentThread().getName() + &quot;要关门了&quot;); &#125;&#125; 同步屏障CyclicBarrierDemo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/** * @Description: * 从字面上的意思可以知道，这个类的中文意思是“循环栅栏”。大概的意思就是一个可循环利用的屏障。 * 该命令只在每个屏障点运行一次。若在所有参与线程之前更新共享状态，此屏障操作很有用 * * 1.CyclicBarrier(int parties, Runnable barrierAction) 创建一个CyclicBarrier实例，parties指定参与相互等待的线程数， * barrierAction一个可选的Runnable命令，该命令只在每个屏障点运行一次，可以在执行后续业务之前共享状态。该操作由最后一个进入屏障点的线程执行。 * * 2.CyclicBarrier(int parties) 创建一个CyclicBarrier实例，parties指定参与相互等待的线程数。 * * 3.await() 该方法被调用时表示当前线程已经到达屏障点，当前线程阻塞进入休眠状态，直到所有线程都到达屏障点，当前线程才会被唤醒。 * * 参考文章：https://www.cnblogs.com/jelly12345/p/12111094.html */public class CyclicBarrierDemo &#123; public static void main(String[] args) throws Exception&#123; //倒计数器 CountDownLatch countDownLatch = new CountDownLatch(3); CyclicBarrier cyclicBarrier = new CyclicBarrier(3,() -&gt; &#123; System.out.println(&quot;恭喜大家一起过关了&quot;); &#125;); //组队打boss过关卡游戏。 for (int i = 1; i &lt;= 3; i++) &#123; new Thread(() -&gt; &#123; try &#123; System.out.println(&quot;第&quot; + Thread.currentThread().getName() + &quot;号选手开始第一关&quot;); Thread.sleep(new Random().nextInt(3)); System.out.println(&quot;第&quot; + Thread.currentThread().getName() + &quot;号选手开始打第一关Boss&quot;); //只能被调用三次 三次过后 唤醒 cyclicBarrier.await();//第i号选手必须在此等待其它选手一起进入下一关口 System.out.println(&quot;第&quot; + Thread.currentThread().getName() + &quot;号选手开始第二关&quot;); Thread.sleep(new Random().nextInt(3)); System.out.println(&quot;第&quot; + Thread.currentThread().getName() + &quot;号选手开始打第二关Boss&quot;); cyclicBarrier.await();//第i号选手必须在此等待其它选手一起进入下一关口 System.out.println(&quot;第&quot; + Thread.currentThread().getName() + &quot;号选手开始第三关&quot;); Thread.sleep(new Random().nextInt(3)); System.out.println(&quot;第&quot; + Thread.currentThread().getName() + &quot;号选手开始第三关打Boss&quot;); //cyclicBarrier.await();//第i号选手必须在此等待其它选手一起进入下一关口 //减一次 countDownLatch.countDown(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;,String.valueOf(i)).start(); &#125; //等待 减到0时 唤醒 countDownLatch.await(); System.out.println(&quot;恭喜大家吃鸡了&quot;); &#125;&#125; CyclicBarrier和CountDownLatch的区别？ CountDownLatch的计数器只能使用一次，而CyclicBarrier的计数器可以使用reset()方法重置，可以使用多次，所以CyclicBarrier能够处理更为复杂的场景； CountDownLatch允许一个或多个线程等待一组事件的产生，而CyclicBarrier用于等待其他线程运行到栅栏位置。 CountDownLatch 是一个线程(或者多个)，等待另外N个线程完成某个事情之后才能执行；CyclicBarrie是N个线程相互等待，任何一个线程完成之前，所有的线程都必须等待。 信号量 Semaphore12345678910111213141516171819202122232425262728293031323334353637383940/** * @Description: * * 信号量 * Semaphore翻译成字面意思为 信号量，Semaphore可以控制同时访问的线程个数。非常适合需求量大，而资源又很紧张的情况。 * 比如给定一个资源数目有限的资源池，假设资源数目为N，每一个线程均可获取一个资源，但是当资源分配完毕时，后来线程需要阻塞等待， * 直到前面已持有资源的线程释放资源之后才能继续。 * * 案例：6辆车抢占3个车位 * 1、6个线程 * 2、3个车位 3个信号量 * 信号量能增加 public void release() 释放 * * 信号量能减少 public void acquire() * */public class SemaphoreDemo &#123; public static void main(String[] args) &#123; Semaphore semaphore = new Semaphore(3);//3个信号量 for (int i = 1; i &lt;= 6; i++) &#123; new Thread(() -&gt; &#123; try &#123; //抢占停车位 semaphore.acquire();//抢不到 阻塞状态 等待 System.out.println(&quot;第&quot; + Thread.currentThread().getName() + &quot;辆车抢到了车位&quot;); TimeUnit.MILLISECONDS.sleep(new Random().nextInt(1500)); System.out.println(&quot;第&quot; + Thread.currentThread().getName() + &quot;辆离开了车位&quot;); semaphore.release();//归还车位 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;,String.valueOf(i)).start(); &#125; &#125; &#125;","categories":[{"name":"Java","slug":"Java","permalink":"https://zhaoshuchao.top/categories/Java/"},{"name":"JUC","slug":"Java/JUC","permalink":"https://zhaoshuchao.top/categories/Java/JUC/"}],"tags":[{"name":"JUC","slug":"JUC","permalink":"https://zhaoshuchao.top/tags/JUC/"},{"name":"并发编程","slug":"并发编程","permalink":"https://zhaoshuchao.top/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"Lock","slug":"Lock","date":"2021-06-02T07:41:11.000Z","updated":"2021-06-02T08:32:41.989Z","comments":true,"path":"2021/06/01/Lock/","link":"","permalink":"https://zhaoshuchao.top/2021/06/01/Lock/","excerpt":"","text":"LOCKlock锁是什么Lock是JDK中的一个接口，两个直接实现类 ReentrantLock（重入锁）, ReentrantReadWriteLock（读写锁）。 Lock锁，使用时手动获取锁和释放锁，比synchronized更加灵活；可中断的获取锁；超时获取锁。 12345678 //默认情况下 锁是不公平的; 公平锁(参数true)表示等待时间长优先获取到锁 private final static Lock lock = new ReentrantLock(true);lock.lock(); // 上锁try &#123; // access the resource protected by this lock&#125; finally &#123; lock.unlock(); // 解锁&#125; Lock锁的常用API lock（） 获得锁。 unlock() 释放锁。 tryLock(long time, TimeUnit unit) 尝试获取锁，参数为等待时长，超时将不等待 Lock和synchronized的简单对比 synchronized是Java的关键字，在JVM层面； Lock是一个接口，JDK层面 synchronized不管代码运行是否异常，都会释放锁； lock必须在finally中释放锁，不然容易造成死锁 synchronized没有获取到锁的线程会一直等待 ； lock可以选择不等待，或者设置等待时间 synchronized无法判断锁的状态； lock可以通过 tryLock判断锁的状态 synchronized是非公平锁，不可中断；lock可以选择是否公平，通过lock.lockInterruptibly();中断 ReentrantLock可重入锁。如果当前线程t1通过调用lock方法获取了锁之后，再次调用lock，是不会再阻塞去获取锁的，直接增加重入次数就行了。与每次lock对应的是unlock，unlock会减少重入次数，重入次数减为0才会释放锁。 ReentrantLock构造器为一个公平锁或者不公平锁 123public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync();&#125; 12// new FairSync() : new NonfairSync()又继承了Sync锁static final class NonfairSync extends Sync 12// Sync继承了AQSabstract static class Sync extends AbstractQueuedSynchronizer &#123; 源码解析：https://www.cnblogs.com/lixuwu/p/10788297.html ReentrantReadWriteLock123private final ReentrantReadWriteLock lock = new ReentrantReadWriteLock(); lock.writeLock().lock(); 写锁 lock.readLock().lock(); 读锁 可重入读写锁。读写锁维护了一个读锁，一个写锁。 读锁同一时刻允许多个读线程访问。 写锁同一时刻只允许一个写线程，其他读/写线程都需要阻塞。 CopyOnWrite容器什么是CopyOnWrite容器 CopyOnWrite容器（简称COW容器）即写时复制的容器。通俗的理解是当我们往一个容器添加元素的时候，不直接往当前容器添加，而是先将当前容器进行Copy，复制出一个新的容器，然后新的容器里添加元素，添加完元素之后，再将原容器的引用指向新的容器。这样做的好处是我们可以对CopyOnWrite容器进行并发的读，而不需要加锁，因为当前容器不会添加任何元素。所以CopyOnWrite容器也是一种读写分离的思想，读和写不同的容器。 从JDK1.5开始Java并发包里提供了两个使用CopyOnWrite机制实现的并发容器,它们是CopyOnWriteArrayList和CopyOnWriteArraySet。 CopyOnWrite并发容器用于读多写少的并发场景。比如：白名单，黑名单。假如我们有一个搜索网站，用户在这个网站的搜索框中，输入关键字搜索内容，但是某些关键字不允许被搜索。这些不能被搜索的关键字会被放在一个黑名单当中，黑名单一定周期才会更新一次。 缺点：内存占用问题。写的时候会创建新对象添加到新容器里，而旧容器的对象还在使用，所以有两份对象内存。通过压缩容器中的元素的方法来减少大对象的内存消耗，比如，如果元素全是10进制的数字，可以考虑把它压缩成36进制或64进制。或者不使用CopyOnWrite容器，而使用其他的并发容器，如ConcurrentHashMap。 数据一致性问题。CopyOnWrite容器只能保证数据的最终一致性，不能保证数据的实时一致性。所以如果你希望写入的的数据，马上能读到，请不要使用CopyOnWrite容器。 CAS机制CAS机制全称compare and swap，翻译为比较并交换，是一种有名的无锁（lock-free）算法。也是一种现代 CPU 广泛支持的CPU指令级的操作，只有一步原子操作，所以非常快。而且CAS避免了请求操作系统来裁定锁的问题，直接在CPU内部就完成了。 CAS是乐观锁的一种实现方式，是一种轻量级锁 Unsafe类是CAS的核心类，提供硬件级别的原子操作（目前所有CPU基本都支持硬件级别的CAS操作） 12// 对象、对象的地址、预期值、修改值public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5); CAS 是怎么实现线程安全的？执行函数：CAS(V,E,N) 其包含3个参数 V表示要更新的变量 E表示预期值 N表示新值 如果V值等于E值，则将V的值设为N。若V值和E值不同，则说明已经有其他线程做了更新，则当前线程什么都不做。通俗的理解就是CAS操作需要我们提供一个期望值，当期望值与当前线程的变量值相同时，说明还没线程修改该值，当前线程可以进行修改，也就是执行CAS操作，但如果期望值与当前线程不符，则说明该值已被其他线程修改，此时不执行更新操作，但可以选择重新读取该变量再尝试再次修改该变量，也可以放弃操作，原理图如下 比如：我们要更新一个字段的值，由A更新到B ,这个时候，我们那A，B去更新，先拿到A的值跟原数据比较，如果数据还是A值，说明没有操作过这个数据，那么修改为B成功，若此时字段的值变成了C，则说明有人操作过，操作失败，重新再执行一次。 由于CAS操作属于乐观派，它总认为自己可以成功完成操作，当多个线程同时使用CAS操作一个变量时，只有一个会胜出，并成功更新，其余均会失败，但失败的线程并不会被挂起，仅是被告知失败，并且允许再次尝试（自旋），当然也允许失败的线程放弃操作，这点从图中也可以看出来。基于这样的原理，CAS操作即使没有锁，同样知道其他线程对共享资源操作影响，并执行相应的处理措施。同时从这点也可以看出，由于无锁操作中没有锁的存在，因此不可能出现死锁的情况，也就是说无锁操作天生免疫死锁。 CAS产生的问题：ABA问题解决方案：加标志位，例如搞个自增的字段，操作一次就自增加一，或者搞个时间戳，比较时间戳的值。 举个栗子：现在我们去要求操作数据库，根据CAS的原则我们本来只需要查询原本的值就好了，现在我们一同查出他的标志位版本字段vision。 在atomic包中有一个类：AtomicStampedReference，参数可以设置时间戳 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * @author zsc * @date 2021/5/24 * AtomicStampedReference时间戳解决aba问题 */public class ABADemo &#123; public static void main(String[] args) &#123; Integer stamped = 1; User user = new User(100, &quot;小明&quot;); User user1 = new User(110, &quot;小明2&quot;); User user2 = new User(2000, &quot;小明3&quot;); User user3 = new User(3000, &quot;小明4&quot;); AtomicStampedReference&lt;User&gt; reference = new AtomicStampedReference&lt;User&gt;(user,stamped); System.out.println(reference.getReference());//获取值 System.out.println(reference.getStamp());// 获取时间戳 boolean b = reference.compareAndSet(user, user2, 1, 2); System.out.println(b); System.out.println(reference.getReference());//获取值 System.out.println(reference.getStamp());// 获取时间戳 boolean c = reference.compareAndSet(user2, user3, 3, 4); System.out.println(c); System.out.println(reference.getReference());//获取值 System.out.println(reference.getStamp());// 获取时间戳 &#125;&#125;@Dataclass User&#123; private Integer age; private String name; public User(Integer age, String name) &#123; this.age = age; this.name = name; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; User user = (User) o; return Objects.equals(age, user.age) &amp;&amp; Objects.equals(name, user.name); &#125; @Override public int hashCode() &#123; return Objects.hash(age, name); &#125;&#125; 自旋时间过长使用CAS时非阻塞同步，也就是说不会将线程挂起，会自旋（无非就是一个死循环）进行下一次尝试，如果这里自旋时间过长对性能是很大的消耗。如果JVM能支持处理器提供的pause指令，那么在效率上会有一定的提升。 只能保证一个共享变量的原子操作当对一个共享变量执行操作时CAS能保证其原子性，如果对多个共享变量进行操作，CAS就不能保证其原子性。有一个解决方案是利用对象整合多个共享变量，即一个类中的成员变量就是这几个共享变量。然后将这个对象做CAS操作就可以保证其原子性。atomic中提供了AtomicReference来保证引用对象之间的原子性。 AQS概述：AbstractQueuedSynchronizer抽象队列同步器简称AQS，它是实现同步器的基础组件（框架），juc下面Lock的实现以及一些并发工具类（Semaphore、CountDownLatch、CyclicBarrier等）就是通过AQS来实现的。具体用法是通过继承AQS实现其模板方法，然后将子类作为同步组件的内部类。 实现过程：AQS的核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并将共享资源设置为锁定状态，如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制AQS是用CLH队列锁实现的，即将暂时获取不到锁的线程加入到队列中。CLH（Craig，Landin，and Hagersten）队列是一个虚拟的双向队列，虚拟的双向队列即不存在队列实例，仅存在节点之间的关联关系。AQS是将每一条请求共享资源的线程封装成一个CLH锁队列的一个结点（Node），来实现锁的分配。 用大白话来说，AQS就是基于CLH队列，用volatile修饰共享变量state，线程通过CAS去改变状态符，成功则获取锁成功，失败则进入等待队列，等待被唤醒。 AQS 定义了两种资源共享方式：1.Exclusive：独占，只有一个线程能执行，如ReentrantLock2.Share：共享，多个线程可以同时执行，如Semaphore、CountDownLatch、ReadWriteLock，CyclicBarrier 类似于Synchronized争抢监视器monitor 这里AQS队列中的线程争抢资源state AQS详解：https://blog.csdn.net/javazejian/article/details/75043422 乐观锁：CAS 自旋锁：do while(!CAS) 悲观锁：AQS底层 + volatile state ReentrantLock 阻塞队列（BlockingQueue）什么是BlockingQueue在多线程领域：所谓阻塞，在某些情况下会挂起线程（即阻塞），一旦条件满足，被挂起的线程又会自动被唤起 BlockingQueue即阻塞队列，是java.util.concurrent下的一个接口，因此不难理解，BlockingQueue是为了解决多线程中数据高效安全传输而提出的。从阻塞这个词可以看出，在某些情况下对阻塞队列的访问可能会造成阻塞。被阻塞的情况主要有如下两种： 当队列满了的时候进行入队列操作 当队列空了的时候进行出队列操作 因此，当一个线程试图对一个已经满了的队列进行入队列操作时，它将会被阻塞，除非有另一个线程做了出队列操作；同样，当一个线程试图对一个空队列进行出队列操作时，它将会被阻塞，除非有另一个线程进行了入队列操作。 阻塞队列主要用在生产者/消费者的场景，下面这幅图展示了一个线程生产、一个线程消费的场景： ​ 为什么需要BlockingQueue 好处是我们不需要关心什么时候需要阻塞线程，什么时候需要唤醒线程，因为这一切BlockingQueue都给你一手包办了。在concurrent包发布以前，在多线程环境下，我们每个程序员都必须去自己控制这些细节，尤其还要兼顾效率和线程安全，而这会给我们的程序带来不小的复杂度。 认识BlockingQueuejava.util.concurrent 包里的 BlockingQueue是一个接口，继承Queue接口，Queue接口继承 Collection。 实现类 BlockingQueue接口主要有以下7个实现类： ArrayBlockingQueue：由数组结构组成的有界阻塞队列。 LinkedBlockingQueue：由链表结构组成的有界（但大小默认值为integer.MAX_VALUE）阻塞队列。 PriorityBlockingQueue：支持优先级排序的无界阻塞队列。 DelayQueue：使用优先级队列实现的延迟无界阻塞队列。 SynchronousQueue：不存储元素的阻塞队列，也即单个元素的队列。 LinkedTransferQueue：由链表组成的无界阻塞队列。 LinkedBlockingDeque：由链表组成的双向阻塞队列。","categories":[{"name":"Java","slug":"Java","permalink":"https://zhaoshuchao.top/categories/Java/"},{"name":"JUC","slug":"Java/JUC","permalink":"https://zhaoshuchao.top/categories/Java/JUC/"}],"tags":[{"name":"JUC","slug":"JUC","permalink":"https://zhaoshuchao.top/tags/JUC/"},{"name":"并发编程","slug":"并发编程","permalink":"https://zhaoshuchao.top/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"锁","slug":"锁","permalink":"https://zhaoshuchao.top/tags/%E9%94%81/"}]},{"title":"JMM及Volatile关键字","slug":"JMM及Volatile关键字","date":"2021-05-05T06:48:04.000Z","updated":"2021-06-02T08:32:34.876Z","comments":true,"path":"2021/05/04/JMM及Volatile关键字/","link":"","permalink":"https://zhaoshuchao.top/2021/05/04/JMM%E5%8F%8AVolatile%E5%85%B3%E9%94%AE%E5%AD%97/","excerpt":"","text":"JMM（JavaMemoryModel）理解Java内存区域与Java内存模型Java内存区域 Java虚拟机在运行程序时会把其自动管理的内存划分为以上几个区域，每个区域都有的用途以及创建销毁的时机，其中蓝色部分代表的是所有线程共享的数据区域，而绿色部分代表的是每个线程的私有数据区域。 方法区（Method Area）方法区属于线程共享的内存区域，又称Non-Heap（非堆），主要用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据，根据Java 虚拟机规范的规定，当方法区无法满足内存分配需求时，将抛出OutOfMemoryError 异常。值得注意的是在方法区中存在一个叫运行时常量池(Runtime Constant Pool）的区域，它主要用于存放编译器生成的各种字面量和符号引用，这些内容将在类加载后存放到运行时常量池中，以便后续使用。 JVM堆（Java Heap）Java 堆也是属于线程共享的内存区域，它在虚拟机启动时创建，是Java 虚拟机所管理的内存中最大的一块，主要用于存放对象实例，几乎所有的对象实例都在这里分配内存，注意Java 堆是垃圾收集器管理的主要区域，因此很多时候也被称做GC 堆，如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError 异常。 程序计数器(Program Counter Register)属于线程私有的数据区域，是一小块内存空间，主要代表当前线程所执行的字节码行号指示器。字节码解释器工作时，通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 虚拟机栈(Java Virtual Machine Stacks)属于线程私有的数据区域，与线程同时创建，总数与线程关联，代表Java方法执行的内存模型。每个方法执行时都会创建一个栈桢来存储方法的的变量表、操作数栈、动态链接方法、返回值、返回地址等信息。每个方法从调用直结束就对于一个栈桢在虚拟机栈中的入栈和出栈过程 本地方法栈(Native Method Stacks)本地方法栈属于线程私有的数据区域，这部分主要与虚拟机用到的 Native 方法相关，一般情况下，我们无需关心此区域。 这里之所以简要说明这部分内容，注意是为了区别Java内存模型与Java内存区域的划分，毕竟这两种划分是属于不同层次的概念。 Java内存模型概述Java内存模型(即Java Memory Model，简称JMM)本身是一种抽象的概念，并不真实存在，它描述的是一组规则或规范，通过这组规范定义了程序中各个变量（包括实例字段，静态字段和构成数组对象的元素）的访问方式。由于JVM运行程序的实体是线程，而每个线程创建时JVM都会为其创建一个工作内存(有些地方称为栈空间)，用于存储线程私有的数据。 而Java内存模型中规定所有变量都存储在主内存，主内存是共享内存区域，所有线程都可以访问，但线程对变量的操作(读取赋值等)必须在工作内存中进行，首先要将变量从主内存拷贝的自己的工作内存空间，然后对变量进行操作，操作完成后再将变量写回主内存，不能直接操作主内存中的变量，工作内存中存储着主内存中的变量副本拷贝，前面说过，工作内存是每个线程的私有数据区域，因此不同的线程间无法访问对方的工作内存，线程间的通信(传值)必须通过主内存来完成，其简要访问过程如下图 需要注意的是，JMM与Java内存区域的划分是不同的概念层次，更恰当说JMM描述的是一组规则，通过这组规则控制程序中各个变量在共享数据区域和私有数据区域的访问方式 JMM是围绕原子性，有序性、可见性展开的。JMM与Java内存区域唯一相似点，都存在共享数据区域和私有数据区域，在JMM中主内存属于共享数据区域，从某个程度上讲应该包括了堆和方法区，而工作内存数据线程私有数据区域，从某个程度上讲则应该包括程序计数器、虚拟机栈以及本地方法栈。或许在某些地方，我们可能会看见主内存被描述为堆内存，工作内存被称为线程栈，实际上他们表达的都是同一个含义。关于JMM中的主内存和工作内存说明如下 主内存主要存储的是Java实例对象，所有线程创建的实例对象都存放在主内存中，不管该实例对象是成员变量还是方法中的本地变量(也称局部变量)，当然也包括了共享的类信息、常量、静态变量。由于是共享数据区域，多条线程对同一个变量进行访问可能会发现线程安全问题。 工作内存主要存储当前方法的所有本地变量信息(工作内存中存储着主内存中的变量副本拷贝)，每个线程只能访问自己的工作内存，即线程中的本地变量对其它线程是不可见的，就算是两个线程执行的是同一段代码，它们也会各自在自己的工作内存中创建属于当前线程的本地变量，当然也包括了字节码行号指示器、相关Native方法的信息。注意由于工作内存是每个线程的私有数据，线程间无法相互访问工作内存，因此存储在工作内存的数据不存在线程安全问题。 弄清楚主内存和工作内存后，接了解一下主内存与工作内存的数据存储类型以及操作方式，根据虚拟机规范，对于一个实例对象中的成员方法而言，如果方法中包含本地变量是基本数据类型（boolean,byte,short,char,int,long,float,double），将直接存储在工作内存的帧栈结构中，但倘若本地变量是引用类型，那么该变量的引用会存储在功能内存的帧栈中，而对象实例将存储在主内存(共享数据区域，堆)中。但对于实例对象的成员变量，不管它是基本数据类型或者包装类型(Integer、Double等)还是引用类型，都会被存储到堆区。至于static变量以及类本身相关信息将会存储在主内存中。需要注意的是，在主内存中的实例对象可以被多线程共享，倘若两个线程同时调用了同一个对象的同一个方法，那么两条线程会将要操作的数据拷贝一份到自己的工作内存中，执行完成操作后才刷新到主内存，简单示意图如下所示： Java内存模型的特地原子性原子性指的是一个操作是不可中断的，即使是在多线程环境下，一个操作一旦开始就不会被其他线程影响。比如对于一个静态变量int x，两条线程同时对他赋值，线程A赋值为1，而线程B赋值为2，不管线程如何运行，最终x的值要么是1，要么是2，线程A和线程B间的操作是没有干扰的，这就是原子性操作，不可被中断的特点。 可见性可见性指的是当一个线程修改了某个共享变量的值，其他线程是否能够马上得知这个修改的值。对于串行程序来说，可见性是不存在的，因为我们在任何一个操作中修改了某个变量的值，后续的操作中都能读取这个变量值，并且是修改过的新值。但在多线程环境中可就不一定了，前面我们分析过，由于线程对共享变量的操作都是线程拷贝到各自的工作内存进行操作后才写回到主内存中的，这就可能存在一个线程A修改了共享变量x的值，还未写回主内存时，另外一个线程B又对主内存中同一个共享变量x进行操作，但此时A线程工作内存中共享变量x对线程B来说并不可见，这种工作内存与主内存同步延迟现象就造成了可见性问题，另外指令重排以及编译器优化也可能导致可见性问题，通过前面的分析，我们知道无论是编译器优化还是处理器优化的重排现象，在多线程环境下，确实会导致程序轮序执行的问题，从而也就导致可见性问题。 有序性有序性是指对于单线程的执行代码，我们总是认为代码的执行是按顺序依次执行的，这样的理解并没有毛病，毕竟对于单线程而言确实如此，但对于多线程环境，则可能出现乱序现象，因为程序编译成机器码指令后可能会出现指令重排现象，重排后的指令与原指令的顺序未必一致，要明白的是，在Java程序中，倘若在本线程内，所有操作都视为有序行为，如果是多线程环境下，一个线程中观察另外一个线程，所有操作都是无序的，前半句指的是单线程内保证串行语义执行的一致性，后半句则指指令重排现象和工作内存与主内存同步延迟现象。 JMM提供的解决方案原子性问题，除了JVM自身提供的对基本数据类型读写操作的原子性外，对于方法级别或者代码块级别的原子性操作，可以使用synchronized关键字或者重入锁(ReentrantLock)保证程序执行的原子性 工作内存与主内存同步延迟现象可见性问题，可以使用synchronized关键字或者volatile关键字解决，它们都可以使一个线程修改后的变量立即对其他线程可见。 对于指令重排导致的可见性问题和有序性问题，则可以利用volatile关键字解决，因为volatile的另外一个作用就是禁止重排序优化，关于volatile稍后会进一步分析。 除了靠sychronized和volatile关键字来保证原子性、可见性以及有序性外，JMM内部还定义一套happens-before 原则来保证多线程环境下两个操作间的原子性、可见性以及有序性。 理解指令重排计算机在执行程序时，为了提高性能，编译器和处理器的常常会对指令做重排，一般分以下3种 编译器优化的重排 编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令并行的重排 现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性(即后一个执行的语句无需依赖前面执行的语句的结果)，处理器可以改变语句对应的机器指令的执行顺序 内存系统的重排 由于处理器使用缓存和读写缓存冲区，这使得加载(load)和存储(store)操作看上去可能是在乱序执行，因为三级缓存的存在，导致内存与缓存的数据同步存在时间差。 其中编译器优化的重排属于编译期重排，指令并行的重排和内存系统的重排属于处理器重排，在多线程环境中，这些重排优化可能会导致程序出现内存可见性问题 指令重排遵从两大原则 as-if-serial语义 happens-before规则 as-if-serial语义as-if-serial语义的意思是：不管怎么重排序，单个线程中的程序的执行结果不会改变。编译器、runtime和处理器都必须遵守as-if-serial语义。 为了遵守as-if-serial语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。 但是，如果操作之间不存在数据依赖关系，这些操作就可能被编译器和处理器重排序。 1234567891011// 这个时候如果多线程的情况下，CPU指令重排可能会先执行int b = 2;在执行int a = 1;// 因为这个程序中不管怎么执行，他的语义都不会变化 int a = 1; int b = 2; -------------------------------------------------------------------------// 这个时候即使是在多线程的情况下，CPU也不会对指令进行重排// 因为下面的语句依赖上面的语句，这种重排序会对程序结果产生影响 int a = 1; int b = a + 1; JMM中的happens-before 原则倘若在程序开发中，仅靠sychronized和volatile关键字来保证原子性、可见性以及有序性，那么编写并发程序可能会显得十分麻烦，幸运的是，在Java内存模型中，还提供了happens-before 原则来辅助保证程序执行的原子性、可见性以及有序性的问题，它是判断数据是否存在竞争、线程是否安全的依据，happens-before 原则内容如下 程序顺序原则，即在一个线程内必须保证语义串行性，也就是说按照代码顺序执行。 锁规则 解锁(unlock)操作必然发生在后续的同一个锁的加锁(lock)之前，也就是说，如果对于一个锁解锁后，再加锁，那么加锁的动作必须在解锁动作之后(同一个锁)。 volatile规则 volatile变量的写，先发生于读，这保证了volatile变量的可见性，简单的理解就是，volatile变量在每次被线程访问时，都强迫从主内存中读该变量的值，而当该变量发生变化时，又会强迫将最新的值刷新到主内存，任何时刻，不同的线程总是能够看到该变量的最新值。 线程启动规则 线程的start()方法先于它的每一个动作，即如果线程A在执行线程B的start方法之前修改了共享变量的值，那么当线程B执行start方法时，线程A对共享变量的修改对线程B可见 传递性 A先于B ，B先于C 那么A必然先于C 线程终止规则 线程的所有操作先于线程的终结，Thread.join()方法的作用是等待当前执行的线程终止。假设在线程B终止之前，修改了共享变量，线程A从线程B的join方法成功返回后，线程B对共享变量的修改将对线程A可见。 线程中断规则 对线程 interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过Thread.interrupted()方法检测线程是否中断。 对象终结规则 对象的构造函数执行，结束先于finalize()方法 volatile内存语义volatile在并发编程中很常见，但也容易被滥用，现在我们就进一步分析volatile关键字的语义。volatile是Java虚拟机提供的轻量级的同步机制。volatile关键字有如下两个作用 可见性 禁止指令重排序优化 volatile的可见性关于volatile的可见性作用，我们必须意识到被volatile修饰的变量对所有线程总数立即可见的，对volatile变量的所有写操作总是能立刻反应到其他线程中，但是对于volatile变量运算操作在多线程环境并不保证安全性，如下 1234567public class VolatileVisibility &#123; public static volatile int i =0; public static void increase()&#123; i++; &#125;&#125; 正如上述代码所示，i变量的任何改变都会立马反应到其他线程中，但是如此存在多条线程同时调用increase()方法的话，就会出现线程安全问题，毕竟i++;操作并不具备原子性，该操作是先读取值，然后写回一个新值，相当于原来的值加上1，分两步完成，如果第二个线程在第一个线程读取旧值和写回新值期间读取i的域值，那么第二个线程就会与第一个线程一起看到同一个值，并执行相同值的加1操作，这也就造成了线程安全失败，因此对于increase方法必须使用synchronized修饰，以便保证线程安全，需要注意的是一旦使用synchronized修饰方法后，由于synchronized本身也具备与volatile相同的特性，即可见性，因此在这样种情况下就完全可以省去volatile修饰变量。 1234567public class VolatileVisibility &#123; public static int i =0; public synchronized static void increase()&#123; i++; &#125;&#125; 现在来看另外一种场景，可以使用volatile修饰变量达到线程安全的目的，如下 1234567891011121314public class VolatileSafe &#123; volatile boolean close; public void close()&#123; close=true; &#125; public void doWork()&#123; while (!close)&#123; System.out.println(&quot;safe....&quot;); &#125; &#125;&#125; 由于对于boolean变量close值的修改属于原子性操作，因此可以通过使用volatile修饰变量close，使用该变量对其他线程立即可见，从而达到线程安全的目的。那么JMM是如何实现让volatile变量对其他线程立即可见的呢？实际上，当写一个volatile变量时，JMM会把该线程对应的工作内存中的共享变量值刷新到主内存中，当读取一个volatile变量时，JMM会把该线程对应的工作内存置为无效，那么该线程将只能从主内存中重新读取共享变量。volatile变量正是通过这种写-读方式实现对其他线程可见（但其内存语义实现则是通过内存屏障，稍后会说明）。 前我们说过当多个处理器的运算任务都涉及同一块主内存区域时，将可能导致各自的缓存数据不一致，举例说明变量在多个CPU之间的共享。 如果真的发生这种情况，那同步回到主内存时以谁的缓存数据为准呢？ 为了解决一致性的问题，需要各个处理器访问缓存时都遵循一些协议，在读写时要根据协议来进行操作，这类协议有MSI、MESI（IllinoisProtocol）、MOSI、Synapse、Firefly及DragonProtocol等。 MESI（缓存一致性协议）当CPU写数据时，如果发现操作的变量是共享变量，即在其他CPU中也存在该变量的副本，会发出信号通知其他CPU将该变量的缓存行置为无效状态，因此当其他CPU需要读取这个变量时，发现自己缓存中缓存该变量的缓存行是无效的，那么它就会从内存重新读取。 至于是怎么发现数据是否失效呢？嗅探每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作的时候，会重新从系统内存中把数据读到处理器缓存里。 总线风暴 由于Volatile的MESI缓存一致性协议，需要不断的从主内存嗅探和cas不断循环，无效交互会导致总线带宽达到峰值。 所以不要大量使用Volatile，至于什么时候去使用Volatile什么时候使用锁，根据场景区分。 volatile禁止重排优化volatile关键字另一个作用就是禁止指令重排优化，从而避免多线程环境下程序出现乱序执行的现象，关于指令重排优化前面已详细分析过，这里主要简单说明一下volatile是如何实现禁止指令重排优化的。 先了解一个概念，内存屏障(Memory Barrier）。内存屏障，又称内存栅栏，是一个CPU指令，它的作用有两个，一是保证特定操作的执行顺序，二是保证某些变量的内存可见性（利用该特性实现volatile的内存可见性）。 由于编译器和处理器都能执行指令重排优化。如果在指令间插入一条Memory Barrier则会告诉编译器和CPU，不管什么指令都不能和这条Memory Barrier指令重排序，也就是说通过插入内存屏障禁止在内存屏障前后的指令执行重排序优化。 Memory Barrier的另外一个作用是强制刷出各种CPU的缓存数据，因此任何CPU上的线程都能读取到这些数据的最新版本。总之，volatile变量正是通过内存屏障实现其在内存中的语义，即可见性和禁止重排优化。 屏障类型 指令示例 说明 LoadLoad Load1;LoadLoad;Load2 保证Load1的读取操作在Load2及后续读取操作之前执行 StoreStore Store1;StoreStore;Store2 在Store2及其后的写操作执行前，保证Store1的写操作已刷新到主内存 LoadStore Load1;LoadStore;Store2 在Store2及其后的写操作执行前，保证Load1的读操作已读取结束 StoreLoad Store1;StoreLoad;Load2 保证load1的写操作已刷新到主内存之后，load2及其后的读操作才能执行 1234567891011121314151617class Singleton &#123; // 可见性和指令重排都保证 private volatile static Singleton instance = null; private Singleton()&#123;&#125; public static Singleton getInstance()&#123; // 第一次检查锁定 if(instance == null)&#123; // 同步锁定代码块 synchronized(Singleton.class)&#123; if(instance == null)&#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125; volatile与synchronized的区别volatile只能修饰实例变量和类变量，而synchronized可以修饰方法，以及代码块。 volatile保证数据的可见性，但是不保证原子性(多线程进行写操作，不保证线程安全);而synchronized是一种排他(互斥)的机制。 volatile用于禁止指令重排序：可以解决单例双重检查对象初始化代码执行乱序问题。 volatile可以看做是轻量版的synchronized，volatile不保证原子性，但是如果是对一个共享变量进行多个线程的赋值，而没有其他的操作，那么就可以用volatile来代替synchronized，因为赋值本身是有原子性的，而volatile又保证了可见性，所以就可以保证线程安全了。 总结 volatile修饰符适用于以下场景：某个属性被多个线程共享，其中有一个线程修改了此属性，其他线程可以立即得到修改后的值，比如booleanflag;或者作为触发器，实现轻量级同步。 volatile属性的读写操作都是无锁的，它不能替代synchronized，因为它没有提供原子性和互斥性。因为无锁，不需要花费时间在获取锁和释放锁_上，所以说它是低成本的。 volatile只能作用于属性，我们用volatile修饰属性，这样compilers就不会对这个属性做指令重排序。 volatile提供了可见性，任何一个线程对其的修改将立马对其他线程可见，volatile属性不会被线程缓存，始终从主 存中读取。 volatile提供了happens-before保证，对volatile变量v的写入happens-before所有其他线程后续对v的读操作。 volatile可以使得long和double的赋值是原子的。 volatile可以在单例双重检查中实现可见性和禁止指令重排序，从而保证安全性。","categories":[{"name":"Java","slug":"Java","permalink":"https://zhaoshuchao.top/categories/Java/"},{"name":"JUC","slug":"Java/JUC","permalink":"https://zhaoshuchao.top/categories/Java/JUC/"}],"tags":[{"name":"JUC","slug":"JUC","permalink":"https://zhaoshuchao.top/tags/JUC/"},{"name":"并发编程","slug":"并发编程","permalink":"https://zhaoshuchao.top/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"MySQL中的事务","slug":"MySQL中的事务","date":"2021-05-02T05:47:39.000Z","updated":"2021-06-02T08:30:05.996Z","comments":true,"path":"2021/05/01/MySQL中的事务/","link":"","permalink":"https://zhaoshuchao.top/2021/05/01/MySQL%E4%B8%AD%E7%9A%84%E4%BA%8B%E5%8A%A1/","excerpt":"","text":"什么是事务事务是由一条或者多条DML语句组成逻辑执行单元，这系列操作要么全部执行成功，要么全部执行失败。通常一个事务对应一个完整的业务(例如银行账户转账业务，该业务就是一个最小的工作单元) 事务的特性（ACID）原子性(atomicity)： 事务的最小工作单元，要么全成功，要么全失败。 一致性(consistency)： 事务开始和结束后，数据库的完整性不会被破坏。（最重要） 隔离性(isolation)： 不同事务之间互不影响，四种隔离级别为RU（读未提交）、RC（读已提交）、RR（可重复读）、SERIALIZABLE （串行化）。隔离级别由低到高，性能由高到低。隔离级别总是和性能相违背。 持久性(durability)： 事务提交后，对数据的修改是永久性的，即使系统故障也不会丢失。 ACID之间的关系： 只有满足一致性，事务的结果才是正确的。 在无并发的情况下，事务串行执行，隔离性一定能够满足。此时只要能满足原子性，就一定能满足一致性。在并发的情况下，多个事务并行执行，事务不仅要满足原子性，还需要满足隔离性，才能满足一致性。 事务满足持久化是为了能应对数据库崩溃的情况。 Innodb的隔离性有哪些 未提交读（READ UNCOMMITTED） 事务中的修改，即使没有提交，对其他事务也是可见的。 提交读（READ COMMITTED） 一个事务只能读取已经提交的事务所做的修改。换句话说，一个事务所做的修改在提交之前对其他事务是不可见的。 又称为不可重复读，一个事务因为读取到另一个事务已提交的修改数据，导致在当前事务的不同时间读取同一条数据获取的结果不一致。 可重复读（REPEATABLE READ） 保证在同一个事务中多次读取同样数据的结果是一样的。 在当前事务中，不论读取多少次，数据任然是第一次读取的值，不会因为在第一次读取之后，其他事务再修改提交此数据而产生改变。 串行化（SERIALIZABLE） 强制事务串行执行。 所有的数据库的读或者写操作都为串行执行，读加读锁，写加写锁。当前隔离级别下只支持单个请求同时执行，所有的操作都需要队列执行。所以种隔离级别下所有的数据是最稳定的，但是性能也是最差的。 每个隔离性会造成什么问题 脏读： （针对未提交数据）如果一个事务中对数据进行了更新，但事务还没有提交，另一个事务可以“看到”该事务没有提交的更新结果，这样造成的问题就是，如果第一个事务回滚，那么，第二个事务在此之前所“看到”的数据就是一笔脏数据。 不可重复读 （针对其他提交前后，读取数据本身的对比）不可重复读取是指同一个事务在整个事务过程中对同一笔数据进行读取，每次读取结果都不同。如果事务1在事务2的更新操作之前读取一次数据，在事务2的更新操作之后再读取同一笔数据一次，两次结果是不同的，所以，Read Uncommitted也无法避免不可重复读取的问题。 幻读 （针对其他提交前后，读取数据条数的对比） 幻读是指同样一笔查询在整个事务过程中多次执行后，查询所得的结果集是不一样的。幻读针对的是多笔记录。在Read Uncommitted隔离级别下， 不管事务2的插入操作是否提交，事务1在插入操作之前和之后执行相同的查询，取得的结果集是不同的，所以，Read Uncommitted同样无法避免幻读的问题。 不可重复读的重点是修改:同样的条件, 你读取过的数据, 再次读取出来发现值不一样了 幻读的重点在于新增或者删除 (数据条数变化) 比如： select * from tableName where id = 1 ； 前后前次读取该id=1的数据字段值值不一致，不可重复读； select * from tableName where id &gt; 3 ; 前后两次读取数据条数不一致，幻读； 隔离级别 脏读 不可重复读 幻读 未提交读 √ √ √ 提交读 × √ √ 可重复读 × × √（在MySQL中，加入临键锁可以防止） 可串行化 × × × 事务怎么保证ACID事务怎么保证一致性这个问题分为两个层面来说。 从数据库层面，数据库通过原子性、隔离性、持久性来保证一致性。也就是说ACID四大特性之中，C(一致性)是目的，A(原子性)、I(隔离性)、D(持久性)是手段，是为了保证一致性，数据库提供的手段。数据库必须要实现AID三大特性，才有可能实现一致性。例如，原子性无法保证，显然一致性也无法保证。 但是，如果你在事务里故意写出违反约束的代码，一致性还是无法保证的。例如，你在转账的例子中，你的代码里故意不给B账户加钱，那一致性还是无法保证。因此，还必须从应用层角度考虑。 从应用层面，通过代码判断数据库数据是否有效，然后决定回滚还是提交数据！ 事务怎么保证原子性是利用Innodb的undo log。 undo-log名为回滚日志，是实现原子性的关键，当事务回滚时能够撤销所有已经执行成功的SQL语句，他需要记录你要回滚的相应日志信息。实际上就是记录SQL操作的相反SQL操作，SQL为insert，undolog中记录一条delete。rollback操作实际就是执行undolog的SQL起到覆盖作用。 例如 (1)当你delete一条数据的时候，就需要记录这条数据的信息，回滚的时候，insert这条旧数据 (2)当你update一条数据的时候，就需要记录之前的旧值，回滚的时候，根据旧值执行update操作 (3)当年insert一条数据的时候，就需要这条记录的主键，回滚的时候，根据主键执行delete操作 undo log记录了这些回滚需要的信息，当事务执行失败或调用了rollback，导致事务需要回滚，便可以利用undo log中的信息将数据回滚到修改之前的样子。 事务怎么保证持久性 在Mysql中，为了解决CPU和磁盘速度不一致问题，Mysql是将磁盘上的数据加载到内存，对内存进行操作，然后再回写磁盘。好，假设此时宕机了，在内存中修改的数据全部丢失了，持久性就无法保证。 是利用Innodb中的redolog。 正如上面所说，MySQL是先把磁盘上的数据加载到内存当中。当做数据修改的时候，不仅在内存中操作，还会在redolog中记录这次的操作。当事务提交的时候，会将redolog日志进行刷盘（redolog一部分在内存中（redolog buffer ），一部分在磁盘中）。当数据库宕机重启的时候，会将redolog中的内容回复到数据库中。在根据binlog和undolog内容决定回滚数据还是提交事务。 为什么要采用redolog刷盘，而不是每次提交事务进行数据刷盘？ MySQL加载的内存中Buffer pool 的数据最小单位是页。一页有16K大小。只修改一个页的几个字节，就要将整个页面刷入磁盘，太浪费性能和资源。而且一个事务中的SQL可能牵涉到多个数据页的修改，而这次数据页可能也不是相邻的。也就是刷盘的时候要进行随机IO，速度会比较慢。 而redo log 体积小，毕竟只记录了哪一页修改了啥，因此体积小，刷盘快。redo log是一直向末尾进行追加，输入顺序IO，效率快（这个刷盘指的是redo log内存中的数据刷盘到磁盘中的redolog。若MySQL服务器正常，则redolog不会对数据库进行操作，只有当mysql出现宕机重启，才会触发redolog写入数据库中。） MySQL会预读数据，能把一些“可能要访问”的页提前加入缓冲池buffer poll，避免未来的磁盘IO操作；管理算法是LRU 具体详解：https://blog.csdn.net/suo082407128/article/details/102580630 Redo log刷盘机制 redo log日志在磁盘的大小是固定的，即记录满了以后就从头循环写。 参数：innodb_flush_log_at_trx_commit innodb事物提交的重要参数，取值只有0、1、2，默认为1，动态参数。 该参数是用来控制提交操作和高性能的，如果需要更高的性能，在crash时可能存在数据丢失风险，也就是不具备ACID的持久性。 0：log每秒写入磁盘。crash时可能会丢失数据（因为那些已提交完成的事物还没有落盘） 1：默认值是1，在事务提交时刷脏数据到盘 2：log每秒写入磁盘，且在每次提交时写入磁盘。此状态时也不会丢失数据。 Buffer Pool的刷盘机制 1、当innodb中的脏页比例超过innodb_max_dirty_pages_pct_lwm的值时（默认值为75），这个时候就会开始刷新脏页到磁盘。 2、当innodb中的脏页比例超过innodb_max_dirty_pages_pct_lwm的值，而且还超过innodb_max_dirty_pages_pct时 innodb就会进入勤快刷新模式(agressively flush）这个模式下innodb会把脏页更快的刷新到磁盘。 3、还有一种情况叫做sharp checkpoint ,当innodb要重用它之前的redo文件时，就会把innodb_buffer_pool中所有与这 个文件有关的页面都要刷新到磁盘；这样做就有可能引起磁盘的IO风暴了，轻者影响性能，重者影响可用性。 可以简单的理解为：每秒都会进行一次刷盘（checkpoint：定期将db buffer的内容刷盘）。当脏页数据太快的时候，会触发innodb的勤快刷新模式。 参考文章： https://blog.csdn.net/molaifeng/article/details/113820047 https://www.cnblogs.com/JiangLe/p/7419835.html?utm_source=itdadao&amp;utm_medium=referral https://www.jianshu.com/p/06d09aa71a15 事务怎么保证隔离性RC,RR–MVCC 因为READ UNCOMMITIED总是读取最新的数据行，而不是符合当前事务版本的数据行。 而SERIALIZABLE则会对所有读取的行都加锁。 MVCC是什么MVCC的概念多版本并发控制（Multi-Version Concurrency Control, MVCC）是 MySQL 的 InnoDB 存储引擎实现隔离级别的一种具体方式，用于实现提交读和可重复读这两种隔离级别。 而未提交读隔离级别总是读取最新的数据行，无需使用 MVCC。 可串行化隔离级别需要对所有读取的行都加锁，单纯使用 MVCC 无法实现。 MVCC是一种用来解决读-写冲突的无锁并发控制，其基本思想是为每次事务生成一个新版本的数据，为每个修改保存一个版本，版本与事务时间戳关联，读操作只读该事务开始前的数据库的快照。 这样在读操作不用阻塞写操作，写操作不用阻塞读操作的同时，避免了脏读和不可重复读。 MVCC 在mysql 中的实现依赖的是 undo log 与 read view 。 名词解释：版本号 系统版本号：是一个递增的数字，每开始一个新的事务，系统版本号就会自动递增。 事务版本号：事务开始时的系统版本号。 隐藏的列 MVCC 在每行记录后面都保存着两个隐藏的列，用来存储两个版本号： trx_id：每次一个事务对某条记录进行修改时，都会把该事务的事务id赋值给trx_id隐藏列； roll_pointer：每次对一个记录进行修改时，都会把旧版本写入undo日志中，然后把这个隐藏列就相当于一个指针，可以通过它来找到该记录修改前的信息； 创建版本号：指示创建一个数据行的快照时的系统版本号； 删除版本号：如果该快照的删除版本号大于当前事务版本号表示该快照有效，否则表示该快照已经被删除了。 MVCC是怎么保证隔离级别的以下实现过程针对可重复读隔离级别： SELECT InnoDB 会根据以下两个条件检查每行记录： InnoDB只查找版本早于当前事务版本的数据行（也就是，行的事务编号小于或等于当前事务的事务编号），这样可以确保事务读取的行，要么是在事务开始前已经存在的，要么是事务自身插入或者修改过的。 删除的行要事务ID判断，读取到事务开始之前状态的版本，只有符合上述两个条件的记录，才能返回作为查询结果。 INSERT InnoDB为新插入的每一行保存当前事务编号作为行版本号。 DELETE InnoDB为删除的每一行保存当前事务编号作为行删除标识。 UPDATE InnoDB为插入一行新记录，保存当前事务编号作为行版本号，同时保存当前事务编号到原来的行作为行删除标识。 保存这两个额外事务编号，使大多数读操作都可以不用加锁。这样设计使得读数据操作很简单，性能很好，并且也能保证只会读取到符合标准的行。不足之处是每行记录都需要额外的存储空间，需要做更多的行检查工作，以及一些额外的维护工作。 MVCC 在mysql 中的实现MVCC 在mysql 中的实现依赖的是 undo log 与 read view 。 undo log根据行为的不同，undo log分为两种： insert undo log 和 update undo log insert undo log insert 操作中产生的undo log，因为insert操作记录只对当前事务本身可见，对于其他事务此记录不可见，所以 insert undo log 可以在事务提交后直接删除而不需要进行purge操作。 purge的主要任务是将数据库中已经 mark del 的数据删除，另外也会批量回收undo pages 数据库 Insert时的数据初始状态：此时回滚指针是null; update undo log： update 或 delete 操作中产生的 undo log。 因为会对已经存在的记录产生影响，为了提供 MVCC机制，因此update undo log 不能在事务提交时就进行删除，而是将事务提交时放到入 history list 上，等待 purge 线程进行最后的删除操作。 数据第一次被修改时： 当另一个事务第二次修改当前数据： 为了保证事务并发操作时，在写各自的undo log时不产生冲突，InnoDB采用回滚段的方式来维护undo log的并发写入和持久化。回滚段实际上是一种 Undo 文件组织方式 ReadView对于 RU(READ UNCOMMITTED) 隔离级别下，所有事务直接读取数据库的最新值即可，和 SERIALIZABLE 隔离级别，所有请求都会加锁，同步执行。所以这对这两种情况下是不需要使用到 Read View 的版本控制。 对于 RC(READ COMMITTED) 和 RR(REPEATABLE READ) 隔离级别的实现就是通过上面的版本控制来完成。两种隔离界别下的核心处理逻辑就是判断所有版本中哪个版本是当前事务可见的处理。针对这个问题InnoDB在设计上增加了ReadView的设计，ReadView中主要包含当前系统中还有哪些活跃的读写事务，把它们的事务id放到一个列表中，我们把这个列表命名为为m_ids。 对于查询时的版本链数据是否看见的判断逻辑： 如果被访问版本的 trx_id 属性值小于 m_ids 列表中最小的事务id，表明生成该版本的事务在生成 ReadView 前已经提交，所以该版本可以被当前事务访问。 如果被访问版本的 trx_id 属性值大于 m_ids 列表中最大的事务id，表明生成该版本的事务在生成 ReadView 后才生成，所以该版本不可以被当前事务访问。 如果被访问版本的 trx_id 属性值在 m_ids 列表中最大的事务id和最小事务id之间，那就需要判断一下 trx_id 属性值是不是在 m_ids 列表中，如果在，说明创建 ReadView 时生成该版本的事务还是活跃的，该版本不可以被访问；如果不在，说明创建 ReadView 时生成该版本的事务已经被提交，该版本可以被访问。 这个ReadView可以理解为一个读视图。 当隔离级别为RR时：只会在第一次快照读的时候生成ReadView,之后执行的快照读都是用第一次生成的readView，在该级别的事务中，两次相同的select查询返回的结果是相同的，无关其他事务已经提交的修改结果。（保证可重复读）。当时进行update，insert，delete，以及select…for update 操作时，更新一次readView视图。（保证其他事务修改记录不会丢失） 当隔离级别为RC是：每次进行快照读的时候都会生成一个ReadView，所以其他事务已经提交的数据能够看到。这时执行相同的select语句返回的结果可能不是相同的，因为两次执行之间可能存在别的事务提交了当前查询列的修改，第二次select时刷新到最新的一个ReadView。（造成不可重复读） MVCC总结：所谓的MVCC（Multi-Version Concurrency Control ，多版本并发控制）指的就是在使用 READ COMMITTD 、REPEATABLE READ 这两种隔离级别的事务在执行普通的 SEELCT 操作时访问记录的版本链的过程，这样子可以使不同事务的 读-写 、 写-读 操作并发执行，从而提升系统性能。 在 MySQL 中， READ COMMITTED 和 REPEATABLE READ 隔离级别的的一个非常大的区别就是它们生成 ReadView 的时机不同。在 READ COMMITTED 中每次快照读都会生成一个实时的 ReadView，做到保证每次提交后的数据是处于当前的可见状态。而 REPEATABLE READ 中，在当前事务第一次查询时生成当前的 ReadView，并且当前的 ReadView 会一直沿用到当前事务提交，以此来保证可重复读（REPEATABLE READ）。 快照读和当前读在可重复读级别中，通过MVCC机制，虽然让数据变得可重复读，但我们读到的数据可能是历史数据，是不及时的数据，不是数据库当前的数据！这在一些对于数据的时效特别敏感的业务中，就很可能出问题。 对于这种读取历史数据的方式，我们叫它快照读 (snapshot read)，而读取数据库当前版本数据的方式，叫当前读 (current read)。很显然，在MVCC中： 快照读 MVCC 的 SELECT 操作是快照中的数据，不需要进行加锁操作。 1select * from table ….; 当前读 MVCC 其它会对数据库进行修改的操作（INSERT、UPDATE、DELETE）需要进行加锁操作，从而读取最新的数据。可以看到 MVCC 并不是完全不用加锁，而只是避免了 SELECT 的加锁操作。 12345678INSERT;UPDATE;DELETE;在进行 SELECT 操作时，可以强制指定进行加锁操作。以下第一个语句需要加 S 锁，第二个需要加 X 锁。- select * from table where ? lock in share mode; 共享锁 允许多个线程读取数据，不允许修改数据（仅可读）- select * from table where ? for update; 排他锁 只允许一个线程读写。其他线程阻塞 S锁 和 X锁 都是悲观锁 事务的隔离级别实际上都是定义的当前读的级别，MySQL为了减少锁处理（包括等待其它锁）的时间，提升并发能力，引入了快照读的概念，使得select不用加锁。而update、insert这些“当前读”的隔离性，就需要通过加锁来实现了。 RR隔离级别下解决幻读–临键锁Record Lock 行锁单个行记录上的锁我们通常讲的行锁，它的实质是通过对索引的加锁实现；只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁。在事务隔离级别为读已提交下，仅采用Record Lock。 如果表没有设置索引，InnoDB 会自动在主键上创建隐藏的聚簇索引，因此 Record Locks 依然可以使用。 Gap Lock 间隙锁锁定索引之间的间隙，但是不包含索引本身。例如当一个事务执行以下语句，其它事务就不能在 t.c 中插入 15。 1SELECT c FROM t WHERE c BETWEEN 10 and 20 FOR UPDATE; Next-Key Lock 临键锁它是 Record Locks 和 Gap Locks 的结合，不仅锁定一个记录上的索引，也锁定索引之间的间隙。例如一个索引包含以下值：10, 11, 13, and 20，那么就需要锁定以下区间： 12345(-∞, 10](10, 11](11, 13](13, 20](20, +∞) 在 InnoDB 存储引擎中，SELECT 操作的幻读问题通过 MVCC 的快照读得到了解决， 而 UPDATE、DELETE 的幻读问题通过 Record Lock 解决，INSERT 的不可重复读问题是通过 Next-Key Lock（Record Lock + Gap Lock）解决的。间隙锁的引入，可能会导致同样的语句锁住更大的范围，这其实是影响了并发度的。 我们总是牺牲性能来换取安全稳定。 串行化是怎么加锁的串行化就相当于给操作的记录上一个共享锁（读写锁），即当读某条记录时就占用这条记录的读锁，此时其它事务一样可以申请到这条记录的读锁来读取，但是不能写（读锁被占的话，写锁就不能被占；读锁可以被多个事务同时占有） MySQL中锁MySQL锁概述相对其他数据库而言，MySQL的锁机制比较简单，其最 显著的特点是不同的存储引擎支持不同的锁机制。比如，MyISAM和MEMORY存储引擎采用的是表级锁（table-level locking）；BDB存储引擎采用的是页面锁（page-level locking），但也支持表级锁；InnoDB存储引擎既支持行级锁（row-level locking），也支持表级锁，但默认情况下是采用行级锁。表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低。行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。页面锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般从上述特点可见，很难笼统地说哪种锁更好，只能就具体应用的特点来说哪种锁更合适！仅从锁的角度 来说：表级锁更适合于以查询为主，只有少量按索引条件更新数据的应用，如Web应用；而行级锁则更适合于有大量按索引条件并发更新少量不同数据，同时又有 并发查询的应用，如一些在线事务处理（OLTP）系统。 死锁和死锁检测在并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁 当出现死锁以后，有两种策略： 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数innodb_lock_wait_timeout来设置另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect设置为on，表示开启这个逻辑 乐观锁和悲观锁乐观锁不是数据库自带的，需要我们自己去实现。乐观锁是指操作数据库时(更新操作)，想法很乐观，认为这次的操作不会导致冲突，在操作数据时，并不进行任何其他的特殊处理（也就是不加锁），而在进行更新后，再去判断是否有冲突了。通常实现是这样的：在表中的数据进行操作时(更新)，先给数据表加一个版本(version)字段，每操作一次，将那条记录的版本号加1。也就是先查询出那条记录，获取出version字段,如果要对那条记录进行操作(更新),则先判断此刻version的值是否与刚刚查询出来时的version的值相等，如果相等，则说明这段期间，没有其他程序对其进行操作，则可以执行更新，将version字段的值加1；如果更新时发现此刻的version值与刚刚获取出来的version的值不相等，则说明这段期间已经有其他程序对其进行操作了，则不进行更新操作。 与乐观锁相对应的就是悲观锁了。悲观锁就是在操作数据时，认为此操作会出现数据冲突，所以在进行每次操作时都要通过获取锁才能进行对相同数据的操作，这点跟java中的synchronized很相似，所以悲观锁需要耗费较多的时间。另外与乐观锁相对应的，悲观锁是由数据库自己实现了的，要用的时候，我们直接调用数据库的相关语句就可以了。 共享锁和排它锁共享锁(S锁)和排它锁（X锁）是悲观锁的不同的实现，它俩都属于悲观锁的范畴。 共享锁指的就是对于多个不同的事务，对同一个资源共享同一个锁。相当于对于同一把门，它拥有多个钥匙一样 如果事务T对数据A加上共享锁后，则其他事务只能对A再加共享锁，不能加排他锁。获准共享锁的事务只能读数据，不能修改数据。 对于悲观锁，一般数据库已经实现了，共享锁也属于悲观锁的一种，那么共享锁在mysql中是通过什么命令来调用呢。通过查询资料，了解到通过在执行语句后面加上 lock in share mode就代表对某些资源加上共享锁了。 排它锁与共享锁相对应，就是指对于多个不同的事务，对同一个资源只能有一把锁。 如果事务T对数据A加上排他锁后，则其他事务不能再对A加任任何类型的封锁。获准排他锁的事务既能读数据，又能修改数据。 与共享锁类型，在需要执行的语句后面加上 for update就可以了 共享锁 允许多个线程读取数据，不允许修改数据,所有的线程都是仅可读 排他锁 只允许一个线程读写。其他线程阻塞 InnoDB默认采用行锁，在未使用索引字段查询时升级为表锁。MySQL这样设计并不是给你挖坑。它有自己的设计目的。即便你在条件中使用了索引字段，MySQL会根据自身的执行计划，考虑是否使用索引(所以explain命令中会有possible_key 和 key)。如果MySQL认为全表扫描效率更高，它就不会使用索引，这种情况下InnoDB将使用表锁，而不是行锁。因此，在分析锁冲突时，别忘了检查SQL的执行计划，以确认是否真正使用了索引。 第一种情况：全表更新。事务需要更新大部分或全部数据，且表又比较大。若使用行锁，会导致事务执行效率低，从而可能造成其他事务长时间锁等待和更多的锁冲突。 第二种情况：多表查询。事务涉及多个表，比较复杂的关联查询，很可能引起死锁，造成大量事务回滚。这种情况若能一次性锁定事务涉及的表，从而可以避免死锁、减少数据库因事务回滚带来的开销。 一致性锁定读和一致性非锁定读在默认配置下innodb的隔离级别是repeatable read，innodb的select操作使用的是一致性非锁定读 一致性的非锁定行读（consistent nonlocking read，简称CNR）是指InnoDB存储引擎通过行多版本控制（multi versioning）的方式来读取当前执行时间数据库中运行的数据。如果读取的行正在执行delete、update操作，这时读取操作不会因此而会等待行上锁的释放，相反，InnoDB存储引擎会去读取行的一个快照数据。 之所以称为非锁定度，是因为不需要等待访问数据行上的X锁的释放。快照数据是指该行之前版本的数据，通过undo段来实现（undo用来在事务中回滚数据）。 在Read Committed和Repeatable Read模式下，innodb存储引擎使用默认的一致性非锁定读。在Read Committed隔离级别下，对于快照数据，一致性非锁定读总是读取被锁定行的最新一份快照数据；而在Repeatable Read隔离级别下，对于快照数据，一致性非锁定读总是读取事务开始时的行数据版本。 一致性锁定读默认情况下，innodb存储引擎的select操作使用一致性非锁定读，但是在某些情况下，需要对读操作进行加锁以保证数据逻辑的一致性。Innodb存储引擎对select语句支持2种一致性锁定读(locking read)操作; SELECT … FOR UPDATE对于读取的行记录加一个X排它锁，其他事务不能对锁定的行加任何锁。 SELECT … LOCK IN SHARE MODE对于读取的行记录添加一个S共享锁。其它事务可以向被锁定的行加S锁，但是不允许添加X锁，否则会被阻塞。 简单理解就是一致性非锁定读就是快照读 : select….; 一致性锁定读就是当前读: select…for update; MySQL的三种日志重做日志（redo log）确保事务的持久性。防止在发生故障的时间点，尚有脏页未写入磁盘，在重启mysql服务的时候，根据redo log进行重做，从而达到事务的持久性这一特性。 回滚日志（undo log）保存了事务发生之前的数据的一个版本，可以用于回滚，同时可以提供多版本并发控制下的读（MVCC） 在数据修改的时候，不仅记录了redo，还记录了相对应的undo，如果因为某些原因导致事务失败或回滚了，可以借助该undo进行回滚。 undo log和redo log记录物理日志不一样，它是逻辑日志。可以认为当delete一条记录时，undo log中会记录一条对应的insert记录，反之亦然，当update一条记录时，它记录一条对应相反的update记录。 二进制日志（binlog）用于复制，在主从复制中，从库利用主库上的binlog进行重播，实现主从同步。用于数据库的基于时间点的还原。 Mysql的日志系统主要有redo log（重做日志）和binlog （归档日志）。redo log是Innodb存储引擎层面的日志，binlog是MySQL Server层的记录日志。两者都是记录了某种操作的日志，自然会有写重复。 MySQL的逻辑架构图： redo log日志模块redo log是InnoDB存储引擎层的日志，又称重做日志文件，用于记录事务操作的变化，记录的是数据修改之后的值，不管事务是否提交都会记录下来。在实例和介质失败（media failure）时，redo log文件就能派上用场，如数据库掉电，InnoDB存储引擎会使用redo log恢复到掉电前的时刻，以此来保证数据的完整性。 在一条更新语句进行执行的时候，InnoDB引擎会把更新记录写到redo log日志中，然后更新内存，此时算是语句执行完了，然后在空闲的时候或者是按照设定的更新策略将redo log中的内容更新到磁盘中，这里涉及到WAL即Write Ahead logging技术，他的关键点是先写日志，再写磁盘。兵马未动，粮草先行 有了redo log日志，那么在数据库进行异常重启的时候，可以根据redo log日志进行恢复，也就达到了crash-safe。 redo log日志的大小是固定的，即记录满了以后就从头循环写。 binlog日志模块binlog是属于MySQL Server层面的，又称为归档日志，属于逻辑日志，是以二进制的形式记录的是这个语句的原始逻辑，依靠binlog是没有crash-safe能力的 binlog是记录所有数据库表结构变更（例如CREATE、ALTER TABLE…）以及表数据修改（INSERT、UPDATE、DELETE…）的二进制日志。，如果update操作没有造成数据变化，也是会记入binlog。 MySQL binlog的三种工作模式：ROW（行模式）, Statement（语句模式）, Mixed（混合模式） redo log和binlog区别 redo log是属于innoDB层面，binlog属于MySQL Server层面的，这样在数据库用别的存储引擎时可以达到一致性的要求。 redo log是物理日志，记录该数据页更新的内容；binlog是逻辑日志，记录的是这个更新语句的原始逻辑 redo log是循环写，日志空间大小固定；binlog是追加写，是指一份写到一定大小的时候会更换下一个文件，不会覆盖。 binlog可以作为恢复数据使用，主从复制搭建，redo log作为异常宕机或者介质故障后的数据恢复使用。 一条更新语句执行的顺序二阶段提交 update T set c=c+1 where ID=2; 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。 WAL WAL（Write-ahead logging，预写式日志）是数据库系统提供原子性和持久化的一系列技术。 在使用 WAL 的系统中，所有的修改在提交之前都要先写入 log 文件中。 MySQL的WAL（Write Ahead Log）在InnoDB中被称作redo log 「修改并不直接写入到数据库文件中，而是写入到另外一个称为 WAL 的文件中；如果事务失败，WAL 中的记录会被忽略，撤销修改；如果事务成功，它将在随后的某个时间被写回到数据库文件中，提交修改。」 WAL 的优点 读和写可以完全地并发执行，不会互相阻塞（但是写之间仍然不能并发）。 WAL 在大多数情况下，拥有更好的性能（因为无需每次写入时都要写两个文件）。 磁盘 I/O 行为更容易被预测。 使用更少的 fsync()操作，减少系统脆弱的问题。 二阶段提交（prepare ，commit） 两阶段提交原理描述:阶段1：InnoDB redo log 写盘，InnoDB 事务进入 prepare 状态 阶段2：如果前面prepare成功，binlog 写盘，那么再继续将事务日志持久化到binlog，如果持久化成功，那么InnoDB 事务 则进入 commit 状态(实际是在redo log里面写上一个commit记录) 备注: 每个事务binlog的末尾，会记录一个 XID event，标志着事务是否提交成功，也就是说，recovery 过程中，binlog 最后一个 XID event 之后的内容都应该被 purge。 最终:mysql在落盘日志的时候,先落盘binlog,再落盘redo. 组提交组提交概念：将多个刷盘操作合并成一个，最大化每次刷盘手里，提升性能，降低资源开销。 在没有开启binlog时：Redo log的刷盘操作将会是最终影响MySQL TPS的瓶颈所在。为了缓解这一问题，MySQL使用了组提交，将多个刷盘操作合并成一个，如果说10个事务依次排队刷盘的时间成本是10，那么将这10个事务一次性一起刷盘的时间成本则近似于1。 当开启binlog时：为了保证Redo log和binlog的数据一致性，MySQL使用了二阶段提交，由binlog作为事务的协调者。而 引入二阶段提交 使得binlog又成为了性能瓶颈，先前的Redo log 组提交 也成了摆设。为了再次缓解这一问题，MySQL增加了binlog的组提交，目的同样是将binlog的多个刷盘操作合并成一个，结合Redo log本身已经实现的 组提交，分为三个阶段(Flush 阶段、Sync 阶段、Commit 阶段)完成binlog 组提交，最大化每次刷盘的收益，弱化磁盘瓶颈，提高性能。 组提交参考文章：https://blog.csdn.net/n88Lpo/article/details/81187372","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://zhaoshuchao.top/categories/MySQL/"},{"name":"事务","slug":"MySQL/事务","permalink":"https://zhaoshuchao.top/categories/MySQL/%E4%BA%8B%E5%8A%A1/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://zhaoshuchao.top/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"https://zhaoshuchao.top/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"线程通讯","slug":"线程通讯","date":"2021-04-25T07:37:48.000Z","updated":"2021-06-02T08:32:19.916Z","comments":true,"path":"2021/04/24/线程通讯/","link":"","permalink":"https://zhaoshuchao.top/2021/04/24/%E7%BA%BF%E7%A8%8B%E9%80%9A%E8%AE%AF/","excerpt":"","text":"概述 线程与线程之间不是相互独立的个体，它们彼此之间需要相互通信和协作。 最典型的例子就是生产者-消费者问题：当队列满时，生产者需要等待队列有空间才能继续往里面放入商品，而在等待的期间内，生产者必须释放对临界资源（即队列）的占用权。因为生产者如果不释放对临界资源的占用权，那么消费者就无法消费队列中的商品，就不会让队列有空间，那么生产者就会一直无限等待下去。因此一般情况下，当队列满时，会让生产者交出对临界资源的占用权，并进入挂起状态。然后等待消费者消费了商品，然后消费者通知生产者队列有空间了。同样地，当队列空时，消费者也必须等待，等待生产者通知它队列中有商品了。这种互相通信的过程就是线程间的协作 wait()-notify()wait()、notify() 和 notifyAll()方法是 本地方法，并且为 final 方法，无法被重写； 调用某个对象的 wait() 方法能让 当前线程释放锁并且进入（等待）阻塞，并且当前线程必须拥有此对象的monitor（即锁）； 调用某个对象的 notify() 方法能够唤醒 一个正在等待这个对象的monitor的线程，如果有多个线程都在等待这个对象的monitor，则只能唤醒其中一个线程； 调用notifyAll()方法能够唤醒所有正在等待这个对象的monitor的线程。 在使用这3个方法时，必须处于synchronized代码块或者synchronized方法中，否则就会抛出IllegalMonitorStateException异常，这是因为调用这几个方法前必须拿到当前对象的监视器monitor对象，也就是说notify/notifyAll和wait方法依赖于monitor对象，在前面的分析中，我们知道monitor 存在于对象头的Mark Word 中(存储monitor引用指针)，而synchronized关键字可以获取 monitor ，这也就是为什么notify/notifyAll和wait方法必须在synchronized代码块或者synchronized方法调用的原因。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * @author zsc * @date 2021/5/24 * 两个线程轮流打印A和B */// 定义一个资源类class DataShare&#123; int status = 0; public synchronized void printAAA()&#123; while (status != 0)&#123; try &#123; this.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(&quot;打印的是:AAAA&quot;); this.notifyAll(); status = 1; &#125; public synchronized void printBBB()&#123; while (status == 0)&#123; try &#123; this.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(&quot;打印的是:BBBB&quot;); this.notifyAll(); status = 0; &#125;&#125;public class NotifyDemo &#123; public static void main(String[] args) &#123; DataShare dataShare = new DataShare(); new Thread(()-&gt;&#123; for (int i = 0; i &lt;10 ; i++) &#123; dataShare.printAAA(); &#125; &#125;,&quot;A&quot; ).start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt;10 ; i++) &#123; dataShare.printBBB(); &#125; &#125;,&quot;A&quot; ).start(); &#125;&#125; 虚假唤醒上面演示的Demo中，判断要使用while而不能是if 注意，消费者被唤醒后是从wait()方法（被阻塞的地方）后面执行，而不是重新从同步块开头。 使用if判断之后，代码从wait方法后面执行，此时不会再进行一次if判断，假如这个时候在进来线程，就会出现混乱 而使用while（）之后，执行到while方法内，还会拉回while方法判断一次 ConditionCondition是在java 1.5中出现的，它用来替代传统的Object的wait()/notify()实现线程间的协作，它的使用依赖于 Lock，Condition、Lock 和 Thread 三者之间的关系如下图所示。 相比使用Object的wait()/notify()，使用Condition的await()/signal()这种方式能够更加安全和高效地实现线程间协作。 Condition是个接口，基本的方法就是await()和signal()方法。Condition依赖于Lock接口，生成一个Condition的基本代码是lock.newCondition() 。 必须要注意的是，Condition 的 await()/signal() 使用都必须在lock保护之内，也就是说，必须在lock.lock()和lock.unlock之间才可以使用。 Conditon的await()/signal() 与 Object的wait()-notify() 有着天然的对应关系：**Conditon中的await()对应Object的wait()；Condition中的signal()对应Object的notify()**；Condition中的signalAll()对应Object的notifyAll()。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103/** * @author zsc * @date 2021/5/24 * 使用lock中的线程通知 * 三个线程轮流打印AA BB CC * 使用Condition进行精准通知 */// 定义一个资源类class DataShare1&#123; private final Lock lock = new ReentrantLock(); private Condition conditionA = lock.newCondition(); private Condition conditionB = lock.newCondition(); private Condition conditionC = lock.newCondition(); int status = 0; public void printAAA() throws InterruptedException &#123; try &#123; lock.lock(); while (status != 0)&#123; conditionA.await(); &#125; System.out.println(&quot;打印的是:AAAA&quot;); status = 1; conditionB.signal(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void printBBB() throws InterruptedException &#123; try &#123; lock.lock(); while (status != 1)&#123; conditionB.await(); &#125; System.out.println(&quot;打印的是:BBBB&quot;); status = 2; conditionC.signal(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void printCCC() throws InterruptedException &#123; try &#123; lock.lock(); while (status != 2)&#123; conditionC.await(); &#125; System.out.println(&quot;打印的是:CCCC&quot;); status = 0; conditionA.signal(); &#125; finally &#123; lock.unlock(); &#125; &#125;&#125;public class NotifyDemo &#123; public static void main(String[] args) &#123; DataShare1 dataShare = new DataShare1(); new Thread(()-&gt;&#123; for (int i = 0; i &lt;10 ; i++) &#123; try &#123; dataShare.printAAA(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;,&quot;A&quot; ).start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt;10 ; i++) &#123; try &#123; dataShare.printBBB(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;,&quot;B&quot; ).start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt;10 ; i++) &#123; try &#123; dataShare.printCCC(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;,&quot;C&quot; ).start(); &#125;&#125; 为什么这些操作线程的方法要定义在object类中呢？ Condition实现原理： https://blog.csdn.net/javazejian/article/details/75043422 暂时理解不到位，以后再看，先记录一下 Java中，任何对象都可以作为锁(synchronized)，既然wait是放弃对象锁，当然就要把wait定义在这个对象所属的类中。更通用一些，由于所有类都继承于Object，我们完全可以把wait方法定义在Object类中，这样，当我们定义一个新类，并需要以它的一个对象作为锁时，不需要我们再重新定义wait方法的实现，而是直接调用父类的wait(也就是Object的wait)，此处，用到了Java的继承。 有的人会说，既然是线程放弃对象锁，那也可以把wait定义在Thread类里面啊，新定义的线程继承于Thread类，也不需要重新定义wait方法的实现。然而，这样做有一个非常大的问题，一个线程完全可以持有很多锁，你一个线程放弃锁的时候，到底要放弃哪个锁？当然了，这种设计并不是不能实现，只是管理起来更加复杂。","categories":[{"name":"Java","slug":"Java","permalink":"https://zhaoshuchao.top/categories/Java/"},{"name":"JUC","slug":"Java/JUC","permalink":"https://zhaoshuchao.top/categories/Java/JUC/"}],"tags":[{"name":"JUC","slug":"JUC","permalink":"https://zhaoshuchao.top/tags/JUC/"},{"name":"并发编程","slug":"并发编程","permalink":"https://zhaoshuchao.top/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"线程","slug":"线程","permalink":"https://zhaoshuchao.top/tags/%E7%BA%BF%E7%A8%8B/"}]},{"title":"JVM内存结构","slug":"JVM内存结构","date":"2021-04-24T07:50:31.000Z","updated":"2021-06-02T08:32:37.775Z","comments":true,"path":"2021/04/23/JVM内存结构/","link":"","permalink":"https://zhaoshuchao.top/2021/04/23/JVM%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84/","excerpt":"","text":"本文都是基于JDK1.8讨论 类加载器什么是类加载器Java中的所有类，都需要由类加载器装载到JVM中才能运行。类加载器本身也是一个类，而它的工作就是把class文件从硬盘读取到内存中。在写程序的时候，我们几乎不需要关心类的加载，因为这些都是隐式装载的，除非我们有特殊的用法，像是反射，就需要显式的加载所需要的类。 Java类的加载是动态的，它并不会一次性将所有类全部加载后再运行，而是保证程序运行的基础类(像是基类)完全加载到jvm中，至于其他类，则在需要的时候才加载。这当然就是为了节省内存开销。 类加载类型Java的类加载器有三个，对应Java的三种类: Bootstrap Loader(启动类加载器)由C++写的,由JVM启动 启动类加载器，也叫引导加载器。是虚拟机自身的一部分。负责将存放在\\lib目录中的类库加载到虚拟机中。其无法被Java程序直接引用。 负责加载系统类 (指的是内置类 像是String) ExtClassLoader(扩展加载器)是一个Java类,继承自URLClassLoader 扩展类加载器, 负责加载扩展类(如所有javax.*开头的类和存放在%JAVA_HOME%\\lib\\ext目录中的jar和class等 AppClassLoader(应用加载器)Java类,继承自URLClassLoader 系统类加载器, 负责加载用户类路径（ClassPath）上所指定的类库(程序员自定义的类) 自定义加载器这个由程序员自己来写 双亲委派机制三个加载器各自完成自己的工作，但它们是如何协调工作呢？哪一个类该由哪个类加载器完成呢？为了解决这个问题，Java采用了委托模型机制。(双亲委派模式) 工作原理当某个类加载器需要加载某个.class文件时，它首先把这个任务委托给他的上级类加载器，递归这个操作，如果上级的类加载器没有加载，自己才会去加载这个类 双亲委派机制的作用 防止重复加载同一个.class。通过委托去向上面问一问，加载过了，就不用再加载一遍。保证数据安全。 保证核心.class不能被篡改。通过委托方式，不会去篡改核心.class，即使篡改也不会去加载，即使加载也不会是同一个.class对象了。不同的加载器加载同一个.class也不是同一个Class对象。这样保证了Class执行安全 我们可以通过这样的代码来获取类加载器: 12ClassLoader loader = ClassName.class.getClassLoader();ClassLoader ParentLoader = loader.getParent(); 注意一个很重要的问题，就是Java在逻辑上并不存在BootstrapKLoader的实体！因为它是用C++编写的，所以打印其内容将会得到null。 类加载的过程前面是对类加载器的简单介绍，它的原理机制非常简单，就是下面几个步骤: 装载:查找和导入class文件; 连接: 检查:检查载入的class文件数据的正确性; 准备:为类的静态变量分配存储空间; 解析:将符号引用转换成直接引用(这一步是可选的) 初始化:初始化静态变量，静态代码块 使用：代码中根据Class类型new对象或执行其它操作。 卸载：虚拟机通过垃圾回收将类信息及相关的实例数据从虚拟机内存区域中移除。 JVM中的内存结构JVM内存结构可以大致可划分为线程私有区域和共享区域，线程私有区域由虚拟机栈、本地方法栈、程序计数器组成，而共享区域由堆、元数据空间（方法区）组成。 ps：在JDK1.8中，Sun HotSpot虚拟机把虚拟机栈和本地方法栈合并为 Java栈 方法区是jdk1.7之前的叫法，在1.8中叫做元空间 虚拟机栈虚拟机栈是用于描述java方法执行的内存模型。 作用：每个java方法在执行时，会创建一个“栈帧（stack frame）”，栈帧的结构分为“局部变量表、操作数栈、动态链接、方法出口”几个部分。我们常说的“堆内存、栈内存”中的“栈内存”指的便是虚拟机栈，确切地说，指的是虚拟机栈的栈帧中的局部变量表，因为这里存放了一个方法的所有局部变量。 栈中放置以下内容：栈中存放基本类型的原值和引用类型的地址值. 基本类型包括：byte,short,int,long,char,float,double,Boolean,returnAddress ；引用类型包括：类类型，接口类型和数组。 方法调用时，创建栈帧，并压入虚拟机栈；方法执行完毕，栈帧出栈并被销毁，如下图所示： 局部变量表 : 保存函数参数,局部变量(当前函数有效,函数执行结束它销毁) 操作数栈 : 存中间运算结果, 临时存储空间 帧数据区 : 保存访问常量池指针, 异常处理表 虚拟机栈是线程隔离的，即每个线程都有自己独立的虚拟机栈。 若单个线程请求的栈深度大于虚拟机允许的深度，则会抛出StackOverflowError（栈溢出错误）。 举个粟子：如下图 ，假设JVM参数-Xss设置为1m，如果某个方法里面创建一个128kb的数组，那这个方法在同一个线程中只能递归4次，再递归第五次的时候就会报StackOverflowException异常，因为虚拟机栈的大小只有1m，每次递归都需要为方法在虚拟机栈中分配128kb的空间，很显示到第五次的时候就空间不足了。 本地方法栈本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的， 用于本地方法调用, JDK源码中好多使用了Native关键字, 也就是调用底层C语言编写的方法. 与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常 程序计数器也叫PC寄存器 是一个记录着当前线程所执行的字节码的行号指示器。JVM的多线程是通过CPU时间片轮转（即线程轮流切换并分配处理器执行时间）算法来实现的。也就是说，某个线程在执行过程中可能会因为时间片耗尽而被挂起，而另一个线程获取到时间片开始执行。 简单的说程序计数器的主要功能就是记录着当前线程所执行的字节码的行号指示器。 方法区在JDK8叫元数据区 方法区存储了类的元数据信息、静态变量、常量等数据。 方法区的大小决定系统可以保存多少个类。如果系统定义太多的类，导致方法区溢出。虚拟机同样会抛出内存溢出的错误。 堆（heap)平常大家使用new关键字创建的对象都会进入堆中，堆也是GC重点照顾的区域，堆会被划分为：新生代、老年代，而新生代还会被进一步划分为Eden区和Survivor区： 在堆中产生了一个数组或者对象后，还可以在栈中定义一个特殊的变量，这个变量的取值等于数组或者对象在堆内存中的首地址，在栈中的这个特殊的变量就变成了数组或者对象的引用变量，以后就可以在程序中使用栈内存中的引用变量来访问堆中的数组或者对象，引用变量相当于为数组或者对象起的一个别名或者代号 String a = new Stirng(“123”) 此时new出来的对象放在heap中，但是这个引用a要放在栈中 在JDK1.8之前还有一个永久区，在1.8中去掉，永久区的数据现在保存在元空间中 新生代中的Eden区和Survivor（From区和To区）区，是根据 JVM回收算法来的，只是现在大部分都是使用的分代回收算法，所以在介绍堆的时候会直接将新生代归纳为Eden区和Survivor区。 堆的详情信息会在GC中说明 直接内存作用 : 提高一些场景中的性能. 直接内存并不是虚拟机运行时数据区的一部分，也不是Java 虚拟机规范中农定义的内存区域。 在JDK1.4 中新加入了NIO(New Input/Output)类，引入了一种基于通道(Channel)与缓冲区（Buffer）的I/O 方式，它可以使用native 函数库直接分配堆外内存，然后通过一个存储在Java堆中的DirectByteBuffer 对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了在Java堆和Native堆中来回复制数据。 本机直接内存的分配不会受到Java 堆大小的限制，受到本机总内存大小限制 配置虚拟机参数时，不要忽略直接内存 防止出现OutOfMemoryError异常 直接内存（堆外内存）与堆内存比较 直接内存申请空间耗费更高的性能，当频繁申请到一定量时尤为明显 直接内存IO读写的性能要优于普通的堆内存，在多次读写操作的情况下差异明显 JVM内存模型小结： JVM内存模型划分为线程私有区域和共享区域 虚拟机栈/本地方法栈负责存放线程执行方法栈帧 程序计数器用于记录线程执行指令的位置 元空间区存储类的元数据信息、静态变量、常量等数据 堆（heap)使用new关键字创建的对象都会进入堆中，堆被划分为新生代和老年代 内存模型以及分区，需要详细到每个区放什么？ 方法区：主要是存储类信息，常量池（static 常量和 static 变量），编译后的代码（字节码）等数 堆：初始化的对象，成员变量 （那种非 static 的变量），所有的对象实例和数组都要在堆上分配 栈：引用放在栈里面；栈的结构是栈帧组成的，调用一个方法就压入一帧，帧上面存储局部变量表，操作数栈，方法出口等信息，局部变量表存放的是 8 大基础类型加上一个应用类型，所以还是一个指向地址的指针 程序计数器：记录当前线程执行的行号 直接内存：NIO的操作 创建一个对象在内存中的变化java在new一个对象的时候，会先查看对象所属的类有没有被加载到内存，如果没有的话，就会先通过类的全限定名来加载。加载并初始化类完成后，再进行对象的创建工作。 我们先假设是第一次使用该类，这样的话new一个对象就可以分为两个过程：加载并初始化类和创建对象。 一、类加载过程（第一次使用该类）java是使用双亲委派模型来进行类的加载的，所以在描述类加载过程前，我们先看一下它的工作过程： 双亲委托模型的工作过程是：如果一个类加载器（ClassLoader）收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委托给父类加载器去完成，每一个层次的类加载器都是如此，因此所有的加载请求最终都应该传送到顶层的启动类加载器中，只有当父类加载器反馈自己无法完成这个加载请求（它的搜索范围中没有找到所需要加载的类）时，子加载器才会尝试自己去加载。 使用双亲委托机制的好处是：能够有效确保一个类的全局唯一性，当程序中出现多个限定名相同的类时，类加载器在执行加载时，始终只会加载其中的某一个类。 1、加载由类加载器负责根据一个类的全限定名来读取此类的二进制字节流到JVM内部，并存储在运行时内存区的方法区，然后将其转换为一个与目标类型对应的java.lang.Class对象实例 2、验证 格式验证：验证是否符合class文件规范 语义验证：检查一个被标记为final的类型是否包含子类；检查一个类中的final方法是否被子类进行重写；确保父类和子类之间没有不兼容的一些方法声明（比如方法签名相同，但方法的返回值不同） 操作验证：在操作数栈中的数据必须进行正确的操作，对常量池中的各种符号引用执行验证（通常在解析阶段执行，检查是否可以通过符号引用中描述的全限定名定位到指定类型上，以及类成员信息的访问修饰符是否允许访问等） 3、准备为类中的所有静态变量分配内存空间，并为其设置一个初始值（由于还没有产生对象，实例变量不在此操作范围内） 被final修饰的static变量（常量），会直接赋值； 4、解析将常量池中的符号引用转为直接引用（得到类或者字段、方法在内存中的指针或者偏移量，以便直接调用该方法），这个可以在初始化之后再执行。解析需要静态绑定的内容。 // 所有不会被重写的方法和域都会被静态绑定 以上2、3、4三个阶段又合称为链接阶段，链接阶段要做的是将加载到JVM中的二进制字节流的类数据信息合并到JVM的运行时状态中。 5、初始化（先父后子） 4.1 为静态变量赋值 4.2 执行static代码块 注意：static代码块只有jvm能够调用 如果是多线程需要同时初始化一个类，仅仅只能允许其中一个线程对其执行初始化操作，其余线程必须等待，只有在活动线程执行完对类的初始化操作之后，才会通知正在等待的其他线程。 因为子类存在对父类的依赖，所以类的加载顺序是先加载父类后加载子类，初始化也一样。不过，父类初始化时，子类静态变量的值也有有的，是默认值。 最终，方法区会存储当前类类信息，包括类的静态变量、类初始化代码（定义静态变量时的赋值语句 和 静态初始化代码块）、实例变量定义、实例初始化代码（定义实例变量时的赋值语句实例代码块和构造方法）和实例方法，还有父类的类信息引用。 二、创建对象 堆内初始化，栈内引用 1、在堆区分配对象需要的内存分配的内存包括本类和父类的所有实例变量，但不包括任何静态变量 2、对所有实例变量赋默认值将方法区内对实例变量的定义拷贝一份到堆区，然后赋默认值 3、执行实例初始化代码初始化顺序是先初始化父类再初始化子类，初始化时先执行实例代码块然后是构造方法 4、如果有类似于Child c = new Child()形式的c引用的话，在栈区定义Child类型引用变量c，然后将堆区对象的地址赋值给它 需要注意的是，每个子类对象持有父类对象的引用，可在内部通过super关键字来调用父类对象，但在外部不可访问 补充： 通过实例引用调用实例方法的时候，先从方法区中对象的实际类型信息找，找不到的话再去父类类型信息中找。 如果继承的层次比较深，要调用的方法位于比较上层的父类，则调用的效率是比较低的，因为每次调用都要经过很多次查找。这时候大多系统会采用一种称为虚方法表的方法来优化调用的效率。 所谓虚方法表，就是在类加载的时候，为每个类创建一个表，这个表包括该类的对象所有动态绑定的方法及其地址，包括父类的方法，但一个方法只有一条记录，子类重写了父类方法后只会保留子类的。当通过对象动态绑定方法的时候，只需要查找这个表就可以了，而不需要挨个查找每个父类。","categories":[{"name":"Java","slug":"Java","permalink":"https://zhaoshuchao.top/categories/Java/"},{"name":"JVM","slug":"Java/JVM","permalink":"https://zhaoshuchao.top/categories/Java/JVM/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://zhaoshuchao.top/tags/JVM/"}]},{"title":"Synchronized","slug":"Synchronized","date":"2021-04-16T05:47:39.000Z","updated":"2021-06-02T08:29:55.001Z","comments":true,"path":"2021/04/15/Synchronized/","link":"","permalink":"https://zhaoshuchao.top/2021/04/15/Synchronized/","excerpt":"","text":"前言：造成线程安全问题的主要诱因有两点: 一是存在共享数据(也称临界资源)，二是存在多条线程共同操作共享数据。因此为了解决这个问题，我们可能需要这样一个方案，当存在多个线程操作共享数据时，需要保证同一时刻有且只有一个线程在操作共享数据，其他线程必须等到该线程处理完数据后再进行，这种方式有个高尚的名称叫互斥锁，即能达到互斥访问目的的锁，也就是说当一个共享数据被当前正在访问的线程加上互斥锁后，在同一个时刻，其他线程只能处于等待的状态，直到当前线程处理完毕释放该锁。在 Java 中，关键字 synchronized可以保证在同一个时刻，只有一个线程可以执行某个方法或者某个代码块(主要是对方法或者代码块中存在共享数据的操作)，同时我们还应该注意到synchronized另外一个重要的作用，synchronized可保证一个线程的变化(主要是共享数据的变化)被其他线程所看到（保证可见性，完全可以替代Volatile功能），这点确实也是很重要的。 synchronized的三种应用方式synchronized关键字最主要有以下3种应用方式 修饰实例方法，作用于当前实例加锁，进入同步代码前要获得当前实例的锁 修饰静态方法，作用于当前类对象加锁，进入同步代码前要获得当前类对象的锁 修饰代码块，指定加锁对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。 1. synchronized作用于实例方法所谓的实例对象锁就是用synchronized修饰实例对象中的实例方法，注意是实例方法不包括静态方法，如下 123456789101112131415161718192021222324252627282930313233343536373839404142/** * @author zsc * @date 2021/5/18 * synchronized作用于实例方法 */public class AccountingSync implements Runnable&#123; //共享资源(临界资源) static int i=0; /** * synchronized 修饰实例方法 */ public synchronized void increase()&#123; i++; &#125; /** * 这里没有synchronized 结果很有可能就小于2000000 i++操作并不具备原子性 */ @Override public void run() &#123; for(int j=0;j&lt;1000000;j++)&#123; increase(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; /** * 锁的是当前实例，也就是AccountingSync的一个实例instance * instance实例的下的所有synchronized修饰的实例方法 * 若此时该实例还有一个实例方式 synchronized test()，线程访问test,也要等待increase方法释放掉锁才可以 */ AccountingSync instance=new AccountingSync(); Thread t1=new Thread(instance); Thread t2=new Thread(instance); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println(i); &#125;&#125; 上述代码中，我们开启两个线程操作同一个共享资源即变量i，由于i++;操作并不具备原子性，该操作是先读取值，然后写回一个新值，相当于原来的值加上1，分两步完成 如果第二个线程在第一个线程读取旧值和写回新值期间读取i的域值，那么第二个线程就会与第一个线程一起看到同一个值，并执行相同值的加1操作，这也就造成了线程安全失败，因此对于increase方法必须使用synchronized修饰，以便保证线程安全。 此时我们应该注意到synchronized修饰的是实例方法increase，在这样的情况下，当前线程的锁便是实例对象instance，注意Java中的线程同步锁可以是任意对象。从代码执行结果来看确实是正确的，倘若我们没有使用synchronized关键字，其最终输出结果就很可能小于2000000，这便是synchronized关键字的作用。 这里我们还需要意识到，当一个线程正在访问一个对象的 synchronized 实例方法，那么其他线程不能访问该对象的其他 synchronized 方法，毕竟一个对象只有一把锁。 当一个线程获取了该对象的锁之后，其他线程无法获取该对象的锁，所以无法访问该对象的其他synchronized实例方法，但是其他线程还是可以访问该实例对象的其他非synchronized方法 当然如果是一个线程 A 需要访问实例对象 obj1 的 synchronized 方法 f1(当前对象锁是obj1)，另一个线程 B 需要访问实例对象 obj2 的 synchronized 方法 f2(当前对象锁是obj2)，这样是允许的，因为两个实例对象锁并不同相同，此时如果两个线程操作数据并非共享的，线程安全是有保障的，遗憾的是如果两个线程操作的是共享数据，那么线程安全就有可能无法保证了，如下代码将演示出该现象 123456789101112131415161718192021222324252627282930313233343536373839/** * @author zsc * @date 2021/5/18 * 创建两个实例，t1和t2都会进入各自的对象锁 */public class AccountingSyncBad implements Runnable&#123; static int i=0; public synchronized void increase()&#123; i++; &#125; @Override public void run() &#123; for(int j=0;j&lt;1000000;j++)&#123; increase(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; //new新实例 Thread t1=new Thread(new AccountingSyncBad()); //new新实例 Thread t2=new Thread(new AccountingSyncBad()); t1.start(); t2.start(); //join含义:当前线程A等待thread线程终止之后才能从thread.join()返回 t1.join(); t2.join(); System.out.println(i); &#125; /** * 上述代码与前面不同的是我们同时创建了两个新实例AccountingSyncBad， * 然后启动两个不同的线程对共享变量i进行操作，但很遗憾操作结果是1452317而不是期望结果2000000， * 因为上述代码犯了严重的错误，虽然我们使用synchronized修饰了increase方法，但却new了两个不同的实例对象， * 这也就意味着存在着两个不同的实例对象锁，因此t1和t2都会进入各自的对象锁，也就是说t1和t2线程使用的是不同的锁， * 因此线程安全是无法保证的。解决这种困境的的方式是将synchronized作用于静态的increase方法，这样的话，对象锁就当前类对象， * 由于无论创建多少个实例对象，但对于的类对象拥有只有一个，所有在这样的情况下对象锁就是唯一的。 * ———————————————— */&#125; 2. synchronized作用于静态方法当synchronized作用于静态方法时，其锁就是当前类的class对象锁。由于静态成员不专属于任何一个实例对象，是类成员，因此通过class对象锁可以控制静态 成员的并发操作。需要注意的是如果一个线程A调用一个实例对象的非static synchronized方法，而线程B需要调用这个实例对象所属类的静态 synchronized方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的class对象，而访问非静态 synchronized 方法占用的锁是当前实例对象锁，看如下代码 123456789101112131415161718192021222324252627282930313233343536373839404142/** * @author zsc * @date 2021/5/18 * synchronized作用于静态方法 */public class AccountingSyncClass implements Runnable&#123; static int i=0; /** * 作用于静态方法,锁是当前class对象,也就是 * AccountingSyncClass类对应的class对象 */ public static synchronized void increase()&#123; i++; &#125; /** * 非静态,访问时锁不一样不会发生互斥 */ public synchronized void increase4Obj()&#123; i++; &#125; @Override public void run() &#123; for(int j=0;j&lt;1000000;j++)&#123; increase(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; //new新实例 Thread t1=new Thread(new AccountingSyncClass()); //new新实例 Thread t2=new Thread(new AccountingSyncClass()); //启动线程 t1.start();t2.start(); t1.join();t2.join(); System.out.println(i); &#125;&#125; 由于synchronized关键字修饰的是静态increase方法，与修饰实例方法不同的是，其锁对象是当前类的class对象。注意代码中的increase4Obj方法是实例方法，其对象锁是当前实例对象，如果别的线程调用该方法，将不会产生互斥现象，毕竟锁对象不同，但我们应该意识到这种情况下可能会发现线程安全问题(操作了共享静态变量i)。 3. synchronized同步代码块除了使用关键字修饰实例方法和静态方法外，还可以使用同步代码块，在某些情况下，我们编写的方法体可能比较大，同时存在一些比较耗时的操作，而需要同步的代码又只有一小部分，如果直接对整个方法进行同步操作，可能会得不偿失，此时我们可以使用同步代码块的方式对需要同步的代码进行包裹，这样就无需对整个方法进行同步操作了，同步代码块的使用示例如下： 123456789101112131415161718192021222324252627282930313233/** * @author zsc * @date 2021/5/18 * synchronized同步代码块 */public class AccountingSyncBody implements Runnable&#123; static int i=0; @Override public void run() &#123; // 用类的class对象表示是class对象锁，锁class synchronized(AccountingSyncBody.class)&#123; for(int j=0;j&lt;1000000;j++)&#123; i++; &#125; &#125; // 这里用this代表当前实例对象锁// synchronized(this)&#123;// for(int j=0;j&lt;1000000;j++)&#123;// i++;// &#125;// &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; AccountingSyncBody instance=new AccountingSyncBody(); AccountingSyncBody instance2=new AccountingSyncBody(); Thread t1=new Thread(instance); Thread t2=new Thread(instance2); t1.start();t2.start(); t1.join();t2.join(); System.out.println(i); &#125;&#125; 从代码看出，将synchronized作用于一个给定的实例对象instance，即当前实例对象就是锁对象，每次当线程进入synchronized包裹的代码块时就会要求当前线程持有instance实例对象锁，如果当前有其他线程正持有该对象锁，那么新到的线程就必须等待，这样也就保证了每次只有一个线程执行i++;操作。当然除了instance作为对象外，我们还可以使用this对象(代表当前实例)或者当前类的class对象作为锁 总结： synchronized有三种实现方式，分别为锁实例方法（非static方法），锁静态方法，锁代码块 锁实例方法的时候，JVM会给该实例方法的实例对象加锁，有且只有一个线程可以进入该实例方法，其他线程必须要等待进入的线程执行完代码释放锁才能进入，其他线程也不能进入该实例对象的其他synchronized修饰的实例方法。因为锁是加给该实例对象的。但是同一个线程允许可重入操作。加锁的方式为在对象头通过monitor对象管理实现的，下面会有关于可重入操作和加锁的具体解释 锁静态方法的时候，JVM会给该静态方法的类对象加锁，也就是无论该Class创建多少实例，都会互斥。要注意的是该类的synchronized修饰的静态方法被锁住的时候，其他线程是可以访问该类synchronized修饰的非static方法的。因为static方法加锁加Class对象上，而非static方法加锁在实例上，两者互不干扰 锁代码块的时候，可以指定加锁的类型，synchronized(XXX.class)表示给该代码块加上一个类对象锁。synchronized(this)表示给该代码块加上一个实例对象锁。 Synchronized底层语义原理JVM中的同步(Synchronization)基于Monitor对象实现， 无论是显式同步(有明确的 monitorenter 和 monitorexit 指令,即同步代码块)还是隐式同步都是如此。 在 Java 语言中，同步用的最多的地方可能是被 synchronized 修饰的同步方法。同步方法 并不是由 monitorenter 和 monitorexit 指令来实现同步的，而是由方法调用指令读取运行时常量池中方法的 ACC_SYNCHRONIZED 标志来隐式实现的，关于这点，稍后详细分析。下面先来了解一个概念Java对象头，这对深入理解synchronized实现原理非常关键。 理解Java对象头与Monitor在 JVM 中，对象在内存中分为三块区域： 对象头 （这个是重点）它实现synchronized的锁对象的基础，一般而言，synchronized使用的锁对象是存储在Java对象头里的，jvm中采用2个字来存储对象头(如果对象是数组则会分配3个字，多出来的1个字记录的是数组长度)，其主要结构是由Mark Word 和 Class Metadata Address （也可以说Klass Point，意思一样，都为类型指针）组成 这个字的本意的word, C++中的概念。JVM是C++开发的 Mark Word（标记字段）：默认存储对象的HashCode，分代年龄和锁标志位信息。它会根据对象的状态复用自己的存储空间，也就是说在运行期间Mark Word里存储的数据会随着锁标志位的变化而变化。 Klass Point（类型指针）：对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。 实例数据 这部分主要是存放类的数据信息，父类的信息。如果是数组的实例部分还包括数组的长度，这部分内存按4字节对齐。 填充数据 由于虚拟机要求对象起始地址必须是8字节的整数倍，填充数据不是必须存在的，仅仅是为了字节对齐。 ps：不知道大家有没有被问过一个空对象占多少个字节？就是8个字节，是因为对齐填充的关系哈，不到8个字节对其填充会帮我们自动补齐。 重量级锁也就是通常说synchronized的对象锁，其中指针指向的是monitor对象（也称为管程或监视器锁）的起始地址。每个对象都存在着一个 monitor 与之关联，对象与其 monitor 之间的关系有存在多种实现方式，如monitor可以与对象一起创建销毁或当线程试图获取对象锁时自动生成，但当一个 monitor 被某个线程持有后，它便处于锁定状态。在Java虚拟机(HotSpot)中，monitor是由ObjectMonitor实现的，其主要数据结构如下（位于HotSpot虚拟机源码ObjectMonitor.hpp文件，C++实现的） 123456789101112131415161718ObjectMonitor() &#123; _header = NULL; _count = 0; //记录个数 _waiters = 0, _recursions = 0; _object = NULL; _owner = NULL; _WaitSet = NULL; //处于wait状态的线程，会被加入到_WaitSet _WaitSetLock = 0 ; _Responsible = NULL ; _succ = NULL ; _cxq = NULL ; FreeNext = NULL ; _EntryList = NULL ; //处于等待锁block状态的线程，会被加入到该列表 _SpinFreq = 0 ; _SpinClock = 0 ; OwnerIsThread = 0 ; &#125; ObjectMonitor中有两个队列，**_WaitSet （待唤醒的线程）和 _EntryList（也就是等待的线程）**，用来保存ObjectWaiter对象列表( 每个等待锁的线程都会被封装成ObjectWaiter对象)，_owner指向持有ObjectMonitor对象的线程， 当多个线程同时访问一段同步代码时，首先会进入 _EntryList 集合， 当线程获取到对象的monitor 后进入 _Owner 区域并把monitor中的owner变量设置为当前线程同时monitor中的计数器count加1 若线程调用 wait() 方法，将释放当前持有的monitor，owner变量恢复为null，count自减1， 同时该线程进入 WaitSet集合中等待被唤醒。 若当前线程执行完毕也将释放monitor(锁)并复位变量的值，以便其他线程进入获取monitor(锁) 由此看来，monitor对象存在于每个Java对象的对象头中(存储的指针的指向)，synchronized锁便是通过这种方式获取锁的，也是为什么Java中任意对象可以作为锁的原因，同时也是notify/notifyAll/wait等方法存在于顶级对象Object中的原因 synchronized代码块底层原理现在我们重新定义一个synchronized修饰的同步代码块，在代码块中操作共享变量i，如下 123456789public class SyncCodeBlock &#123; public int i; public void syncTask()&#123; //同步代码库 synchronized (this)&#123; i++; &#125; &#125;&#125; 编译上述代码并使用javap反编译后得到字节码如下(这里我们省略一部分没有必要的信息)： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849 Last modified 2017-6-2; size 426 bytes MD5 checksum c80bc322c87b312de760942820b4fed5 Compiled from &quot;SyncCodeBlock.java&quot;public class com.zejian.concurrencys.SyncCodeBlock minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_SUPERConstant pool: //........省略常量池中数据 //构造函数 public com.zejian.concurrencys.SyncCodeBlock(); descriptor: ()V flags: ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return LineNumberTable: line 7: 0 //===========主要看看syncTask方法实现================ public void syncTask(); descriptor: ()V flags: ACC_PUBLIC Code: stack=3, locals=3, args_size=1 0: aload_0 1: dup 2: astore_1 3: monitorenter //注意此处，进入同步方法 4: aload_0 5: dup 6: getfield #2 // Field i:I 9: iconst_1 10: iadd 11: putfield #2 // Field i:I 14: aload_1 15: monitorexit //注意此处，退出同步方法 16: goto 24 19: astore_2 20: aload_1 21: monitorexit //注意此处，退出同步方法 22: aload_2 23: athrow 24: return Exception table: //略其他字节码.......&#125;SourceFile: &quot;SyncCodeBlock.java&quot; 主要看的字节码代码： 1234563: monitorenter //进入同步方法//..........省略其他 15: monitorexit //退出同步方法16: goto 24//省略其他.......21: monitorexit //退出同步方法 显式同步的流程 从字节码中可知同步语句块的实现使用的是monitorenter 和 monitorexit 指令，其中monitorenter指令指向同步代码块的开始位置，monitorexit指令则指明同步代码块的结束位置 当执行monitorenter指令时，当前线程将试图获取 objectref(即对象锁) 所对应的 monitor 的持有权，当 objectref 的 monitor 的进入计数器为 0，那线程可以成功取得 monitor，并将计数器值设置为 1，取锁成功。 如果当前线程已经拥有 objectref 的 monitor 的持有权，那它可以重入这个 monitor (关于重入性稍后会分析)，重入时计数器的值也会加 1。 倘若其他线程已经拥有 objectref 的 monitor 的所有权，那当前线程将被阻塞，直到正在执行线程执行完毕，即monitorexit指令被执行，执行线程将释放 monitor(锁)并设置计数器值为0 ，其他线程将有机会持有 monitor 。 值得注意的是编译器将会确保无论方法通过何种方式完成，方法中调用过的每条 monitorenter 指令都有执行其对应 monitorexit 指令，而无论这个方法是正常结束还是异常结束。 为了保证在方法异常完成时 monitorenter 和 monitorexit 指令依然可以正确配对执行，编译器会自动产生一个异常处理器，这个异常处理器声明可处理所有的异常，它的目的就是用来执行 monitorexit 指令。从字节码中也可以看出多了一个monitorexit指令，它就是异常结束时被执行的释放monitor 的指令。 synchronized方法底层原理1234567public class SyncMethod &#123; public int i; //共享变量i public synchronized void syncTask()&#123; i++; &#125;&#125; 反编译之后的字节码： 1234567891011121314151617181920212223242526272829 Last modified 2017-6-2; size 308 bytes MD5 checksum f34075a8c059ea65e4cc2fa610e0cd94 Compiled from &quot;SyncMethod.java&quot;public class com.zejian.concurrencys.SyncMethod minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_SUPERConstant pool; //省略没必要的字节码 //==================syncTask方法====================== public synchronized void syncTask(); descriptor: ()V //方法标识ACC_PUBLIC代表public修饰，ACC_SYNCHRONIZED指明该方法为同步方法 flags: ACC_PUBLIC, ACC_SYNCHRONIZED Code: stack=3, locals=1, args_size=1 0: aload_0 1: dup 2: getfield #2 // Field i:I 5: iconst_1 6: iadd 7: putfield #2 // Field i:I 10: return LineNumberTable: line 12: 0 line 13: 10&#125;SourceFile: &quot;SyncMethod.java&quot; 隐式同步 方法级的同步是隐式，即无需通过字节码指令来控制的，它实现在方法调用和返回操作之中。JVM可以从方法常量池中的方法表结构(method_info Structure) 中的 ACC_SYNCHRONIZED 访问标志区分一个方法是否同步方法。这便是synchronized锁在同步代码块和同步方法上实现的基本原理 当方法调用时，调用指令将会 检查方法的 ACC_SYNCHRONIZED 访问标志是否被设置，如果设置了，执行线程将先持有monitor（虚拟机规范中用的是管程一词）， 然后再执行方法，最后再方法完成(无论是正常完成还是非正常完成)时释放monitor。 在方法执行期间，执行线程持有了monitor，其他任何线程都无法再获得同一个monitor。 如果一个同步方法执行期间抛 出了异常，并且在方法内部无法处理此异常，那这个同步方法所持有的monitor将在异常抛到同步方法之外时自动释放 ACC_SYNCHRONIZED会去隐式调用刚才的两个指令：monitorenter和monitorexit。 所以归根究底，还是monitor对象的争夺。 同时我们还必须注意到的是在Java早期版本中，synchronized属于重量级锁，效率低下，因为监视器锁（monitor）是依赖于底层的操作系统的Mutex Lock来实现的，而操作系统实现线程之间的切换时需要从用户态转换到核心态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，这也是为什么早期的synchronized效率低的原因。庆幸的是在Java 6之后Java官方对从JVM层面对synchronized较大优化，所以现在的synchronized锁效率也优化得很不错了，Java 6之后，为了减少获得锁和释放锁所带来的性能消耗，引入了轻量级锁和偏向锁 Java虚拟机对synchronized的优化锁的状态总共有四种，无锁状态、偏向锁、轻量级锁和重量级锁。随着锁的竞争，锁可以从偏向锁升级到轻量级锁，再升级的重量级锁，但是锁的升级是单向的，也就是说只能从低到高升级，不会出现锁的降级，关于重量级锁，前面我们已详细分析过，下面我们将介绍偏向锁和轻量级锁以及JVM的其他优化手段 偏向锁偏向锁是Java 6之后加入的新锁，它是一种针对加锁操作的优化手段，经过研究发现，在大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，因此为了减少同一线程获取锁(会涉及到一些CAS操作,耗时)的代价而引入偏向锁。偏向锁的核心思想是，如果一个线程获得了锁，那么锁就进入偏向模式，此时Mark Word 的结构也变为偏向锁结构，当这个线程再次请求锁时，无需再做任何同步操作，即获取锁的过程，这样就省去了大量有关锁申请的操作，从而也就提供程序的性能。所以，对于没有锁竞争的场合，偏向锁有很好的优化效果，毕竟极有可能连续多次是同一个线程申请相同的锁。但是对于锁竞争比较激烈的场合，偏向锁就失效了，因为这样场合极有可能每次申请锁的线程都是不相同的，因此这种场合下不应该使用偏向锁，否则会得不偿失，需要注意的是，偏向锁失败后，并不会立即膨胀为重量级锁，而是先升级为轻量级锁。下面我们接着了解轻量级锁。 轻量级锁倘若偏向锁失败，虚拟机并不会立即升级为重量级锁，它还会尝试使用一种称为轻量级锁的优化手段(1.6之后加入的)，此时Mark Word 的结构也变为轻量级锁的结构。轻量级锁能够提升程序性能的依据是“对绝大部分的锁，在整个同步周期内都不存在竞争”，注意这是经验数据。需要了解的是，轻量级锁所适应的场景是线程交替执行同步块的场合，如果存在同一时间访问同一锁的场合，就会导致轻量级锁膨胀为重量级锁。 自旋锁轻量级锁失败后，虚拟机为了避免线程真实地在操作系统层面挂起，还会进行一项称为自旋锁的优化手段。这是基于在大多数情况下，线程持有锁的时间都不会太长，如果直接挂起操作系统层面的线程可能会得不偿失，毕竟操作系统实现线程之间的切换时需要从用户态转换到核心态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，因此自旋锁会假设在不久将来，当前的线程可以获得锁，因此虚拟机会让当前想要获取锁的线程做几个空循环(这也是称为自旋的原因)，一般不会太久，可能是50个循环或100循环，在经过若干次循环后，如果得到锁，就顺利进入临界区。如果还不能获得锁，那就会将线程在操作系统层面挂起，这就是自旋锁的优化方式，这种方式确实也是可以提升效率的。最后没办法也就只能升级为重量级锁了。 关于锁升级的详细内容：https://www.jianshu.com/p/36eedeb3f912 关于synchronized 可能需要了解的关键点synchronized的可重入性从互斥锁的设计上来说，当一个线程试图操作一个由其他线程持有的对象锁的临界资源时，将会处于阻塞状态，但当一个线程再次请求自己持有对象锁的临界资源时，这种情况属于重入锁，请求将会成功，在java中synchronized是基于原子性的内部锁机制，是可重入的，因此在一个线程调用synchronized方法的同时在其方法体内部调用该对象另一个synchronized方法，也就是说一个线程得到一个对象锁后再次请求该对象锁，是允许的，这就是synchronized的可重入性。注意由于synchronized是基于monitor实现的，因此每次重入monitor中的计数器仍会加1。 不可中断性不可中断就是指，一个线程获取锁之后，另外一个线程处于阻塞或者等待状态，前一个不释放，后一个也一直会阻塞或者等待，不可以被中断。 与sleep方法不同的是wait方法调用完成后，线程将被暂停，但wait方法将会释放当前持有的监视器锁(monitor)，直到有线程调用notify/notifyAll方法后方能继续执行，而sleep方法只让线程休眠并不释放锁。同时notify/notifyAll方法调用后，并不会马上释放监视器锁，而是在相应的synchronized(){}/synchronized方法执行结束后才自动释放锁。 synchronized特性： 有序性 : as-if-serial和happens-before 可见性：内存强制刷新 原子性：单一线程持有 可重入性：计数器 有序性参考：https://blog.csdn.net/byhook/article/details/87971081 用synchronized还是Lock呢？我们先看看他们的区别： synchronized是关键字，是JVM层面的底层啥都帮我们做了，而Lock是一个接口，是JDK层面的有丰富的API。 synchronized会自动释放锁，而Lock必须手动释放锁。 synchronized是不可中断的，Lock可以中断也可以不中断。 通过Lock可以知道线程有没有拿到锁，而synchronized不能。 synchronized能锁住方法和代码块，而Lock只能锁住代码块。 Lock可以使用读锁提高多线程读效率。 synchronized是非公平锁，ReentrantLock可以控制是否是公平锁。","categories":[{"name":"Java","slug":"Java","permalink":"https://zhaoshuchao.top/categories/Java/"},{"name":"JUC","slug":"Java/JUC","permalink":"https://zhaoshuchao.top/categories/Java/JUC/"},{"name":"同步锁","slug":"Java/JUC/同步锁","permalink":"https://zhaoshuchao.top/categories/Java/JUC/%E5%90%8C%E6%AD%A5%E9%94%81/"}],"tags":[{"name":"JUC","slug":"JUC","permalink":"https://zhaoshuchao.top/tags/JUC/"},{"name":"并发编程","slug":"并发编程","permalink":"https://zhaoshuchao.top/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"锁","slug":"锁","permalink":"https://zhaoshuchao.top/tags/%E9%94%81/"},{"name":"Java","slug":"Java","permalink":"https://zhaoshuchao.top/tags/Java/"}]},{"title":"线程池","slug":"线程池","date":"2021-03-02T07:34:23.000Z","updated":"2021-06-02T08:32:17.661Z","comments":true,"path":"2021/03/01/线程池/","link":"","permalink":"https://zhaoshuchao.top/2021/03/01/%E7%BA%BF%E7%A8%8B%E6%B1%A0/","excerpt":"","text":"线程池的优势总体来说，线程池有如下的优势： （1）降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 （2）提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 （3）提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。 线程池的使用四种构造方法线程池的真正实现类是 ThreadPoolExecutor，其构造方法有如下4种： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), defaultHandler);&#125; public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, threadFactory, defaultHandler);&#125; public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, RejectedExecutionHandler handler) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), handler);&#125; public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; 参数：corePoolSize（必需）：核心线程数。默认情况下，核心线程会一直存活，但是当将 allowCoreThreadTimeout 设置为 true 时，核心线程也会超时回收。maximumPoolSize（必需）：线程池所能容纳的最大线程数。当活跃线程数达到该数值后，后续的新任务将会阻塞。keepAliveTime（必需）：线程闲置超时时长。如果超过该时长，非核心线程就会被回收。如果将 allowCoreThreadTimeout 设置为 true 时，核心线程也会超时回收。unit（必需）：指定 keepAliveTime 参数的时间单位。常用的有：TimeUnit.MILLISECONDS（毫秒）、TimeUnit.SECONDS（秒）、TimeUnit.MINUTES（分）。workQueue（必需）：任务队列。通过线程池的 execute() 方法提交的 Runnable 对象将存储在该参数中。其采用阻塞队列实现。threadFactory（可选）：线程工厂。用于指定为线程池创建新线程的方式。handler（可选）：拒绝策略。当达到最大线程数时需要执行的饱和策略。线程池的使用流程如下： 线程池的工作原理下面来描述一下线程池工作的原理，同时对上面的参数有一个更深的了解。其工作原理流程图如下： 通过上图，相信大家已经对所有参数有个了解了。下面再对任务队列、线程工厂和拒绝策略做更多的说明。 线程池的参数任务队列（workQueue）任务队列是基于阻塞队列实现的，即采用生产者消费者模式，在 Java 中需要实现 BlockingQueue 接口。但 Java 已经为我们提供了 7 种阻塞队列的实现： ArrayBlockingQueue：一个由数组结构组成的有界阻塞队列（数组结构可配合指针实现一个环形队列）。 LinkedBlockingQueue： 一个由链表结构组成的有界阻塞队列，在未指明容量时，容量默认为 Integer.MAX_VALUE。 PriorityBlockingQueue： 一个支持优先级排序的无界阻塞队列，对元素没有要求，可以实现 Comparable 接口也可以提供 Comparator 来对队列中的元素进行比较。跟时间没有任何关系，仅仅是按照优先级取任务。 DelayQueue：类似于PriorityBlockingQueue，是二叉堆实现的无界优先级阻塞队列。要求元素都实现 Delayed 接口，通过执行时延从队列中提取任务，时间没到任务取不出来。 SynchronousQueue： 一个不存储元素的阻塞队列，消费者线程调用 take() 方法的时候就会发生阻塞，直到有一个生产者线程生产了一个元素，消费者线程就可以拿到这个元素并返回；生产者线程调用 put() 方法的时候也会发生阻塞，直到有一个消费者线程消费了一个元素，生产者才会返回。 LinkedBlockingDeque： 使用双向队列实现的有界双端阻塞队列。双端意味着可以像普通队列一样 FIFO（先进先出），也可以像栈一样 FILO（先进后出）。 LinkedTransferQueue： 它是ConcurrentLinkedQueue、LinkedBlockingQueue 和 SynchronousQueue 的结合体，但是把它用在 ThreadPoolExecutor 中，和 LinkedBlockingQueue 行为一致，但是是无界的阻塞队列。注意有界队列和无界队列的区别：如果使用有界队列，当队列饱和时并超过最大线程数时就会执行拒绝策略；而如果使用无界队列，因为任务队列永远都可以添加任务，所以设置 maximumPoolSize 没有任何意义。 线程工厂（threadFactory）线程工厂指定创建线程的方式，需要实现 ThreadFactory 接口，并实现 newThread(Runnable r) 方法。该参数可以不用指定，Executors 框架已经为我们实现了一个默认的线程工厂： 1234567891011121314151617181920212223242526272829/** * The default thread factory. */private static class DefaultThreadFactory implements ThreadFactory &#123; private static final AtomicInteger poolNumber = new AtomicInteger(1); private final ThreadGroup group; private final AtomicInteger threadNumber = new AtomicInteger(1); private final String namePrefix; DefaultThreadFactory() &#123; SecurityManager s = System.getSecurityManager(); group = (s != null) ? s.getThreadGroup() : Thread.currentThread().getThreadGroup(); namePrefix = &quot;pool-&quot; + poolNumber.getAndIncrement() + &quot;-thread-&quot;; &#125; public Thread newThread(Runnable r) &#123; Thread t = new Thread(group, r, namePrefix + threadNumber.getAndIncrement(), 0); if (t.isDaemon()) t.setDaemon(false); if (t.getPriority() != Thread.NORM_PRIORITY) t.setPriority(Thread.NORM_PRIORITY); return t; &#125;&#125; 拒绝策略（handler）当线程池的线程数达到最大线程数时，需要执行拒绝策略。拒绝策略需要实现 RejectedExecutionHandler 接口，并实现 rejectedExecution(Runnable r, ThreadPoolExecutor executor) 方法。不过 Executors 框架已经为我们实现了 4 种拒绝策略： AbortPolicy（默认）：丢弃任务并抛出 RejectedExecutionException 异常。 CallerRunsPolicy：由调用线程处理该任务。 DiscardPolicy：丢弃任务，但是不抛出异常。可以配合这种模式进行自定义的处理方式。 DiscardOldestPolicy：丢弃队列最早的未处理任务，然后重新尝试执行任务。 功能线程池（不推荐使用）嫌上面使用线程池的方法太麻烦？其实Executors已经为我们封装好了 4 种常见的功能线程池，如下： 定长线程池（FixedThreadPool） 定时线程池（ScheduledThreadPool ） 可缓存线程池（CachedThreadPool） 单线程化线程池（SingleThreadExecutor） 定长线程池（FixedThreadPool）创建方法的源码： 1234567891011public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125;public static ExecutorService newFixedThreadPool(int nThreads, ThreadFactory threadFactory) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(), threadFactory);&#125; 特点：只有核心线程，线程数量固定，执行完立即回收，任务队列为链表结构的有界队列。应用场景：控制线程最大并发数。使用示例： 12345678910// 1. 创建定长线程池对象 &amp; 设置线程池线程数量固定为3ExecutorService fixedThreadPool = Executors.newFixedThreadPool(3);// 2. 创建好Runnable类线程对象 &amp; 需执行的任务Runnable task =new Runnable()&#123; public void run() &#123; System.out.println(&quot;执行任务啦&quot;); &#125;&#125;;// 3. 向线程池提交任务fixedThreadPool.execute(task); 定时线程池（ScheduledThreadPool ）创建方法的源码： 123456789101112131415161718192021private static final long DEFAULT_KEEPALIVE_MILLIS = 10L;public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) &#123; return new ScheduledThreadPoolExecutor(corePoolSize);&#125;public ScheduledThreadPoolExecutor(int corePoolSize) &#123; super(corePoolSize, Integer.MAX_VALUE, DEFAULT_KEEPALIVE_MILLIS, MILLISECONDS, new DelayedWorkQueue());&#125;public static ScheduledExecutorService newScheduledThreadPool( int corePoolSize, ThreadFactory threadFactory) &#123; return new ScheduledThreadPoolExecutor(corePoolSize, threadFactory);&#125;public ScheduledThreadPoolExecutor(int corePoolSize, ThreadFactory threadFactory) &#123; super(corePoolSize, Integer.MAX_VALUE, DEFAULT_KEEPALIVE_MILLIS, MILLISECONDS, new DelayedWorkQueue(), threadFactory);&#125; 特点：核心线程数量固定，非核心线程数量无限，执行完闲置 10ms 后回收，任务队列为延时阻塞队列。应用场景：执行定时或周期性的任务。使用示例： 1234567891011// 1. 创建 定时线程池对象 &amp; 设置线程池线程数量固定为5ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(5);// 2. 创建好Runnable类线程对象 &amp; 需执行的任务Runnable task =new Runnable()&#123; public void run() &#123; System.out.println(&quot;执行任务啦&quot;); &#125;&#125;;// 3. 向线程池提交任务scheduledThreadPool.schedule(task, 1, TimeUnit.SECONDS); // 延迟1s后执行任务scheduledThreadPool.scheduleAtFixedRate(task,10,1000,TimeUnit.MILLISECONDS);// 延迟10ms后、每隔1000ms执行任务 可缓存线程池（CachedThreadPool）创建方法的源码： 1234567891011public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125;public static ExecutorService newCachedThreadPool(ThreadFactory threadFactory) &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;(), threadFactory);&#125; 特点：无核心线程，非核心线程数量无限，执行完闲置 60s 后回收，任务队列为不存储元素的阻塞队列。应用场景：执行大量、耗时少的任务。使用示例： 12345678910// 1. 创建可缓存线程池对象ExecutorService cachedThreadPool = Executors.newCachedThreadPool();// 2. 创建好Runnable类线程对象 &amp; 需执行的任务Runnable task =new Runnable()&#123; public void run() &#123; System.out.println(&quot;执行任务啦&quot;); &#125;&#125;;// 3. 向线程池提交任务cachedThreadPool.execute(task); 单线程化线程池（SingleThreadExecutor）创建方法的源码： 12345678910111213public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125;public static ExecutorService newSingleThreadExecutor(ThreadFactory threadFactory) &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(), threadFactory));&#125; 特点：只有 1 个核心线程，无非核心线程，执行完立即回收，任务队列为链表结构的有界队列。应用场景：不适合并发但可能引起 IO 阻塞性及影响 UI 线程响应的操作，如数据库操作、文件操作等。 使用示例： 12345678910// 1. 创建单线程化线程池ExecutorService singleThreadExecutor = Executors.newSingleThreadExecutor();// 2. 创建好Runnable类线程对象 &amp; 需执行的任务Runnable task =new Runnable()&#123; public void run() &#123; System.out.println(&quot;执行任务啦&quot;); &#125;&#125;;// 3. 向线程池提交任务singleThreadExecutor.execute(task); 对比 总结Executors 的 4 个功能线程池虽然方便，但现在已经不建议使用了，而是建议直接通过使用 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。 其实 Executors 的 4 个功能线程有如下弊端： FixedThreadPool 和 SingleThreadExecutor：主要问题是堆积的请求处理队列均采用 LinkedBlockingQueue，可能会耗费非常大的内存，甚至 OOM。CachedThreadPool 和 ScheduledThreadPool：主要问题是线程数最大数是 Integer.MAX_VALUE，可能会创建数量非常多的线程，甚至 OOM。 线程数量如何设置cpu密集型的任务 一般设置 线程数 = 核心数N + 1 io密集型的任务 一般设置 线程数 = 核心数N*2 + 1 12//获取当前机器的核数public static final int cpuNum = Runtime.getRuntime().availableProcessors(); 封装线程池123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123/** * @author zsc * @date 2020/7/31 */import java.util.LinkedList;import java.util.Queue;import java.util.concurrent.*;/** * 线程池管理(线程统一调度管理) */public final class ThreadPoolManager &#123; private static ThreadPoolManager sThreadPoolManagerManager = new ThreadPoolManager(); // 线程池维护线程的最少数量 private static final int SIZE_CORE_POOL = 3; // 线程池维护线程的最大数量 private static final int SIZE_MAX_POOL = 10; // 线程池维护线程所允许的空闲时间 private static final int TIME_KEEP_ALIVE = 10 * 1000; // 线程池所使用的缓冲队列大小 private static final int SIZE_WORK_QUEUE = 500; // 任务调度周期 private static final int PERIOD_TASK_QOS = 1000; /* * 线程池单例创建方法 */ public static ThreadPoolManager getInstance() &#123; return sThreadPoolManagerManager; &#125; // 任务缓冲队列 private final Queue&lt;Runnable&gt; mTaskQueue = new LinkedList&lt;Runnable&gt;(); /* * 线程池超出界线时将任务加入缓冲队列 */ private final RejectedExecutionHandler mHandler = new RejectedExecutionHandler() &#123; @Override public void rejectedExecution(Runnable task, ThreadPoolExecutor executor) &#123; mTaskQueue.offer(task); &#125; &#125;; /* * 将缓冲队列中的任务重新加载到线程池 */ private final Runnable mAccessBufferThread = new Runnable() &#123; @Override public void run() &#123; if (hasMoreAcquire()) &#123; mThreadPool.execute(mTaskQueue.poll()); &#125; &#125; &#125;; /* * 创建一个调度线程池 */ private final ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(1); /* * 通过调度线程周期性的执行缓冲队列中任务 */ protected final ScheduledFuture&lt;?&gt; mTaskHandler = scheduler.scheduleAtFixedRate(mAccessBufferThread, 0, PERIOD_TASK_QOS, TimeUnit.MILLISECONDS); /* * 线程池 */ private final ThreadPoolExecutor mThreadPool = new ThreadPoolExecutor(SIZE_CORE_POOL, SIZE_MAX_POOL, TIME_KEEP_ALIVE, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;Runnable&gt;(SIZE_WORK_QUEUE), mHandler); /* * 将构造方法访问修饰符设为私有，禁止任意实例化。 */ private ThreadPoolManager() &#123; &#125; public void prepare() &#123; if (mThreadPool.isShutdown() &amp;&amp; !mThreadPool.prestartCoreThread()) &#123; @SuppressWarnings(&quot;unused&quot;) int startThread = mThreadPool.prestartAllCoreThreads(); &#125; &#125; /* * 消息队列检查方法 */ private boolean hasMoreAcquire() &#123; return !mTaskQueue.isEmpty(); &#125; /* * 向线程池中添加任务方法 */ public void executeTask(Runnable task) &#123; if (task != null) &#123; mThreadPool.execute(task); &#125; &#125; protected boolean isTaskEnd() &#123; if (mThreadPool.getActiveCount() == 0) &#123; return true; &#125; else &#123; return false; &#125; &#125; public void shutdown() &#123; mTaskQueue.clear(); mThreadPool.shutdown(); &#125;&#125;","categories":[{"name":"Java","slug":"Java","permalink":"https://zhaoshuchao.top/categories/Java/"},{"name":"JUC","slug":"Java/JUC","permalink":"https://zhaoshuchao.top/categories/Java/JUC/"}],"tags":[{"name":"JUC","slug":"JUC","permalink":"https://zhaoshuchao.top/tags/JUC/"},{"name":"并发编程","slug":"并发编程","permalink":"https://zhaoshuchao.top/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"线程","slug":"线程","permalink":"https://zhaoshuchao.top/tags/%E7%BA%BF%E7%A8%8B/"}]}],"categories":[{"name":"Java","slug":"Java","permalink":"https://zhaoshuchao.top/categories/Java/"},{"name":"Redis","slug":"Java/Redis","permalink":"https://zhaoshuchao.top/categories/Java/Redis/"},{"name":"MySQL","slug":"MySQL","permalink":"https://zhaoshuchao.top/categories/MySQL/"},{"name":"SQL优化","slug":"MySQL/SQL优化","permalink":"https://zhaoshuchao.top/categories/MySQL/SQL%E4%BC%98%E5%8C%96/"},{"name":"SQL语句","slug":"MySQL/SQL语句","permalink":"https://zhaoshuchao.top/categories/MySQL/SQL%E8%AF%AD%E5%8F%A5/"},{"name":"索引","slug":"MySQL/索引","permalink":"https://zhaoshuchao.top/categories/MySQL/%E7%B4%A2%E5%BC%95/"},{"name":"Java基础","slug":"Java/Java基础","permalink":"https://zhaoshuchao.top/categories/Java/Java%E5%9F%BA%E7%A1%80/"},{"name":"JVM","slug":"Java/JVM","permalink":"https://zhaoshuchao.top/categories/Java/JVM/"},{"name":"JUC","slug":"Java/JUC","permalink":"https://zhaoshuchao.top/categories/Java/JUC/"},{"name":"事务","slug":"MySQL/事务","permalink":"https://zhaoshuchao.top/categories/MySQL/%E4%BA%8B%E5%8A%A1/"},{"name":"同步锁","slug":"Java/JUC/同步锁","permalink":"https://zhaoshuchao.top/categories/Java/JUC/%E5%90%8C%E6%AD%A5%E9%94%81/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://zhaoshuchao.top/tags/Redis/"},{"name":"MySQL","slug":"MySQL","permalink":"https://zhaoshuchao.top/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"https://zhaoshuchao.top/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"JavaSE","slug":"JavaSE","permalink":"https://zhaoshuchao.top/tags/JavaSE/"},{"name":"集合","slug":"集合","permalink":"https://zhaoshuchao.top/tags/%E9%9B%86%E5%90%88/"},{"name":"JVM","slug":"JVM","permalink":"https://zhaoshuchao.top/tags/JVM/"},{"name":"GC","slug":"GC","permalink":"https://zhaoshuchao.top/tags/GC/"},{"name":"JUC","slug":"JUC","permalink":"https://zhaoshuchao.top/tags/JUC/"},{"name":"并发编程","slug":"并发编程","permalink":"https://zhaoshuchao.top/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"锁","slug":"锁","permalink":"https://zhaoshuchao.top/tags/%E9%94%81/"},{"name":"线程","slug":"线程","permalink":"https://zhaoshuchao.top/tags/%E7%BA%BF%E7%A8%8B/"},{"name":"Java","slug":"Java","permalink":"https://zhaoshuchao.top/tags/Java/"}]}